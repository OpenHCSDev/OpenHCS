"""
PipelineGenerator - Generate complete runnable OpenHCS pipelines.

TWO MODES:
1. Registry-based (instant): Uses pre-absorbed cellprofiler_library
2. LLM-based (fallback): For modules not yet absorbed

Takes converted functions and generates a complete pipeline file with:
- All imports
- Function references (registry) or definitions (LLM)
- FunctionStep wrappers
- Pipeline configuration
"""

import json
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, Dict, List, Optional

from .parser import ModuleBlock
from .llm_converter import ConversionResult
from .settings_binder import SettingsBinder

logger = logging.getLogger(__name__)


@dataclass
class GeneratedPipeline:
    """Complete generated OpenHCS pipeline."""
    
    name: str
    code: str
    source_cppipe: str
    converted_modules: List[str]
    failed_modules: List[str]
    
    def save(self, output_path: Path) -> None:
        """Save pipeline to file."""
        output_path.write_text(self.code)
        logger.info(f"Saved pipeline to {output_path}")


class PipelineGenerator:
    """
    Generate complete OpenHCS pipeline from converted functions.

    TWO MODES:
    1. Registry-based: Uses pre-absorbed cellprofiler_library (instant, no LLM)
    2. LLM-based: Inline function definitions (fallback for unabsorbed modules)

    Creates a runnable pipeline file with:
    1. Standard imports (+ registry imports if using absorbed library)
    2. Converted function definitions (only for non-registry functions)
    3. FunctionStep wrappers for each function
    4. pipeline_steps list
    """

    # Standard imports for generated pipelines
    IMPORTS_BASE = '''"""
OpenHCS Pipeline - Converted from CellProfiler
Source: {source_file}

Auto-generated by CellProfiler → OpenHCS converter.
"""

import numpy as np
from typing import Tuple, List, Optional, Dict, Any
from dataclasses import dataclass
from enum import Enum

# OpenHCS imports
from openhcs.core.steps.function_step import FunctionStep
from openhcs.core.config import LazyProcessingConfig
from openhcs.constants.constants import VariableComponents, GroupBy
from openhcs.core.memory.decorators import numpy
from openhcs.processing.backends.lib_registry.unified_registry import ProcessingContract
from openhcs.core.pipeline.function_contracts import special_outputs
from openhcs.processing.materialization import csv_materializer

'''

    def __init__(self, library_root: Optional[Path] = None):
        """
        Initialize generator.

        Args:
            library_root: Path to absorbed cellprofiler_library
        """
        self.library_root = library_root or Path(__file__).parent.parent / "cellprofiler_library"
        self.settings_binder = SettingsBinder()
        self._registry = self._load_registry()

    def _load_registry(self) -> Dict[str, str]:
        """Load module → function mapping from absorbed library."""
        contracts_file = self.library_root / "contracts.json"
        if not contracts_file.exists():
            logger.info("No absorbed library found - will use LLM fallback")
            return {}

        try:
            data = json.loads(contracts_file.read_text())
            registry = {
                module_name: info["function_name"]
                for module_name, info in data.items()
                if info.get("validated", False)
            }
            logger.info(f"Loaded {len(registry)} absorbed functions from registry")
            return registry
        except Exception as e:
            logger.warning(f"Failed to load registry: {e}")
            return {}

    def has_module(self, module_name: str) -> bool:
        """Check if module exists in absorbed library."""
        return module_name in self._registry
    
    def generate_from_registry(
        self,
        pipeline_name: str,
        source_cppipe: Path,
        modules: List[ModuleBlock],
    ) -> GeneratedPipeline:
        """
        Generate pipeline using absorbed library (instant, no LLM).

        Args:
            pipeline_name: Name for the generated pipeline
            source_cppipe: Path to source .cppipe file
            modules: ModuleBlocks from .cppipe parser

        Returns:
            GeneratedPipeline using registry functions
        """
        # Partition modules into registry-available and missing
        registry_modules = []
        missing_modules = []

        for module in modules:
            if module.name in self._registry:
                registry_modules.append(module)
            else:
                missing_modules.append(module)
                logger.warning(f"Module {module.name} not in absorbed library")

        # Build imports
        imports = self.IMPORTS_BASE.format(source_file=source_cppipe.name)

        # Add registry imports for available modules
        if registry_modules:
            func_imports = [
                "# Absorbed CellProfiler functions",
                "from benchmark.cellprofiler_library import ("
            ]
            for module in registry_modules:
                func_name = self._registry[module.name]
                func_imports.append(f"    {func_name},")
            func_imports.append(")")
            imports += "\n".join(func_imports) + "\n\n"

        # Generate steps with bound settings
        steps = self._generate_steps_from_registry(registry_modules)

        # Combine
        code = imports + steps

        return GeneratedPipeline(
            name=pipeline_name,
            code=code,
            source_cppipe=str(source_cppipe),
            converted_modules=[m.name for m in registry_modules],
            failed_modules=[m.name for m in missing_modules],
        )

    def _generate_steps_from_registry(self, modules: List[ModuleBlock]) -> str:
        """Generate pipeline_steps using registry functions with bound settings."""
        lines = [
            "# Pipeline Steps",
            "# Settings from .cppipe are bound as default parameters",
            "pipeline_steps = [",
        ]

        for module in modules:
            func_name = self._registry[module.name]
            step_name = module.name

            # Bind settings to kwargs
            kwargs = self.settings_binder.bind(module.settings)

            lines.append(f"    FunctionStep(")
            lines.append(f"        func={func_name},")
            lines.append(f'        name="{step_name}",')
            lines.append(f"        processing_config=LazyProcessingConfig(")
            lines.append(f"            variable_components=[VariableComponents.SITE]")
            lines.append(f"        ),")

            # Add bound settings as step kwargs if any
            if kwargs:
                lines.append(f"        # Settings from .cppipe:")
                for k, v in list(kwargs.items())[:5]:  # Limit for readability
                    lines.append(f"        # {k}={repr(v)}")

            lines.append(f"    ),")

        lines.append("]")
        return "\n".join(lines)

    def generate(
        self,
        pipeline_name: str,
        source_cppipe: Path,
        conversion_results: List[ConversionResult],
        modules: List[ModuleBlock],
    ) -> GeneratedPipeline:
        """
        Generate complete pipeline from LLM conversion results (fallback mode).

        Args:
            pipeline_name: Name for the generated pipeline
            source_cppipe: Path to source .cppipe file
            conversion_results: Results from LLM conversion
            modules: Original ModuleBlock list (for ordering)

        Returns:
            GeneratedPipeline with complete code
        """
        # Collect successful conversions
        converted = {r.module_name: r for r in conversion_results if r.success}
        failed = [r.module_name for r in conversion_results if not r.success]

        # Build code sections
        imports = self.IMPORTS_BASE.format(source_file=source_cppipe.name)

        # Function definitions
        function_defs = []
        for module in modules:
            if module.name in converted:
                function_defs.append(f"\n# === {module.name} ===\n")
                function_defs.append(converted[module.name].converted_code)

        # FunctionStep wrappers
        steps = self._generate_steps(modules, converted)

        # Combine all sections
        code = imports + "\n".join(function_defs) + "\n\n" + steps

        return GeneratedPipeline(
            name=pipeline_name,
            code=code,
            source_cppipe=str(source_cppipe),
            converted_modules=list(converted.keys()),
            failed_modules=failed,
        )
    
    def _generate_steps(
        self,
        modules: List[ModuleBlock],
        converted: Dict[str, ConversionResult],
    ) -> str:
        """Generate pipeline_steps list."""
        lines = [
            "# Pipeline Steps",
            "pipeline_steps = [",
        ]
        
        for module in modules:
            if module.name in converted:
                # Derive function name from module name
                func_name = self._module_to_function_name(module.name)
                step_name = module.name
                
                lines.append(f"    FunctionStep(")
                lines.append(f"        func={func_name},")
                lines.append(f'        name="{step_name}",')
                lines.append(f"        processing_config=LazyProcessingConfig(")
                lines.append(f"            variable_components=[VariableComponents.SITE]")
                lines.append(f"        )")
                lines.append(f"    ),")
        
        lines.append("]")
        return "\n".join(lines)
    
    def _module_to_function_name(self, module_name: str) -> str:
        """Convert module name to function name (snake_case)."""
        # IdentifyPrimaryObjects -> identify_primary_objects
        import re
        name = re.sub(r'([A-Z])', r'_\1', module_name).lower().lstrip('_')
        return name
    
    def generate_stub_pipeline(
        self,
        pipeline_name: str,
        source_cppipe: Path,
        modules: List[ModuleBlock],
    ) -> GeneratedPipeline:
        """
        Generate stub pipeline without LLM conversion.
        
        Creates placeholder functions that need manual implementation.
        Useful for testing the generator without LLM.
        """
        imports = self.IMPORTS.format(source_file=source_cppipe.name)
        
        # Generate stub functions
        stubs = []
        for module in modules:
            func_name = self._module_to_function_name(module.name)
            stubs.append(f'''
# === {module.name} (STUB - needs implementation) ===
@numpy(contract=ProcessingContract.PURE_2D)
def {func_name}(image: np.ndarray) -> np.ndarray:
    """
    Stub for {module.name}.
    
    Settings from .cppipe:
{self._format_settings(module.settings)}
    """
    # TODO: Implement conversion from CellProfiler
    return image
''')
        
        # Generate steps
        steps = self._generate_steps(
            modules,
            {m.name: ConversionResult(m.name, True) for m in modules}
        )
        
        code = imports + "\n".join(stubs) + "\n\n" + steps
        
        return GeneratedPipeline(
            name=pipeline_name,
            code=code,
            source_cppipe=str(source_cppipe),
            converted_modules=[],
            failed_modules=[m.name for m in modules],
        )
    
    def _format_settings(self, settings: Dict[str, str], indent: int = 8) -> str:
        """Format settings dict for docstring."""
        prefix = " " * indent
        return "\n".join(f"{prefix}{k}: {v}" for k, v in settings.items())

