# Input Conversion Backend Update Solutions

## Problem Statement

When input conversion happens (disk ‚Üí zarr) during step 1 execution, subsequent steps that read from `PIPELINE_START` should use the zarr backend instead of the original disk backend.

## ‚úÖ IMPLEMENTED SOLUTION

**Location:** `openhcs/core/pipeline/materialization_flag_planner.py` (lines 70-82)

**Approach:** Check during backend determination if input conversion will happen, and set zarr backend for PIPELINE_START steps (steps 1+).

### Implementation

```python
# In MaterializationFlagPlanner.assign_materialization_flags():
else:  # Other steps - read from memory (unless already set by chainbreaker logic)
    if READ_BACKEND not in step_plan:
        from openhcs.core.steps.abstract import InputSource
        if step.processing_config.input_source == InputSource.PIPELINE_START:
            # Check if input conversion will happen - if so, use zarr backend
            if "input_conversion_dir" in step_plans[0]:
                step_plan[READ_BACKEND] = Backend.ZARR.value
                # Also update input_dir to point to conversion target
                step_plan['input_dir'] = step_plans[0]["input_conversion_dir"]
                logger.debug(f"Step {i}: PIPELINE_START with conversion ‚Üí zarr backend")
            else:
                # No conversion - use the same backend as the first step
                step_plan[READ_BACKEND] = step_plans[0][READ_BACKEND]
        else:
            step_plan[READ_BACKEND] = Backend.MEMORY.value
```

### Why This Works

1. ‚úÖ **Compilation-time decision** - No runtime mutation of frozen contexts
2. ‚úÖ **Proper architecture** - Backend determination happens in MaterializationFlagPlanner, which is designed for this
3. ‚úÖ **Forward-looking** - Sets up steps to use zarr BEFORE it exists, knowing step 0 will create it
4. ‚úÖ **Step 0 reads from disk** - Still uses original backend for conversion
5. ‚úÖ **Steps 1+ read from zarr** - Automatically benefit from conversion
6. ‚úÖ **Immutable contexts** - No mutations after compilation

### Execution Flow

```
COMPILE TIME (MaterializationFlagPlanner):
‚îú‚îÄ‚îÄ Step 0: read_backend="disk", input_dir="/plate/images" (will convert)
‚îú‚îÄ‚îÄ Step 1: Detects conversion flag ‚Üí read_backend="zarr", input_dir="/plate/zarr"
‚îî‚îÄ‚îÄ Step 2: Detects conversion flag ‚Üí read_backend="zarr", input_dir="/plate/zarr"

RUNTIME (Step 0):
‚îú‚îÄ‚îÄ Load from disk (/plate/images)
‚îú‚îÄ‚îÄ Convert to zarr
‚îú‚îÄ‚îÄ Save to /plate/zarr
‚îî‚îÄ‚îÄ Update metadata

RUNTIME (Step 1):
‚îî‚îÄ‚îÄ Load from zarr (/plate/zarr) ‚úÖ Fast, efficient!

RUNTIME (Step 2):
‚îî‚îÄ‚îÄ Load from zarr (/plate/zarr) ‚úÖ Fast, efficient!
```

### Alternative: Bulk Preload Still Loads from Disk

**Note:** Bulk preload (lines 1076+ in function_step.py) uses `read_backend`, so it will:
- Step 0: Load from disk (before conversion)
- Step 1+: Load from zarr (after conversion) ‚úÖ

---

## Other Solutions Explored (Not Implemented)

All other solutions are documented below for reference but were rejected in favor of the implemented solution above.

**Approach:** Check at runtime if zarr is available before each step executes.

### Implementation

```python
# In function_step.py, at start of process():
def process(self, context: 'ProcessingContext', step_index: int) -> None:
    step_plan = context.step_plans[step_index]
    
    # DYNAMIC BACKEND RESOLUTION for PIPELINE_START steps
    if step_index > 0:  # Skip first step (no conversion yet)
        from openhcs.core.steps.abstract import InputSource
        if hasattr(self, 'processing_config') and \
           self.processing_config.input_source == InputSource.PIPELINE_START:
            # Re-check available backends at runtime
            available_backends = context.microscope_handler.get_available_backends(context.plate_path)
            if Backend.ZARR in available_backends:
                logger.info(f"üîÑ Step {step_index}: Detected zarr backend now available, updating from {step_plan['read_backend']}")
                step_plan['read_backend'] = Backend.ZARR.value
    
    # Continue with normal processing...
    read_backend = step_plan['read_backend']
```

### Pros
- ‚úÖ Minimal changes to existing code
- ‚úÖ Works even if conversion happens in parallel/distributed scenarios
- ‚úÖ Self-correcting - adapts to actual plate state
- ‚úÖ No changes to compilation phase

### Cons
- ‚ùå Modifies "frozen" step plans at runtime (violates immutability)
- ‚ùå Metadata file I/O on every step execution
- ‚ùå Won't work if steps run in parallel (race condition)
- ‚ùå Doesn't handle distributed execution (separate processes)

### Verdict
‚ö†Ô∏è **Pragmatic but hacky.** Works for single-process sequential execution but breaks architectural principles.

---

## Solution 2: Pre-Execution Conversion Phase

**Approach:** Move input conversion BEFORE any step execution, as a separate orchestrator phase.

### Implementation

```python
# In orchestrator.py execute_compiled_plate():
def execute_compiled_plate(self, pipeline_definition, compiled_contexts, ...):
    # NEW PHASE: Pre-execution input conversion
    if self._needs_input_conversion(compiled_contexts):
        logger.info("üîÑ PRE-EXECUTION: Starting input conversion phase...")
        self._execute_input_conversion_phase(compiled_contexts)
        logger.info("üîÑ PRE-EXECUTION: Input conversion complete, recompiling contexts...")
        
        # Recompile contexts to pick up zarr backend
        compiled_contexts = self._recompile_contexts_after_conversion(compiled_contexts)
    
    # EXISTING: Execute steps with updated contexts
    for axis_id, frozen_context in compiled_contexts.items():
        ...

def _execute_input_conversion_phase(self, compiled_contexts):
    """Execute input conversion for all wells before any step processing."""
    for axis_id, context in compiled_contexts.items():
        if "input_conversion_dir" in context.step_plans[0]:
            self._convert_input_for_well(context, axis_id)
```

### Pros
- ‚úÖ Clean separation of concerns
- ‚úÖ All steps see zarr from the start
- ‚úÖ No runtime modifications to step plans
- ‚úÖ Works with parallel step execution
- ‚úÖ Conversion happens once per plate, not per well

### Cons
- ‚ùå Significant refactoring required
- ‚ùå Breaks current architecture (conversion was meant to be per-step)
- ‚ùå Recompilation overhead
- ‚ùå Complex error handling (what if conversion fails?)
- ‚ùå Doesn't work well with per-well parallel execution

### Verdict
‚úÖ **Architecturally sound but expensive.** Best long-term solution but requires major changes.

---

## Solution 3: Update All Step Plans After First Step Completion

**Approach:** After step 1 completes conversion, update backend for all remaining steps.

### Implementation

```python
# In function_step.py, at end of process() after conversion:
if "input_conversion_dir" in step_plan:
    # ... existing conversion code ...
    
    # UPDATE REMAINING STEPS
    logger.info(f"üîÑ INPUT CONVERSION: Updating read_backend for subsequent PIPELINE_START steps")
    from openhcs.core.steps.abstract import InputSource
    
    for i in range(step_index + 1, len(context.step_plans)):
        next_step_plan = context.step_plans[i]
        # Check if step reads from PIPELINE_START (would need step definition access)
        # For now, assume all subsequent steps that have same read_backend should update
        if next_step_plan.get('read_backend') == read_backend:
            next_step_plan['read_backend'] = Backend.ZARR.value
            logger.info(f"üîÑ Updated step {i} read_backend: {read_backend} ‚Üí zarr")
```

### Pros
- ‚úÖ Localized change (only in function_step.py)
- ‚úÖ Updates happen automatically after conversion
- ‚úÖ Minimal overhead
- ‚úÖ Works with sequential execution

### Cons
- ‚ùå Can't distinguish which steps read from PIPELINE_START vs previous step output
- ‚ùå Doesn't work with parallel well execution (each well has separate context)
- ‚ùå Modifies step plans after compilation
- ‚ùå Brittle (assumes backend equality means PIPELINE_START)

### Verdict
‚ö†Ô∏è **Quick fix but fragile.** Works for common cases but makes assumptions.

---

## Solution 4: Smart Backend Detection in Path Getter

**Approach:** Make `get_paths_for_axis` smart enough to check for zarr at runtime.

### Implementation

```python
# In function_step.py get_all_image_paths():
def get_all_image_paths(input_dir, backend, axis_id, filemanager, microscope_handler):
    """Get image paths, with automatic zarr fallback."""
    
    # Try requested backend first
    try:
        all_image_files = filemanager.list_image_files(str(input_dir), backend)
    except FileNotFoundError:
        # If disk fails and we're reading from PIPELINE_START, try zarr
        logger.debug(f"Backend {backend} not found, checking for zarr alternative...")
        
        # Check if zarr subdirectory exists
        zarr_dir = Path(input_dir).parent / "zarr"
        if backend == Backend.DISK.value and zarr_dir.exists():
            logger.info(f"üîÑ AUTO-FALLBACK: Using zarr backend instead of disk for {input_dir}")
            all_image_files = filemanager.list_image_files(str(zarr_dir), Backend.ZARR.value)
        else:
            raise
    
    # ... rest of function
```

### Pros
- ‚úÖ Transparent to rest of system
- ‚úÖ Works automatically without manual intervention
- ‚úÖ No changes to step plans
- ‚úÖ Handles both converted and non-converted plates

### Cons
- ‚ùå Hidden behavior (magic fallback)
- ‚ùå Error messages become confusing
- ‚ùå Doesn't actually use zarr's benefits (just finds files)
- ‚ùå Path structure assumptions (zarr subdir)

### Verdict
‚ùå **Too magical.** Hides problems rather than solving them.

---

## Solution 5: Lazy Backend Resolution with Context Flag

**Approach:** Add a flag to context indicating conversion happened, check it at step start.

### Implementation

```python
# In function_step.py after conversion:
if "input_conversion_dir" in step_plan:
    # ... existing conversion code ...
    
    # Set flag in context
    context._input_converted_to_zarr = True
    context._zarr_input_dir = str(conversion_dir)

# At start of each step's process():
def process(self, context, step_index):
    step_plan = context.step_plans[step_index]
    
    # Check if conversion happened and adjust
    if step_index > 0 and getattr(context, '_input_converted_to_zarr', False):
        from openhcs.core.steps.abstract import InputSource
        if self.processing_config.input_source == InputSource.PIPELINE_START:
            # Use zarr backend and directory
            step_plan['read_backend'] = Backend.ZARR.value
            step_plan['input_dir'] = context._zarr_input_dir
            logger.info(f"üîÑ Step {step_index}: Using converted zarr input")
```

### Pros
- ‚úÖ Explicit flag makes behavior clear
- ‚úÖ Works for sequential and parallel wells (context per well)
- ‚úÖ Minimal changes
- ‚úÖ Easy to debug

### Cons
- ‚ùå Adds mutable state to "frozen" context
- ‚ùå Doesn't persist across process boundaries
- ‚ùå Still modifies step plans at runtime

### Verdict
‚úÖ **Balanced approach.** Good compromise between cleanliness and practicality.

---

## Solution 6: Conversion-Aware Path Planner

**Approach:** Make path planner check for post-conversion zarr during path resolution.

### Implementation

```python
# In path_planner.py:
def _resolve_step_input_dir(self, step_index, step):
    """Resolve input directory, checking for converted zarr."""
    
    if step_index == 0:
        return self.base_input_dir
    
    from openhcs.core.steps.abstract import InputSource
    if step.processing_config.input_source == InputSource.PIPELINE_START:
        # Check if zarr conversion happened or will happen
        if "input_conversion_dir" in self.plans[0]:
            conversion_dir = Path(self.plans[0]["input_conversion_dir"])
            # Use conversion target as input
            return str(conversion_dir)
        return self.base_input_dir
    
    # Previous step output
    return self.plans[step_index - 1]["output_dir"]
```

### Pros
- ‚úÖ Paths are correct from the start
- ‚úÖ No runtime modifications
- ‚úÖ Clean separation - path planner handles paths
- ‚úÖ Backend follows directory automatically

### Cons
- ‚ùå Assumes conversion will happen (might not if step 1 fails)
- ‚ùå Circular dependency (path planner needs conversion info)
- ‚ùå Doesn't update backend, only path

### Verdict
‚ö†Ô∏è **Half solution.** Fixes paths but not backends.

---

## Recommended Solution: Hybrid Approach (5 + 3)

Combine Solution 5's flag with Solution 3's batch update:

```python
# After conversion in step 1:
if "input_conversion_dir" in step_plan:
    # ... conversion code ...
    
    # 1. Set context flag
    context._input_converted_to_zarr = True
    context._zarr_conversion_dir = str(conversion_dir)
    
    # 2. Update all subsequent PIPELINE_START steps
    from openhcs.core.steps.abstract import InputSource
    for i in range(step_index + 1, len(context.step_plans)):
        next_plan = context.step_plans[i]
        # Only update if this step was supposed to read from PIPELINE_START
        # (identified by having same input_dir as step 0)
        if next_plan.get('input_dir') == context.step_plans[0]['input_dir']:
            next_plan['read_backend'] = Backend.ZARR.value
            next_plan['input_dir'] = str(conversion_dir)
            logger.info(f"üîÑ Updated step {i} to use zarr: {conversion_dir}")
```

### Why This Works
- ‚úÖ Explicit and debuggable (flag)
- ‚úÖ Batch update is efficient
- ‚úÖ Input_dir comparison is reliable way to identify PIPELINE_START steps
- ‚úÖ Works within single-well context (parallel-safe per well)
- ‚úÖ Minimal changes to existing architecture

### Limitations
- ‚ö†Ô∏è Still modifies "frozen" plans (but explicitly and safely)
- ‚ö†Ô∏è Each well converts independently (some duplication)

---

## Alternative: Recompilation-Based Solution (Most Correct)

For truly clean architecture, conversion should trigger recompilation:

```python
# In orchestrator execute_compiled_plate():
def execute_compiled_plate(self, pipeline_definition, compiled_contexts, ...):
    conversion_happened = False
    
    for axis_id, context in compiled_contexts.items():
        # Execute step 0
        pipeline_definition[0].process(context, 0)
        
        # Check if conversion happened
        if hasattr(context, '_input_converted_to_zarr'):
            conversion_happened = True
            break  # Only need one well to convert
    
    # Recompile if conversion happened
    if conversion_happened:
        logger.info("üîÑ Input conversion detected, recompiling contexts...")
        # Re-run compilation to pick up zarr backend
        compiled_contexts = self.compile_plate_for_processing(
            pipeline_definition, 
            plate_path=self.plate_path,
            ...
        )
    
    # Execute remaining steps with updated contexts
    for axis_id, context in compiled_contexts.items():
        for step_index in range(1, len(pipeline_definition)):
            pipeline_definition[step_index].process(context, step_index)
```

This is **architecturally perfect** but complex to implement.
