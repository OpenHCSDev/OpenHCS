\section{Encoding Systems and Coherence}\label{sec:foundations}
%==============================================================================

We formalize encoding systems with modification constraints and prove fundamental limits on coherence. The core results apply universally to any domain where facts are encoded at multiple locations and modifications must preserve correctness. Software systems are one instantiation; distributed databases, configuration management, and version control are others.

\subsection{The Encoding Model}\label{sec:epistemic}

We begin with the abstract encoding model: locations, values, and coherence constraints.

\begin{definition}[Encoding System]\label{def:encoding-system}
An \emph{encoding system} for a fact $F$ is a collection of locations $\{L_1, \ldots, L_n\}$, each capable of holding a value for $F$.
\end{definition}

\begin{definition}[Coherence]\label{def:coherence}
An encoding system is \emph{coherent} iff all locations hold the same value:
\[
\forall i, j: \text{value}(L_i) = \text{value}(L_j)
\]
\end{definition}

\begin{definition}[Incoherence]\label{def:incoherence}
An encoding system is \emph{incoherent} iff some locations disagree:
\[
\exists i, j: \text{value}(L_i) \neq \text{value}(L_j)
\]
\end{definition}

\textbf{The Resolution Problem.} When an encoding system is incoherent, no resolution procedure is information-theoretically justified. Any oracle selecting a value leaves another value disagreeing, creating an unresolvable ambiguity.

\begin{theorem}[Oracle Arbitrariness]\label{thm:oracle-arbitrary}
For any incoherent encoding system $S$ and any oracle $O$ that resolves $S$ to a value $v \in S$, there exists a value $v' \in S$ such that $v' \neq v$.
\end{theorem}

\begin{proof}
By incoherence, $\exists v_1, v_2 \in S: v_1 \neq v_2$. Either $O$ picks $v_1$ (then $v_2$ disagrees) or $O$ doesn't pick $v_1$ (then $v_1$ disagrees).
\end{proof}

\textbf{Interpretation.} This theorem parallels zero-error capacity constraints in communication theory. Just as insufficient side information makes error-free decoding impossible, incoherence makes truth-preserving resolution impossible. The encoding system does not contain sufficient information to determine which value is correct. Any resolution requires external information not present in the encodings themselves.

\begin{definition}[Degrees of Freedom]\label{def:dof-epistemic}
The \emph{degrees of freedom} (DOF) of an encoding system is the number of locations that can be modified independently.
\end{definition}

\begin{theorem}[DOF = 1 Guarantees Coherence]\label{thm:dof-one-coherence}
If DOF = 1, then the encoding system is coherent in all reachable states.
\end{theorem}

\begin{proof}
With DOF = 1, exactly one location is independent. All other locations are derived (automatically updated when the source changes). Derived locations cannot diverge from their source. Therefore, all locations hold the value determined by the single independent source. Disagreement is impossible.
\end{proof}

\begin{theorem}[DOF $>$ 1 Permits Incoherence]\label{thm:dof-gt-one-incoherence}
If DOF $> 1$, then incoherent states are reachable.
\end{theorem}

\begin{proof}
With DOF $> 1$, at least two locations are independent. Independent locations can be modified separately. A sequence of edits can set $L_1 = v_1$ and $L_2 = v_2$ where $v_1 \neq v_2$. This is an incoherent state.
\end{proof}

\begin{corollary}[Coherence Forces DOF = 1]\label{cor:coherence-forces-ssot}
If coherence must be guaranteed (no incoherent states reachable), then DOF = 1 is necessary and sufficient.
\end{corollary}

This is the information-theoretic foundation of optimal encoding under coherence constraints.

\textbf{Connection to Minimum Description Length.} The DOF = 1 optimum directly generalizes Rissanen's MDL principle~\cite{rissanen1978mdl}. MDL states that the optimal representation minimizes total description length: $|$model$|$ + $|$data given model$|$. In encoding systems:

\begin{itemize}
\tightlist
\item \textbf{DOF = 1:} The single source is the minimal model. All derived locations are ``data given model'' with zero additional description length (fully determined by the source). Total encoding rate is minimized.
\item \textbf{DOF $>$ 1:} Redundant independent locations require explicit synchronization. Each additional independent location adds description length with no reduction in uncertainty---pure overhead serving no encoding purpose.
\end{itemize}

Gr√ºnwald~\cite{gruenwald2007mdl} proves that MDL-optimal representations are unique under mild conditions. Theorem~\ref{thm:dof-optimal} establishes the analogous uniqueness for encoding systems under modification constraints: DOF = 1 is the unique coherence-guaranteeing rate.

\textbf{Generative Complexity.} Heering~\cite{heering2015generative} formalized this for computational systems: the \emph{generative complexity} of a program family is the length of the shortest generator. DOF = 1 systems achieve minimal generative complexity---the single source is the generator, derived locations are generated instances. This connects our framework to Kolmogorov complexity while remaining constructive (we provide the generator, not just prove existence).

The following sections show how computational systems instantiate this encoding model.

%------------------------------------------------------------------------------

\subsection{Computational Realizations}\label{sec:edit-space}

The abstract encoding model (Definitions~\ref{def:encoding-system}--\ref{def:dof-epistemic}) applies to any system where:
\begin{enumerate}
\tightlist
\item Facts are encoded at multiple locations
\item Locations can be modified
\item Correctness requires coherence across modifications
\end{enumerate}

\textbf{Domains satisfying these constraints:}
\begin{itemize}
\tightlist
\item \textbf{Software codebases:} Type definitions, registries, configurations
\item \textbf{Distributed databases:} Replica consistency under updates
\item \textbf{Configuration systems:} Multi-file settings (e.g., infrastructure-as-code)
\item \textbf{Version control:} Merge resolution under concurrent modifications
\end{itemize}

We focus on \emph{computational realizations}---systems where locations are syntactic constructs manipulated by tools or humans. Software codebases are the primary example, but the encoding model is not software-specific. This subsection is illustrative; the core information-theoretic results do not depend on any particular computational domain.

\begin{definition}[Codebase (Software Realization)]
A \emph{codebase} $C$ is a finite collection of source files, each containing syntactic constructs (classes, functions, statements, expressions). This is the canonical computational encoding system.
\end{definition}

\begin{definition}[Location]
A \emph{location} $L \in C$ is a syntactically identifiable region: a class definition, function body, configuration value, type annotation, database field, or configuration entry.
\end{definition}

\begin{definition}[Modification Space]
For encoding system $C$, the \emph{modification space} $E(C)$ is the set of all valid modifications. Each edit $\delta \in E(C)$ transforms $C$ into $C' = \delta(C)$.
\end{definition}

The modification space is large (exponential in system size). But we focus on modifications that \emph{change a specific fact}.

\subsection{Facts: Atomic Units of Specification}\label{sec:facts}

\begin{definition}[Fact]\label{def:fact}
A \emph{fact} $F$ is an atomic unit of program specification: a single piece of knowledge that can be independently modified. Facts are the indivisible units of meaning in a specification.
\end{definition}

The granularity of facts is determined by the specification, not the implementation. If two pieces of information must always change together, they constitute a single fact. If they can change independently, they are separate facts.

\noindent\textbf{Examples of facts:}

\begin{center}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Fact} & \textbf{Description} \\
\midrule
$F_1$: ``threshold = 0.5'' & A configuration value \\
$F_2$: ``\texttt{PNGLoader} handles \texttt{.png}'' & A type-to-handler mapping \\
$F_3$: ``\texttt{validate()} returns \texttt{bool}'' & A method signature \\
$F_4$: ``\texttt{Detector} is a subclass of \texttt{Processor}'' & An inheritance relationship \\
$F_5$: ``\texttt{Config} has field \texttt{name: str}'' & A dataclass field \\
\bottomrule
\end{tabular}
\end{center}

\begin{definition}[Structural Fact]\label{def:structural-fact}
A fact $F$ is \emph{structural} with respect to encoding system $C$ iff the locations encoding $F$ are fixed at definition time:
\[
\text{structural}(F, C) \Longleftrightarrow \forall L: \text{encodes}(L, F) \rightarrow L \in \text{DefinitionSyntax}(C)
\]
where $\text{DefinitionSyntax}(C)$ comprises declarative constructs that cannot change post-definition without recreation.
\end{definition}

\textbf{Examples across domains:}
\begin{itemize}
\tightlist
\item \textbf{Software:} Class declarations, method signatures, inheritance clauses, attribute definitions
\item \textbf{Databases:} Schema definitions, table structures, foreign key constraints
\item \textbf{Configuration:} Infrastructure topology, service dependencies
\item \textbf{Version control:} Branch structure, merge policies
\end{itemize}

\textbf{Key property:} Structural facts are fixed at \emph{definition time}. Once defined, their structure cannot change without recreation. This is why structural coherence requires definition-time computation: the encoding locations are only mutable during creation.

\textbf{Non-structural facts} (runtime values, mutable state) have encoding locations modifiable post-definition. Achieving DOF = 1 for non-structural facts requires different mechanisms (reactive bindings, event systems) and is outside this paper's scope. We focus on structural facts because they demonstrate the impossibility results most clearly.

\subsection{Encoding: The Correctness Relationship}\label{sec:encoding}

\begin{definition}[Encodes]\label{def:encodes}
Location $L$ \emph{encodes} fact $F$, written $\text{encodes}(L, F)$, iff correctness requires updating $L$ when $F$ changes.

Formally:
\[
\text{encodes}(L, F) \Longleftrightarrow \forall \delta_F: \neg\text{updated}(L, \delta_F) \rightarrow \text{incorrect}(\delta_F(C))
\]

where $\delta_F$ is an edit targeting fact $F$.
\end{definition}

\textbf{Key insight:} This definition is \textbf{forced} by correctness, not chosen. We do not decide what encodes what. Correctness requirements determine it. If failing to update location $L$ when fact $F$ changes produces an incorrect program, then $L$ encodes $F$. This is an objective, observable property.

\begin{example}[Encoding in Practice]\label{ex:encoding}
Consider a type registry:

\begin{verbatim}
# Location L1: Class definition
class PNGLoader(ImageLoader):
    format = "png"

# Location L2: Registry entry
LOADERS = {"png": PNGLoader, "jpg": JPGLoader}

# Location L3: Documentation
# Supported formats: png, jpg
\end{verbatim}

The fact $F$ = ``\texttt{PNGLoader} handles \texttt{png}'' is encoded at:
\begin{itemize}
\tightlist
\item $L_1$: The class definition (primary encoding)
\item $L_2$: The registry dictionary (secondary encoding)
\item $L_3$: The documentation comment (tertiary encoding)
\end{itemize}

If $F$ changes (e.g., to ``\texttt{PNGLoader} handles \texttt{png} and \texttt{apng}''), all three locations must be updated for correctness. The program is incorrect if $L_2$ still says \texttt{\{"png": PNGLoader\}} when the class now handles both formats.
\end{example}

\subsection{Modification Complexity}\label{sec:mod-complexity}

\begin{definition}[Modification Complexity]\label{def:mod-complexity}
\[
M(C, \delta_F) = |\{L \in C : \text{encodes}(L, F)\}|
\]
The number of locations that must be updated when fact $F$ changes.
\end{definition}

Modification complexity is the central metric of this paper. It measures the \emph{cost} of changing a fact. A codebase with $M(C, \delta_F) = 47$ requires 47 edits to correctly implement a change to fact $F$. A codebase with $M(C, \delta_F) = 1$ requires only 1 edit.

\begin{theorem}[Correctness Forcing]\label{thm:correctness-forcing}
$M(C, \delta_F)$ is the \textbf{minimum} number of edits required for correctness. Fewer edits imply an incorrect program.
\end{theorem}

\begin{proof}
Suppose $M(C, \delta_F) = k$, meaning $k$ locations encode $F$. By Definition~\ref{def:encodes}, each encoding location must be updated when $F$ changes. If only $j < k$ locations are updated, then $k - j$ locations still reflect the old value of $F$. These locations create inconsistencies:

\begin{enumerate}
\tightlist
\item The specification says $F$ has value $v'$ (new)
\item Locations $L_1, \ldots, L_j$ reflect $v'$
\item Locations $L_{j+1}, \ldots, L_k$ reflect $v$ (old)
\end{enumerate}

By Definition~\ref{def:encodes}, the program is incorrect. Therefore, all $k$ locations must be updated, and $k$ is the minimum.
\end{proof}

\subsection{Independence and Degrees of Freedom}\label{sec:dof}

Not all encoding locations are created equal. Some are \emph{derived} from others.

\begin{definition}[Independent Locations]\label{def:independent}
Locations $L_1, L_2$ are \emph{independent} for fact $F$ iff they can diverge. Updating $L_1$ does not automatically update $L_2$, and vice versa.

Formally: $L_1$ and $L_2$ are independent iff there exists a sequence of edits that makes $L_1$ and $L_2$ encode different values for $F$.
\end{definition}

\begin{definition}[Derived Location]\label{def:derived}
Location $L_{\text{derived}}$ is \emph{derived from} $L_{\text{source}}$ iff updating $L_{\text{source}}$ automatically updates $L_{\text{derived}}$. Derived locations are not independent of their sources.
\end{definition}

\begin{example}[Independent vs. Derived]\label{ex:independence}
Consider two architectures for the type registry:

\textbf{Architecture A (independent locations):}
\begin{verbatim}
# L1: Class definition
class PNGLoader(ImageLoader): ...

# L2: Manual registry (independent of L1)
LOADERS = {"png": PNGLoader}
\end{verbatim}

Here $L_1$ and $L_2$ are independent. A developer can change $L_1$ without updating $L_2$, causing inconsistency.

\textbf{Architecture B (derived location):}
\begin{verbatim}
# L1: Class definition with registration
class PNGLoader(ImageLoader):
    format = "png"

# L2: Derived registry (computed from L1)
LOADERS = {cls.format: cls for cls in ImageLoader.__subclasses__()}
\end{verbatim}

Here $L_2$ is derived from $L_1$. Updating the class definition automatically updates the registry. They cannot diverge.
\end{example}

\begin{definition}[Degrees of Freedom]\label{def:dof}
\[
\text{DOF}(C, F) = |\{L \in C : \text{encodes}(L, F) \land \text{independent}(L)\}|
\]
The number of \emph{independent} locations encoding fact $F$.
\end{definition}

DOF is the key metric. Modification complexity $M$ counts all encoding locations. DOF counts only the independent ones. If all but one encoding location is derived, DOF = 1 even though $M$ can be large.

\begin{theorem}[DOF = Incoherence Potential]\label{thm:dof-inconsistency}
$\text{DOF}(C, F) = k$ implies $k$ different values for $F$ can coexist in $C$ simultaneously. With $k > 1$, incoherent states are reachable.
\end{theorem}

\begin{proof}
Each independent location can hold a different value. By Definition~\ref{def:independent}, no constraint forces agreement between independent locations. Therefore, $k$ independent locations can hold $k$ distinct values. This is an instance of Theorem~\ref{thm:dof-gt-one-incoherence} applied to software.
\end{proof}

\begin{corollary}[DOF $>$ 1 Implies Incoherence Risk]\label{cor:dof-risk}
$\text{DOF}(C, F) > 1$ implies incoherent states are reachable. The codebase can enter a state where different locations encode different values for the same fact.
\end{corollary}

\subsection{The DOF Lattice}\label{sec:dof-lattice}

DOF values form a lattice with distinct information-theoretic meanings:

\begin{center}
\begin{tabular}{cl}
\toprule
\textbf{DOF} & \textbf{Encoding Status} \\
\midrule
0 & Fact $F$ is not encoded (no representation) \\
1 & Coherence guaranteed (optimal rate under coherence constraint) \\
$k > 1$ & Incoherence possible (redundant independent encodings) \\
\bottomrule
\end{tabular}
\end{center}

\begin{theorem}[DOF = 1 is Uniquely Coherent]\label{thm:dof-optimal}
For any fact $F$ that must be encoded, $\text{DOF}(C, F) = 1$ is the unique value guaranteeing coherence:
\begin{enumerate}
\tightlist
\item DOF = 0: Fact is not represented
\item DOF = 1: Coherence guaranteed (by Theorem~\ref{thm:dof-one-coherence})
\item DOF $>$ 1: Incoherence reachable (by Theorem~\ref{thm:dof-gt-one-incoherence})
\end{enumerate}
\end{theorem}

\begin{proof}
This is a direct instantiation of Corollary~\ref{cor:coherence-forces-ssot} to computational systems:
\begin{enumerate}
\item DOF = 0 means no location encodes $F$. The fact is unrepresented.
\item DOF = 1 means exactly one independent location. All other encodings are derived. Divergence is impossible. Coherence is guaranteed at optimal rate.
\item DOF $>$ 1 means multiple independent locations. By Corollary~\ref{cor:dof-risk}, they can diverge. Incoherence is reachable.
\end{enumerate}

Only DOF = 1 achieves coherent representation. This is an information-theoretic optimality condition, not a design preference.
\end{proof}

%==============================================================================
\subsection{Encoding-Theoretic CAP and FLP}\label{sec:cap-flp}

We now formalize CAP and FLP inside the encoding model.

\begin{definition}[Local Availability]\label{def:local-availability}
An encoding system for fact $F$ is \emph{locally available} iff for every encoding location $L$ of $F$ and every value $v$, there exists a valid edit $\delta \in E(C)$ such that $\text{updated}(L, \delta)$ and for every other encoding location $L'$, $\neg \text{updated}(L', \delta)$. Informally: each encoding location can be updated without coordinating with others.
\end{definition}

\begin{definition}[Partition Tolerance]\label{def:partition-tolerance}
An encoding system for fact $F$ is \emph{partition-tolerant} iff $F$ is encoded at two or more locations:
\[
|\{L \in C : \text{encodes}(L, F)\}| \ge 2.
\]
This is the minimal formal notion of ``replication'' in our model; without it, partitions are vacuous.
\end{definition}

\begin{theorem}[CAP in the Encoding Model]\label{thm:cap-encoding}
No encoding system can simultaneously guarantee coherence (Definition~\ref{def:coherence}), local availability (Definition~\ref{def:local-availability}), and partition tolerance (Definition~\ref{def:partition-tolerance}) for the same fact $F$.
\end{theorem}

\begin{proof}
Partition tolerance gives at least two encoding locations. Local availability allows each to be updated without updating any other encoding location, so by Definition~\ref{def:independent} there exist two independent locations and thus $\text{DOF}(C, F) > 1$. By Theorem~\ref{thm:dof-gt-one-incoherence}, incoherent states are reachable, contradicting coherence.
\end{proof}

\begin{definition}[Resolution Procedure]\label{def:resolution-procedure}
A \emph{resolution procedure} is a deterministic function $R$ that maps an encoding system state to a value present in that state.
\end{definition}

\begin{theorem}[Static FLP in the Encoding Model]\label{thm:static-flp}
For any incoherent encoding system state and any resolution procedure $R$, the returned value is arbitrary relative to the other values present; no deterministic $R$ can be justified by internal information alone.
\end{theorem}

\begin{proof}
Immediate from Theorem~\ref{thm:oracle-arbitrary}: in an incoherent state, at least two distinct values are present, and any choice leaves another value disagreeing.
\end{proof}

These theorems are the encoding-theoretic counterparts of CAP~\cite{brewer2000cap,gilbert2002cap} and FLP~\cite{flp1985impossibility}: CAP corresponds to the impossibility of coherence when replicated encodings remain independently updatable; FLP corresponds to the impossibility of truth-preserving resolution in an incoherent state without side information.

%==============================================================================
\subsection{Coherence Capacity Theorem}\label{sec:capacity}

We now establish a tight capacity result analogous to Shannon's channel capacity theorem. Where Shannon characterizes the maximum rate for reliable communication, we characterize the maximum encoding rate for guaranteed coherence.

\begin{definition}[Coherence Capacity]\label{def:coherence-capacity}
The \emph{coherence capacity} of an encoding system is the supremum of encoding rates (DOF) that guarantee coherence:
\[
C_{\text{coh}} = \sup\{r : \text{DOF} = r \Rightarrow \text{coherence guaranteed}\}
\]
\end{definition}

\begin{theorem}[Coherence Capacity = 1]\label{thm:coherence-capacity}
The coherence capacity of any encoding system under independent modification is exactly 1:
\[
C_{\text{coh}} = 1
\]
This bound is tight: achievable at DOF = 1, impossible at DOF $> 1$.
\end{theorem}

\begin{proof}
\textbf{Achievability (DOF = 1 achieves capacity):} By Theorem~\ref{thm:dof-one-coherence}, DOF = 1 guarantees coherence. Therefore $C_{\text{coh}} \geq 1$.

\textbf{Converse (DOF $> 1$ exceeds capacity):} We prove that any encoding with DOF $> 1$ cannot guarantee coherence.

Let $\text{DOF}(C, F) = k > 1$. By Definition~\ref{def:independent}, there exist locations $L_1, L_2$ that can be modified independently.

Construct the following modification sequence:
\begin{enumerate}
\tightlist
\item Set $L_1 = v_1$ (valid modification)
\item Set $L_2 = v_2$ where $v_2 \neq v_1$ (valid modification, since $L_2$ is independent of $L_1$)
\end{enumerate}

The resulting state has $\text{value}(L_1) \neq \text{value}(L_2)$. By Definition~\ref{def:incoherence}, this is incoherent.

Since incoherent states are reachable, coherence is not guaranteed. Therefore $C_{\text{coh}} < k$ for all $k > 1$.

Combining: $C_{\text{coh}} \geq 1$ (achievable) and $C_{\text{coh}} < k$ for all $k > 1$ (converse).

Therefore $C_{\text{coh}} = 1$ exactly.
\end{proof}

\textbf{Information-theoretic interpretation.} This theorem is analogous to Shannon's noisy channel coding theorem~\cite{shannon1948mathematical}, which states that reliable communication is possible at rates below channel capacity and impossible above. Here:
\begin{itemize}
\tightlist
\item \textbf{Shannon:} Rate $R < C$ achieves arbitrarily low error; $R > C$ has unavoidable errors
\item \textbf{This work:} DOF $\leq 1$ achieves zero incoherence; DOF $> 1$ has reachable incoherent states
\end{itemize}

The parallel extends to the operational meaning: capacity is the boundary between what's achievable and what's fundamentally impossible, not merely difficult.

\begin{corollary}[Capacity-Achieving Encoding is Unique]\label{cor:capacity-unique}
DOF = 1 is the unique capacity-achieving encoding rate. There is no alternative encoding strategy that achieves coherence at a higher rate.
\end{corollary}

\begin{proof}
By Theorem~\ref{thm:coherence-capacity}, any DOF $> 1$ fails to guarantee coherence. By definition, DOF = 0 fails to encode the fact. Therefore DOF = 1 is the unique coherence-guaranteeing rate.
\end{proof}

\subsection{Side Information for Resolution}\label{sec:side-information}

When an encoding system is incoherent (DOF $> 1$ with divergent values), resolution requires external side information. We quantify exactly how much.

\begin{theorem}[Side Information Requirement]\label{thm:side-info}
Given an incoherent encoding system with $k$ independent locations holding distinct values, resolving to the correct value requires at least $\log_2 k$ bits of side information.
\end{theorem}

\begin{proof}
The $k$ independent locations partition the resolution problem into $k$ equally plausible alternatives. Without loss of generality, each location is an equally plausible authoritative source.

By Theorem~\ref{thm:oracle-arbitrary}, no internal information distinguishes them. Resolution requires identifying which of $k$ alternatives is correct.

Information-theoretically, selecting one of $k$ equally likely alternatives requires $\log_2 k$ bits (the entropy of a uniform distribution over $k$ outcomes).

Therefore, resolution requires $\geq \log_2 k$ bits of side information.
\end{proof}

\begin{corollary}[DOF = 1 Requires Zero Side Information]\label{cor:dof1-zero-side}
With DOF = 1, resolution requires 0 bits of side information.
\end{corollary}

\begin{proof}
$\log_2(1) = 0$. With one independent location, that location is trivially authoritative.
\end{proof}

\textbf{Connection to Slepian-Wolf coding.} In distributed source coding~\cite{slepian1973noiseless}, the decoder uses side information $Y$ to decode $X$ at rate $H(X|Y)$ instead of $H(X)$. Our result is analogous: side information about the authoritative source reduces the ``decoding'' (resolution) problem from $\log_2 k$ bits to 0 bits.

\begin{example}[Side Information in Practice]\label{ex:side-info-practice}
Consider a configuration system with DOF = 3:
\begin{itemize}
\tightlist
\item \texttt{config.yaml}: \texttt{threshold: 0.5}
\item \texttt{settings.json}: \texttt{"threshold": 0.7}
\item \texttt{params.toml}: \texttt{threshold = 0.6}
\end{itemize}

To resolve this incoherence requires $\log_2 3 \approx 1.58$ bits of side information. In practice, side information takes forms such as:
\begin{itemize}
\tightlist
\item A priority ordering: ``YAML takes precedence'' (encodes which of 3 is authoritative)
\item A timestamp: ``most recent wins'' (encodes temporal ordering)
\item An explicit declaration: ``params.toml is the source of truth''
\end{itemize}

With DOF = 1, no such side information is needed---the single source is self-evidently authoritative.
\end{example}

\subsection{Structure Theorems: The Derivation Lattice}\label{sec:derivation-structure}

The set of derivation relations on an encoding system has algebraic structure. We characterize this structure and its computational implications.

\begin{definition}[Derivation Relation]\label{def:derivation-relation}
A \emph{derivation relation} $D \subseteq L \times L$ on locations $L$ is a directed relation where $(L_s, L_d) \in D$ means $L_d$ is derived from $L_s$. We require $D$ be acyclic (no location derives from itself through any chain).
\end{definition}

\begin{definition}[DOF under Derivation]\label{def:dof-derivation}
Given derivation relation $D$, the degrees of freedom is:
\[
\text{DOF}(D) = |\{L : \nexists L'. (L', L) \in D\}|
\]
The count of locations with no incoming derivation edges (source locations).
\end{definition}

\begin{theorem}[Derivation Lattice]\label{thm:derivation-lattice}
The set of derivation relations on a fixed set of locations $L$, ordered by inclusion, forms a bounded lattice:
\begin{enumerate}
\tightlist
\item \textbf{Bottom ($\bot$):} $D = \emptyset$ (no derivations, DOF = $|L|$)
\item \textbf{Top ($\top$):} Maximal acyclic $D$ with DOF = 1 (all but one location derived)
\item \textbf{Meet ($\land$):} $D_1 \land D_2 = D_1 \cap D_2$
\item \textbf{Join ($\lor$):} $D_1 \lor D_2 = $ transitive closure of $D_1 \cup D_2$ (if acyclic)
\end{enumerate}
\end{theorem}

\begin{proof}
\textbf{Bottom:} $\emptyset$ is trivially a derivation relation with all locations independent.

\textbf{Top:} For $n$ locations, a maximal acyclic relation has one source (root) and $n-1$ derived locations forming a tree or DAG. DOF = 1.

\textbf{Meet:} Intersection of acyclic relations is acyclic. The intersection preserves only derivations present in both.

\textbf{Join:} If $D_1 \cup D_2$ is acyclic, its transitive closure is the smallest relation containing both. If cyclic, join is undefined (partial lattice).

Bounded: $\emptyset \subseteq D \subseteq \top$ for all valid $D$.
\end{proof}

\begin{theorem}[DOF is Anti-Monotonic]\label{thm:dof-antimonotone}
DOF is anti-monotonic in the derivation lattice:
\[
D_1 \subseteq D_2 \Rightarrow \text{DOF}(D_1) \geq \text{DOF}(D_2)
\]
More derivations imply fewer independent locations.
\end{theorem}

\begin{proof}
Adding a derivation edge $(L_s, L_d)$ to $D$ can only decrease DOF: if $L_d$ was previously a source (no incoming edges), it now has an incoming edge and is no longer a source. Sources can only decrease or stay constant as derivations are added.
\end{proof}

\begin{corollary}[Minimal DOF = 1 Derivations]\label{cor:minimal-dof1}
A derivation relation $D$ with DOF($D$) = 1 is \emph{minimal} iff removing any edge increases DOF.
\end{corollary}

\textbf{Computational implication:} Given an encoding system, there can be multiple DOF-1-achieving derivation structures. The minimal ones use the fewest derivation edges---the most economical way to achieve coherence.

\textbf{Representation model for complexity.} For the algorithmic results below, we assume the derivation relation $D$ is given explicitly as a DAG over the location set $L$. The input size is $|L| + |D|$, and all complexity bounds are measured in this explicit representation.

\begin{theorem}[DOF Computation Complexity]\label{thm:dof-complexity}
Given an encoding system with explicit derivation relation $D$:
\begin{enumerate}
\tightlist
\item Computing DOF($D$) is $O(|L| + |D|)$ (linear in locations plus edges)
\item Deciding if DOF($D$) = 1 is $O(|L| + |D|)$
\item Finding a minimal DOF-1 extension of $D$ is $O(|L|^2)$ in the worst case
\end{enumerate}
\end{theorem}

\begin{proof}
\textbf{(1) DOF computation:} Count locations with in-degree 0 in the DAG. Single pass over edges: $O(|D|)$ to compute in-degrees, $O(|L|)$ to count zeros.

\textbf{(2) DOF = 1 decision:} Compute DOF, compare to 1. Same complexity.

\textbf{(3) Minimal extension:} Must connect $k-1$ source locations to reduce DOF from $k$ to 1. Finding which connections preserve acyclicity requires reachability queries. Naive: $O(|L|^2)$. With better data structures (e.g., dynamic reachability): $O(|L| \cdot |D|)$ amortized.
\end{proof}

%==============================================================================
