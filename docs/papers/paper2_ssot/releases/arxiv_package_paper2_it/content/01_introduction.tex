\section{Introduction}\label{introduction}

\subsection{Zero-Incoherence Capacity}\label{sec:encoding-problem}

We study a fundamental question in encoding theory: \emph{What is the maximum encoding rate that guarantees zero probability of incoherence in a multi-location storage system?}

An \emph{encoding system} stores a fact $F$ (a value from alphabet $\mathcal{V}_F$) at multiple locations $\{L_1, \ldots, L_n\}$. The system is \emph{coherent} if all locations encode the same value; \emph{incoherent} if any two locations disagree. We define the \textbf{zero-incoherence capacity} $C_0$ as the supremum of encoding rates achieving incoherence probability exactly zero, and prove:

\begin{center}
\fbox{$C_0 = 1$}
\end{center}

This extends \emph{zero-error capacity theory}~\cite{shannon1956zero,korner1973graphs,lovasz1979shannon} to interactive encoding systems. Shannon's zero-error capacity characterizes the maximum communication rate with exactly zero error probability. We characterize the maximum encoding rate with exactly zero incoherence probability.

\textbf{Main results.} Let DOF (Degrees of Freedom) denote the encoding rate: the number of independent locations that can hold distinct values simultaneously.
\begin{itemize}
\tightlist
\item \textbf{Achievability (Theorem~\ref{thm:capacity-achievability}):} DOF $= 1$ achieves zero incoherence.
\item \textbf{Converse (Theorem~\ref{thm:capacity-converse}):} DOF $> 1$ does not achieve zero incoherence.
\item \textbf{Capacity (Theorem~\ref{thm:coherence-capacity}):} $C_0 = 1$ exactly. Tight.
\item \textbf{Side Information (Theorem~\ref{thm:side-info}):} Resolution of $k$-way incoherence requires $\geq \log_2 k$ bits.
\end{itemize}

\begin{theorem}[Resolution Impossibility, informal]
For any incoherent encoding system and any resolution procedure, there exists a value present in the system that disagrees with the resolution. Without $\log_2 k$ bits of side information (where $k$ = DOF), no resolution is information-theoretically justified.
\end{theorem}

This parallels zero-error decoding constraints~\cite{korner1973graphs,lovasz1979shannon}: without sufficient side information, error-free reconstruction is impossible.

\subsection{The Capacity Theorem}\label{sec:optimal-rate}

The zero-incoherence capacity follows the achievability/converse structure of Shannon's channel capacity theorem:

\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Encoding Rate} & \textbf{Zero Incoherence?} & \textbf{Interpretation} \\
\midrule
DOF $= 0$ & N/A & Fact not encoded \\
DOF $= 1$ & \textbf{Yes} & Capacity-achieving \\
DOF $> 1$ & No & Above capacity \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Comparison to Shannon capacity.} Shannon's channel capacity $C$ is the supremum of rates $R$ achieving vanishing error probability: $\lim_{n \to \infty} P_e^{(n)} = 0$. Our zero-incoherence capacity is the supremum of rates achieving \emph{exactly zero} incoherence probability---paralleling zero-error capacity~\cite{shannon1956zero}, not ordinary capacity.

\textbf{Connection to MDL.} The capacity theorem generalizes Rissanen's Minimum Description Length principle~\cite{rissanen1978mdl,gruenwald2007mdl} to interactive systems. MDL optimizes description length for static data. We optimize encoding rate for modifiable data subject to coherence constraints. The result: exactly one rate ($R = 1$) achieves zero incoherence, making this a \textbf{forcing theorem}.

\subsection{Applications Across Domains}\label{sec:applications}

The abstract encoding model applies wherever facts are stored redundantly:

\begin{itemize}
\tightlist
\item \textbf{Distributed databases:} Replica consistency under partition constraints~\cite{brewer2000cap}
\item \textbf{Version control:} Merge resolution when branches diverge~\cite{hunt2002vcdiff}
\item \textbf{Configuration systems:} Multi-file settings with coherence requirements~\cite{delaet2010survey}
\item \textbf{Software systems:} Class registries, type definitions, interface contracts~\cite{hunt1999pragmatic}
\end{itemize}

In each domain, the question is identical: given multiple encoding locations, which is authoritative? Our theorems characterize when this question has a unique answer (DOF = 1) versus when it requires arbitrary external resolution (DOF $> 1$).

\subsection{Connection to Classical Information Theory}\label{sec:connection-it}

Our results extend classical source coding theory to interactive multi-terminal systems.

\textbf{1. Multi-terminal source coding.} Slepian-Wolf~\cite{slepian1973noiseless} characterizes distributed encoding of correlated sources. We model encoding locations as terminals: derivation introduces \emph{perfect correlation} (deterministic dependence), reducing effective rate. The capacity result shows that only complete correlation (all terminals derived from one source) guarantees coherence---partial correlation permits divergence. Section~\ref{sec:side-information} develops this connection.

\textbf{2. Zero-error capacity.} Shannon~\cite{shannon1956zero}, Körner~\cite{korner1973graphs}, and Lovász~\cite{lovasz1979shannon} characterize zero-error communication. We characterize \textbf{zero-incoherence encoding}---a storage analog where ``errors'' are disagreements among locations. The achievability/converse structure (Theorems~\ref{thm:capacity-achievability},~\ref{thm:capacity-converse}) parallels zero-error capacity proofs.

\textbf{3. Interactive information theory.} The BIRS workshop~\cite{birs2012interactive} identified interactive IT as encoding/decoding with feedback and multi-round protocols. Our model is interactive: encodings are modified over time, and causal propagation (a realizability requirement) is analogous to channel feedback. Ma-Ishwar~\cite{ma2011distributed} showed interaction can reduce rate; we show derivation (a form of interaction) can reduce effective DOF.

\textbf{4. Rate-complexity tradeoffs.} Rate-distortion theory~\cite{cover2006elements} trades rate $R$ against distortion $D$. We trade encoding rate (DOF) against modification complexity $M$: DOF $= 1$ achieves $M = O(1)$; DOF $> 1$ requires $M = \Omega(n)$. The gap is unbounded (Theorem~\ref{thm:unbounded-gap}).

\subsection{Encoder Realizability}\label{sec:realizability}

A key question: what encoder properties are necessary and sufficient to achieve capacity ($C_0 = 1$)? We prove realizability requires two information-theoretic properties:

\begin{enumerate}
\item \textbf{Causal update propagation (feedback coupling):} Changes to the source must automatically trigger updates to derived locations. This is analogous to \emph{channel coding with feedback}~\cite{cover2006elements}---the encoder (source) and decoder (derived locations) are coupled causally. Without feedback, a temporal window exists where source and derived locations diverge (temporary incoherence).

\item \textbf{Provenance observability (decoder side information):} The system must support queries about derivation structure. This is the encoding-system analog of \emph{Slepian-Wolf side information}~\cite{slepian1973noiseless}---the decoder has access to structural information enabling verification that all terminals are derived from the source.
\end{enumerate}

\begin{theorem}[Encoder Realizability, informal]
An encoding system achieves $C_0 = 1$ iff it provides both causal propagation and provenance observability. Neither alone suffices (Theorem~\ref{thm:independence}).
\end{theorem}

\textbf{Connection to multi-version coding.} Rashmi et al.~\cite{rashmi2015multiversion} prove an ``inevitable storage cost for consistency'' in distributed storage. Our realizability theorem is analogous: systems lacking either encoder property \emph{cannot} achieve capacity---the constraint is information-theoretic, not implementation-specific.

\textbf{Instantiations.} The encoder properties instantiate across domains: programming languages (definition-time hooks, introspection), distributed databases (triggers, system catalogs), configuration systems (dependency graphs, state queries). Section~\ref{sec:evaluation} provides a programming-language instantiation as a corollary; the core theorems are domain-independent.

\subsection{Paper Organization}\label{overview}

All results are machine-checked in Lean 4~\cite{demoura2021lean4} (9,351 lines, 541 theorems, 0 \texttt{sorry} placeholders).

\textbf{Section~\ref{sec:foundations}---Encoding Model and Capacity.} We define multi-location encoding systems, encoding rate (DOF), and coherence/incoherence. We introduce information-theoretic quantities (value entropy, redundancy, incoherence entropy). We prove the \textbf{zero-incoherence capacity theorem} ($C_0 = 1$) with explicit achievability/converse structure, and the \textbf{side information bound} ($\geq \log_2 k$ bits for $k$-way resolution). We formalize encoding-theoretic CAP/FLP.

\textbf{Section~\ref{sec:ssot}---Derivation and Optimal Rate.} We characterize derivation as the mechanism achieving capacity: derived locations are perfectly correlated with their source, contributing zero effective rate.

\textbf{Section~\ref{sec:requirements}---Encoder Realizability.} We prove that achieving capacity requires causal propagation (feedback) and provenance observability (decoder side information). Both necessary; together sufficient. This is an iff characterization.

\textbf{Section~\ref{sec:bounds}---Rate-Complexity Tradeoffs.} We prove modification complexity is $O(1)$ at capacity vs. $\Omega(n)$ above capacity. The gap is unbounded.

\textbf{Sections~\ref{sec:evaluation},~\ref{sec:empirical}---Instantiations (Corollaries).} Programming-language instantiation and worked example. These illustrate the abstract theory; core results are domain-independent.


\subsection{Core Theorems}\label{sec:core-theorems}

We establish five \emph{core} theorems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Theorem~\ref{thm:coherence-capacity} (Zero-Incoherence Capacity):} $C_0 = 1$. The maximum encoding rate guaranteeing zero incoherence is exactly 1.

  \emph{Structure:} Achievability (Theorem~\ref{thm:capacity-achievability}) + Converse (Theorem~\ref{thm:capacity-converse}).

\item
  \textbf{Theorem~\ref{thm:side-info} (Side Information Bound):} Resolution of $k$-way incoherence requires $\geq \log_2 k$ bits of side information. At DOF $= 1$, zero bits suffice.

  \emph{Proof:} The $k$ alternatives have entropy $H(S) = \log_2 k$. Resolution requires mutual information $I(S; Y) \geq H(S)$.

\item
  \textbf{Theorem~\ref{thm:oracle-arbitrary} (Resolution Impossibility):} Without side information, no resolution procedure is information-theoretically justified.

  \emph{Proof:} By incoherence, $k \geq 2$ values exist. Any selection leaves $k-1$ values disagreeing. No internal information distinguishes them.

\item
  \textbf{Theorem~\ref{thm:ssot-iff} (Encoder Realizability):} Achieving capacity requires encoder properties: (a) causal propagation (feedback), and (b) provenance observability (side information). Both necessary; together sufficient.

  \emph{Proof:} Necessity by constructing above-capacity configurations when either is missing. Sufficiency by exhibiting capacity-achieving encoders.

\item
  \textbf{Theorem~\ref{thm:unbounded-gap} (Rate-Complexity Tradeoff):} Modification complexity scales as $O(1)$ at capacity vs. $\Omega(n)$ above capacity. The gap is unbounded.

  \emph{Proof:} At capacity, one source update suffices. Above capacity, $n$ independent locations require $n$ updates.
\end{enumerate}

\textbf{Uniqueness.} $C_0 = 1$ is the \textbf{unique} capacity: DOF $= 0$ fails to encode; DOF $> 1$ exceeds capacity. Given zero-incoherence as a constraint, the rate is mathematically forced.

\subsection{Scope}\label{sec:scope}

This work characterizes SSOT for \emph{structural facts} (class existence, method signatures, type relationships) within \emph{single-language} systems. The complexity analysis is asymptotic, applying to systems where $n$ grows. External tooling can approximate SSOT behavior but operates outside language semantics.

\textbf{Multi-language systems.} When a system spans multiple languages (e.g., Python backend + TypeScript frontend + protobuf schemas), cross-language SSOT requires external code generation tools. The analysis in this paper characterizes single-language SSOT; multi-language SSOT is noted as future work (Section~\ref{sec:conclusion}).

\subsection{Contributions}\label{sec:contributions}

This paper makes five information-theoretic contributions:

\textbf{1. Zero-incoherence capacity (Section~\ref{sec:capacity}):}
\begin{itemize}
\tightlist
\item Definition of encoding rate (DOF) and incoherence
\item \textbf{Theorem~\ref{thm:coherence-capacity}:} $C_0 = 1$ (tight: achievability + converse)
\item \textbf{Theorem~\ref{thm:redundancy-incoherence}:} Redundancy $\rho > 0$ iff incoherence reachable
\end{itemize}

\textbf{2. Side information bounds (Section~\ref{sec:side-information}):}
\begin{itemize}
\tightlist
\item \textbf{Theorem~\ref{thm:side-info}:} $k$-way resolution requires $\geq \log_2 k$ bits
\item \textbf{Corollary~\ref{cor:dof1-zero-side}:} DOF $= 1$ requires zero side information
\item Multi-terminal interpretation: derivation as perfect correlation
\end{itemize}

\textbf{3. Encoder realizability (Section~\ref{sec:requirements}):}
\begin{itemize}
\tightlist
\item \textbf{Theorem~\ref{thm:ssot-iff}:} Capacity achieved iff causal propagation AND provenance observability
\item \textbf{Theorem~\ref{thm:independence}:} Requirements are independent
\item Connection to feedback channels and Slepian-Wolf side information
\end{itemize}

\textbf{4. Rate-complexity tradeoffs (Section~\ref{sec:bounds}):}
\begin{itemize}
\tightlist
\item \textbf{Theorem~\ref{thm:upper-bound}:} $O(1)$ at capacity
\item \textbf{Theorem~\ref{thm:lower-bound}:} $\Omega(n)$ above capacity
\item \textbf{Theorem~\ref{thm:unbounded-gap}:} Gap unbounded
\end{itemize}

\textbf{5. Encoding-theoretic CAP/FLP (Section~\ref{sec:cap-flp}):}
\begin{itemize}
\tightlist
\item \textbf{Theorem~\ref{thm:cap-encoding}:} CAP as encoding impossibility
\item \textbf{Theorem~\ref{thm:static-flp}:} FLP as resolution impossibility
\end{itemize}

\textbf{Instantiations (corollaries).} Sections~\ref{sec:evaluation} and~\ref{sec:empirical} instantiate the realizability theorem for programming languages and provide a worked example. These are illustrative corollaries; the core information-theoretic results are self-contained in Sections~\ref{sec:foundations}--\ref{sec:bounds}.


%==============================================================================
