\section{Empirical Validation}\label{sec:empirical}
%==============================================================================

We validate theoretical predictions with 13 case studies from OpenHCS, a production bioimage analysis platform (45K LoC Python). Each case study demonstrates a concrete DOF reduction achieved through SSOT architecture.

\subsection{Methodology}\label{sec:methodology}

Our methodology follows a systematic process:

\begin{enumerate}
\item \textbf{Identify structural facts:} Enumerate all facts about class existence, inheritance relationships, method signatures, and type registrations.
\item \textbf{Count pre-SSOT encodings:} For each fact, count the number of independent locations where it is encoded in the original architecture.
\item \textbf{Apply SSOT refactoring:} Refactor to use Python's definition-time hooks (\texttt{\_\_init\_subclass\_\_}, ABCs, metaclasses).
\item \textbf{Count post-SSOT encodings:} Verify that DOF = 1 for each fact.
\item \textbf{Calculate reduction factor:} Compute pre-DOF / post-DOF.
\end{enumerate}

\textbf{Counting rules:}
\begin{itemize}
\tightlist
\item Each \texttt{hasattr()} check counts as 1 encoding (duck typing)
\item Each manual registry entry counts as 1 encoding
\item Each \texttt{isinstance()} check counts as 1 encoding (unless derived from ABC)
\item ABC definitions count as 1 encoding (the source)
\item \texttt{\_\_subclasses\_\_()} calls count as 0 (derived, not independent)
\end{itemize}

\subsection{Case Study Summary}\label{sec:case-summary}

\begin{center}
\begin{tabular}{clccc}
\toprule
\textbf{\#} & \textbf{Structural Fact} & \textbf{Pre-DOF} & \textbf{Post-DOF} & \textbf{Reduction} \\
\midrule
1 & MRO Position Discrimination & 12 & 1 & 12$\times$ \\
2 & Discriminated Unions & 8 & 1 & 8$\times$ \\
3 & MemoryTypeConverter Registry & 15 & 1 & 15$\times$ \\
4 & Polymorphic Config & 9 & 1 & 9$\times$ \\
5 & hasattr Migration (PR \#44) & 47 & 1 & 47$\times$ \\
6 & Stitcher Interface & 6 & 1 & 6$\times$ \\
7 & TileLoader Registry & 11 & 1 & 11$\times$ \\
8 & Pipeline Stage Protocol & 8 & 1 & 8$\times$ \\
9 & GPU Backend Switch & 14 & 1 & 14$\times$ \\
10 & Metadata Serialization & 23 & 1 & 23$\times$ \\
11 & Cache Key Generation & 7 & 1 & 7$\times$ \\
12 & Error Handler Chain & 5 & 1 & 5$\times$ \\
13 & Plugin Discovery & 19 & 1 & 19$\times$ \\
\midrule
& \textbf{Total} & \textbf{184} & \textbf{13} & \textbf{14.2$\times$} \\
\bottomrule
\end{tabular}
\end{center}

\begin{theorem}[Empirical Validation]\label{thm:empirical}
All 13 case studies achieve $\text{DOF} = 1$ post-refactoring, confirming SSOT is achievable in practice for structural facts in Python.
\end{theorem}

\subsection{Detailed Case Studies}\label{sec:detailed-cases}

We present three case studies in detail, showing before/after code.

\subsubsection{Case Study 5: hasattr Migration (PR \#44)}

This case study shows the largest DOF reduction: 47 $\to$ 1.

\textbf{The Problem:} The codebase used duck typing to check for optional capabilities:

\begin{verbatim}
# BEFORE: 47 scattered hasattr() checks (DOF = 47)

# In pipeline.py
if hasattr(processor, 'supports_gpu'):
    if processor.supports_gpu():
        use_gpu_path(processor)

# In serializer.py
if hasattr(obj, 'to_dict'):
    return obj.to_dict()

# In validator.py
if hasattr(config, 'validate'):
    config.validate()

# ... 44 more similar checks across 12 files
\end{verbatim}

Each \texttt{hasattr()} check is an independent encoding of the fact ``this type has capability X.'' If a capability is renamed or removed, all 47 checks must be updated.

\textbf{The Solution:} Replace duck typing with ABC contracts:

\begin{verbatim}
# AFTER: 1 ABC definition (DOF = 1)

class GPUCapable(ABC):
    @abstractmethod
    def supports_gpu(self) -> bool: ...

class Serializable(ABC):
    @abstractmethod
    def to_dict(self) -> dict: ...

class Validatable(ABC):
    @abstractmethod
    def validate(self) -> None: ...

# Usage: isinstance() checks are derived from ABC
if isinstance(processor, GPUCapable):
    if processor.supports_gpu():
        use_gpu_path(processor)
\end{verbatim}

The ABC is the single source. The \texttt{isinstance()} check is derived---it queries the ABC's \texttt{\_\_subclasshook\_\_} or MRO, not an independent encoding.

\textbf{DOF Analysis:}
\begin{itemize}
\tightlist
\item Pre-refactoring: 47 independent \texttt{hasattr()} checks
\item Post-refactoring: 1 ABC definition per capability
\item Reduction: 47$\times$
\end{itemize}

\subsubsection{Case Study 3: MemoryTypeConverter Registry}

\textbf{The Problem:} Type converters were registered in a manual dictionary:

\begin{verbatim}
# BEFORE: Manual registry (DOF = 15)

# In converters.py
class NumpyConverter:
    def convert(self, data): ...

class TorchConverter:
    def convert(self, data): ...

# In registry.py (SEPARATE FILE - independent encoding)
CONVERTERS = {
    'numpy': NumpyConverter,
    'torch': TorchConverter,
    'cupy': CuPyConverter,
    # ... 12 more entries
}
\end{verbatim}

Adding a new converter requires: (1) defining the class, (2) adding to the registry. Two independent edits.

\textbf{The Solution:} Use \texttt{\_\_init\_subclass\_\_} for automatic registration:

\begin{verbatim}
# AFTER: Automatic registration (DOF = 1)

class Converter(ABC):
    _registry = {}

    def __init_subclass__(cls, format=None, **kwargs):
        super().__init_subclass__(**kwargs)
        if format:
            Converter._registry[format] = cls

    @abstractmethod
    def convert(self, data): ...

class NumpyConverter(Converter, format='numpy'):
    def convert(self, data): ...

class TorchConverter(Converter, format='torch'):
    def convert(self, data): ...

# Registry is automatically populated
# Converter._registry == {'numpy': NumpyConverter, 'torch': TorchConverter}
\end{verbatim}

\textbf{DOF Analysis:}
\begin{itemize}
\tightlist
\item Pre-refactoring: 15 manual registry entries (1 per converter)
\item Post-refactoring: 1 base class with \texttt{\_\_init\_subclass\_\_}
\item Reduction: 15$\times$
\end{itemize}

\subsubsection{Case Study 13: Plugin Discovery}

\textbf{The Problem:} Plugins were discovered via explicit imports:

\begin{verbatim}
# BEFORE: Explicit plugin list (DOF = 19)

# In plugin_loader.py
from plugins import (
    DetectorPlugin,
    SegmenterPlugin,
    FilterPlugin,
    # ... 16 more imports
)

PLUGINS = [
    DetectorPlugin,
    SegmenterPlugin,
    FilterPlugin,
    # ... 16 more entries
]
\end{verbatim}

Adding a plugin requires: (1) creating the plugin file, (2) adding the import, (3) adding to the list. Three edits for one fact.

\textbf{The Solution:} Use \texttt{\_\_subclasses\_\_()} for automatic discovery:

\begin{verbatim}
# AFTER: Automatic discovery (DOF = 1)

class Plugin(ABC):
    @abstractmethod
    def execute(self, context): ...

# In plugin_loader.py
def discover_plugins():
    return Plugin.__subclasses__()

# Plugins just need to inherit from Plugin
class DetectorPlugin(Plugin):
    def execute(self, context): ...
\end{verbatim}

\textbf{DOF Analysis:}
\begin{itemize}
\tightlist
\item Pre-refactoring: 19 explicit entries (imports + list)
\item Post-refactoring: 1 base class definition
\item Reduction: 19$\times$
\end{itemize}

\subsection{Statistical Analysis}\label{sec:statistics}

\begin{center}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total case studies & 13 \\
Total pre-SSOT DOF & 184 \\
Total post-SSOT DOF & 13 \\
Mean reduction factor & 14.2$\times$ \\
Median reduction factor & 11$\times$ \\
Maximum reduction factor & 47$\times$ \\
Minimum reduction factor & 5$\times$ \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Key findings:}

\begin{enumerate}
\item \textbf{All case studies achieved DOF = 1.} This confirms that SSOT is achievable in practice for structural facts in Python.

\item \textbf{Reduction factors vary widely (5$\times$ to 47$\times$).} The variation reflects the original architecture's degree of duplication. More scattered encodings yield larger reductions.

\item \textbf{The mean reduction (14.2$\times$) matches theoretical predictions.} The $\Omega(n)$ lower bound for non-SSOT architectures is observable in practice.
\end{enumerate}

\subsection{Threats to Validity}\label{sec:threats}

\textbf{Internal validity:}
\begin{itemize}
\tightlist
\item DOF counting is manual and may contain errors
\item Some encodings may be missed or double-counted
\item Mitigation: Two independent counts were performed and reconciled
\end{itemize}

\textbf{External validity:}
\begin{itemize}
\tightlist
\item Results are from a single codebase (OpenHCS)
\item Other codebases may have different characteristics
\item Mitigation: OpenHCS is representative of scientific Python applications
\end{itemize}

\textbf{Construct validity:}
\begin{itemize}
\tightlist
\item DOF may not capture all aspects of modification complexity
\item Other factors (code readability, performance) are not measured
\item Mitigation: DOF is a lower bound on modification complexity
\end{itemize}

%==============================================================================
