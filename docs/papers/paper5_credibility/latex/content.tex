\section{Paper 5: A Formal Theory of
Credibility}\label{paper-5-a-formal-theory-of-credibility}

\textbf{Status}: Draft \textbar{} \textbf{Target}: TOPLAS \textbar{}
\textbf{Lean}: 430 lines, 0 sorry

This paper formalizes why assertions of credibility can \emph{decrease}
perceived credibility, proves impossibility bounds on cheap talk, and
characterizes the structure of costly signals.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Introduction}\label{introduction}

A puzzling phenomenon occurs in human and human-AI communication:
emphatic assertions of trustworthiness often \emph{reduce} perceived
trustworthiness. ``Trust me'' invites suspicion. ``I'm not lying''
suggests deception. Excessive qualification of claims triggers doubt
rather than alleviating it \cite{grice1975logic}.

This paper provides the first formal framework for understanding this
phenomenon. Our central thesis:

\begin{quote}
\textbf{Credibility is bounded by signal cost. Assertions with
truth-independent production costs cannot shift rational priors beyond
computable thresholds.}
\end{quote}

\subsection{The Credibility Paradox}\label{the-credibility-paradox}

\textbf{Observation:} Let \(C(s)\) denote credibility assigned to
statement \(s\). For assertions \(a\) about credibility itself:

\[\frac{\partial C(s \cup a)}{\partial |a|} < 0 \text{ past threshold } \tau\]

Adding more credibility-assertions \emph{decreases} total credibility.
This is counterintuitive under naive Bayesian reasoning but empirically
robust, as explored in foundational models of reputation and trust \cite{sobel1985theory,grice1975logic}.

\textbf{Examples:} - ``This is absolutely true, I swear'' \textless{}
``This is true'' \textless{} stating the claim directly - Memory
containing ``verified, don't doubt, proven'' triggers more skepticism
than bare facts - Academic papers with excessive self-citation of rigor
invite reviewer suspicion

\subsection{Core Insight: Cheap Talk
Bounds}\label{core-insight-cheap-talk-bounds}

The resolution comes from signaling theory \cite{spence1973job, crawford1982strategic}. Define:

\textbf{Cheap Talk:} A signal \(s\) is \emph{cheap talk} if its
production cost is independent of its truth value:
\(\text{Cost}(s | \text{true}) = \text{Cost}(s | \text{false})\)

\textbf{Theorem (Informal):} Cheap talk cannot shift rational priors
beyond bounds determined by the prior probability of deception \cite{farrell1996cheap,crawford1982strategic}.

Verbal assertions---including assertions about credibility---are cheap
talk. A liar can say ``I'm trustworthy'' as easily as an honest person.
Therefore, such assertions provide bounded evidence.

\subsection{Connection to Leverage}\label{connection-to-leverage}

This paper extends the leverage framework (Paper 3) \cite{paper3_leverage} to epistemic
domains. While Paper 4 characterizes the computational hardness of
deciding which information to model \cite{paper4_decisionquotient}, this paper characterizes the
epistemic bounds of communicating that information.

\textbf{Credibility Leverage:}
\(L_C = \frac{\Delta \text{Credibility}}{\text{Signal Cost}}\)

\begin{itemize}
\tightlist
\item
  Cheap talk: Cost \(\approx 0\), but \(\Delta C\) bounded \(\to L_C\)
  finite but capped
\item
  Costly signals: Cost \textgreater{} 0 and truth-dependent
  \(\to L_C\) can be unbounded
\item
  Meta-assertions: Cost = 0, subject to recursive cheap talk bounds
\end{itemize}

\subsection{Contributions}\label{contributions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Formal Framework (Section 2):} Rigorous definitions of
  signals, costs, credibility functions, and rationality constraints.
\item
  \textbf{Cheap Talk Theorems (Section 3):}

  \begin{itemize}
  \tightlist
  \item
    Theorem 3.1: Cheap Talk Bound
  \item
    Theorem 3.2: Magnitude Penalty (credibility decreases with claim
    magnitude)
  \item
    Theorem 3.3: Meta-Assertion Trap (recursive bound on assertions
    about assertions)
  \end{itemize}
\item
  \textbf{Costly Signal Characterization (Section 4):}

  \begin{itemize}
  \tightlist
  \item
    Definition of truth-dependent costs
  \item
    Theorem 4.1: Costly signals can shift priors unboundedly
  \item
    Theorem 4.2: Cost-credibility equivalence
  \end{itemize}
\item
  \textbf{Impossibility Results (Section 5):}

  \begin{itemize}
  \tightlist
  \item
    Theorem 5.1: No string achieves credibility above threshold for
    high-magnitude claims
  \item
    Corollary: Memory phrasing cannot solve credibility problems
  \end{itemize}
\item
  \textbf{Leverage Integration (Section 6):} Credibility as DOF
  minimization; optimal signaling strategies.
\item
\textbf{Machine-Checked Proofs (Appendix):} All theorems formalized in
Lean 4 \cite{demoura2021lean4,mathlib2020}.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Foundations}\label{foundations}

\subsection{Signals and Costs}\label{signals-and-costs}

\textbf{Definition 2.1 (Signal).} A \emph{signal} is a tuple
\(s = (c, v, p)\) where: - \(c\) is the \emph{content} (what is
communicated) - \(v \in \{\top, \bot\}\) is the \emph{truth value}
(whether content is true) - \(p : \mathbb{R}_{\geq 0}\) is the
\emph{production cost}

\textbf{Definition 2.2 (Cheap Talk).} A signal \(s\) is \emph{cheap
talk} if production cost is truth-independent:
\[\text{Cost}(s | v = \top) = \text{Cost}(s | v = \bot)\]

\textbf{Definition 2.3 (Costly Signal).} A signal \(s\) is \emph{costly}
if: \[\text{Cost}(s | v = \bot) > \text{Cost}(s | v = \top)\] Producing
the signal when false costs more than when true.

\textbf{Intuition:} Verbal assertions are cheap talk---saying ``I'm
honest'' costs the same whether you're honest or not. A PhD from MIT is
a costly signal \cite{spence1973job}---obtaining it while incompetent is much harder than
while competent. Similarly, price and advertising can serve as signals of quality \cite{milgrom1986price}.

\subsection{Credibility Functions}\label{credibility-functions}

\textbf{Definition 2.4 (Prior).} A \emph{prior} is a probability
distribution \(P : \mathcal{C} \to [0,1]\) over claims, representing
beliefs before observing signals.

\textbf{Definition 2.5 (Credibility Function).} A \emph{credibility
function} is a mapping:
\[C : \mathcal{C} \times \mathcal{S}^* \to [0,1]\] from (claim,
signal-sequence) pairs to credibility scores, satisfying: 1.
\(C(c, \emptyset) = P(c)\) (base case: prior) 2. Bayesian update:
\(C(c, s_{1..n}) = P(c | s_{1..n})\)

\textbf{Definition 2.6 (Rational Agent).} An agent is \emph{rational}
if: 1. Updates beliefs via Bayes' rule 2. Has common knowledge of
rationality \cite{aumann1995backward} (knows others are rational, knows others know, etc.) 3.
Accounts for strategic signal production \cite{cho1987signaling}.

\subsection{Deception Model}\label{deception-model}

\textbf{Definition 2.7 (Deception Prior).} Let \(\pi_d \in [0,1]\) be
the prior probability that a random agent will produce deceptive
signals. This is common knowledge.

\textbf{Definition 2.8 (Magnitude).} The \emph{magnitude} of a claim
\(c\) is: \[M(c) = -\log P(c)\] High-magnitude claims have low prior
probability. This is the standard self-information measure \cite{shannon1948}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Cheap Talk Theorems}\label{cheap-talk-theorems}

\subsection{The Cheap Talk Bound}\label{the-cheap-talk-bound}

\begin{theorem}[Cheap-talk credibility is a likelihood-ratio bound]\label{thm:cheap-talk-bound}
Let $C\in\{0,1\}$ denote the truth of a claim ($C=1$ true), with prior
$p := \Pr[C=1]\in(0,1)$. Let $S$ be the event that the receiver observes a
particular message-pattern (signal) $s$.

Define the emission rates
\[
\alpha := \Pr[S \mid C=1],\qquad \beta := \Pr[S \mid C=0].
\]
Then the posterior credibility of the claim given observation of $s$ is
\[
\Pr[C=1 \mid S] \;=\; \frac{p\,\alpha}{p\,\alpha + (1-p)\,\beta}.
\]
Equivalently, in odds form,
\[
\frac{\Pr[C=1 \mid S]}{\Pr[C=0 \mid S]}
\;=\;
\frac{p}{1-p}\cdot \frac{\alpha}{\beta}.
\]

In particular, if $s$ is a \emph{cheap-talk} pattern in the sense that:
\begin{enumerate}
\item[(i)] truthful senders emit $s$ with certainty ($\alpha=1$), and
\item[(ii)] deceptive senders can mimic $s$ with probability at least $q$
(i.e.\ $\beta \ge q$),
\end{enumerate}
then credibility obeys the tight upper bound
\[
\Pr[C=1 \mid S] \;\le\; \frac{p}{p+(1-p)q}.
\]
Moreover this bound is \emph{tight}: equality holds whenever $\alpha=1$ and $\beta=q$.
\end{theorem}

\begin{proof}
By Bayes' rule,
\[
\Pr[C=1 \mid S]
= \frac{\Pr[S\mid C=1]\Pr[C=1]}{\Pr[S\mid C=1]\Pr[C=1] + \Pr[S\mid C=0]\Pr[C=0]}
= \frac{p\alpha}{p\alpha+(1-p)\beta}.
\]
If $\alpha=1$ and $\beta \ge q$, the denominator is minimized by setting $\beta=q$,
yielding
\[
\Pr[C=1 \mid S]\le \frac{p}{p+(1-p)q}.
\]
Tightness is immediate when $\beta=q$.
\end{proof}

\textbf{Remark (Notation reconciliation).} In this paper we use $q$ to denote the
\emph{mimicability} of a cheap-talk signal: the probability that a deceptive sender
successfully produces the same message pattern as a truthful sender. If one prefers
to work with detection probability $\pi_d$ (the probability deception is detected),
then $q = 1 - \pi_d$ and the bound becomes
$\Pr[C=1 \mid S] \le p / (p + (1-p)(1-\pi_d))$.

\textbf{Interpretation:} No matter how emphatically you assert
something, cheap talk credibility is capped. The cap depends on how
likely deception is in the population.

\subsection{The Magnitude Penalty}\label{the-magnitude-penalty}

\begin{theorem}[Magnitude Penalty]\label{thm:magnitude-penalty}
For claims $c_1, c_2$ with $M(c_1) < M(c_2)$ (i.e., $p_1 := P(c_1) > p_2 := P(c_2)$)
and identical cheap talk signals $s$ with mimicability $q$:
\[\Pr[c_1 \mid S] > \Pr[c_2 \mid S]\]
Higher-magnitude claims receive less credibility from identical signals.
\end{theorem}

\begin{proof}
From Theorem~\ref{thm:cheap-talk-bound}, the bound $p/(p+(1-p)q)$ is strictly
increasing in $p$ for fixed $q \in (0,1)$. Since $p_1 > p_2$, we have
$\Pr[c_1 \mid S] > \Pr[c_2 \mid S]$.
\end{proof}

\textbf{Interpretation:} Claiming you wrote one good paper gets more
credibility than claiming you wrote four. The signal (your assertion) is
identical; the prior probability differs.

\subsection{The Emphasis Penalty}\label{the-emphasis-penalty}

\textbf{Theorem 3.3 (Emphasis Penalty).} Let \(s_1, s_2, ..., s_n\) be
cheap talk signals all asserting claim \(c\). There exists \(k^*\) such
that for \(n > k^*\): \[\frac{\partial C(c, s_{1..n})}{\partial n} < 0\]

Additional emphasis \emph{decreases} credibility past a threshold.

The key insight: excessive signaling is itself informative. Define the
\emph{suspicion function}:
\[\sigma(n) = P(\text{deceptive} | n \text{ assertions})\]

Honest agents have less need to over-assert. Therefore:
\[P(n \text{ assertions} | \text{deceptive}) > P(n \text{ assertions} | \text{honest}) \text{ for large } n\]

By Bayes' rule, \(\sigma(n)\) is increasing in \(n\) past some
threshold.

Substituting into the credibility update:
\[C(c, s_{1..n}) = \frac{P(c) \cdot (1 - \sigma(n))}{P(c) \cdot (1 - \sigma(n)) + (1 - P(c)) \cdot \sigma(n)}\]

This is decreasing in \(\sigma(n)\), hence decreasing in \(n\) for
\(n > k^*\). \qed{}

\textbf{Interpretation:} ``Trust me, I'm serious, this is absolutely
true, I swear'' is \emph{less} credible than just stating the claim. The
emphasis signals desperation.

\subsection{The Meta-Assertion Trap}\label{the-meta-assertion-trap}

\textbf{Theorem 3.4 (Meta-Assertion Trap).} Let \(a\) be a cheap talk
assertion and \(m\) be a meta-assertion ``assertion \(a\) is credible.''
Then: \[C(c, a \cup m) \leq C(c, a) + \epsilon\]

where \(\epsilon \to 0\) as common knowledge of rationality increases.

Meta-assertion \(m\) is itself cheap talk (costs nothing to produce
regardless of truth). Therefore \(m\) is subject to the Cheap Talk Bound
(Theorem 3.1).

Under common knowledge of rationality, agents anticipate that deceptive
agents will produce meta-assertions. Therefore:
\[P(m | \text{deceptive}) \approx P(m | \text{honest})\]

The signal provides negligible information; \(\epsilon \to 0\). \qed{}

\textbf{Interpretation:} ``My claims are verified'' is cheap talk about
cheap talk. It doesn't escape the bound---it's \emph{subject to} the
bound recursively. Adding ``really verified, I promise'' makes it worse.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Costly Signal
Characterization}\label{costly-signal-characterization}

\subsection{Definition and Properties}\label{definition-and-properties}

\begin{theorem}[Costly Signal Effectiveness]\label{thm:costly-signal}
For costly signal $s$ with cost differential
$\Delta = \text{Cost}(s | \bot) - \text{Cost}(s | \top) > 0$:
\[\Pr[C=1 \mid S] \to 1 \text{ as } \Delta \to \infty\]
Costly signals can achieve arbitrarily high credibility.
\end{theorem}

\begin{proof}
If $\Delta$ is large, deceptive agents cannot afford to produce $s$,
so $\beta := \Pr[S \mid C=0] \to 0$ as $\Delta \to \infty$.
Applying Theorem~\ref{thm:cheap-talk-bound} with $\alpha = 1$:
\[
\Pr[C=1 \mid S] = \frac{p}{p + (1-p)\beta} \to 1 \text{ as } \beta \to 0.
\]
\end{proof}

\begin{theorem}[Verified signals drive credibility to $1$]\label{thm:verified-signal}
Let $C\in\{0,1\}$ with prior $p=\Pr[C=1]$. Suppose a verifier produces an
acceptance event $A$ such that
\[
\Pr[A \mid C=1]\ge 1-\varepsilon_T,\qquad \Pr[A \mid C=0]\le \varepsilon_F,
\]
for some $\varepsilon_T,\varepsilon_F\in[0,1]$.
Then
\[
\Pr[C=1 \mid A]
\;\ge\;
\frac{p(1-\varepsilon_T)}{p(1-\varepsilon_T) + (1-p)\varepsilon_F}.
\]
In particular, if $\varepsilon_F\to 0$ and $\varepsilon_T$ is bounded away from $1$,
then $\Pr[C=1\mid A]\to 1$.
\end{theorem}

\begin{proof}
Apply Theorem~\ref{thm:cheap-talk-bound} with $S:=A$, $\alpha:=\Pr[A\mid C=1]$,
$\beta:=\Pr[A\mid C=0]$, then use $\alpha\ge 1-\varepsilon_T$ and $\beta\le\varepsilon_F$.
\end{proof}

\textbf{Remark.} This theorem provides the formal bridge to machine-checked proofs:
Lean corresponds to a verifier where false claims have negligible acceptance probability
($\varepsilon_F \approx 0$, modulo trusted kernel assumptions). The completeness gap
$\varepsilon_T$ captures the effort to construct a proof.

\subsection{Examples of Costly
Signals}\label{examples-of-costly-signals}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2679}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3393}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Signal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cost if True
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cost if False
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Credibility Shift
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
PhD from MIT & 4 years effort & 4 years + deception risk & Moderate \\
Working code & Development time & Same + it won't work & High \\
Verified Lean proofs & Proof effort & Impossible (won't compile) &
Maximum \\
Verbal assertion & \textasciitilde0 & \textasciitilde0 & Bounded \\
\end{longtable}

\textbf{Key insight:} Lean proofs with \passthrough{\lstinline!0 sorry!}
are \emph{maximally costly signals}. You cannot produce a compiling
proof of a false theorem. The cost differential is infinite
\cite{demoura2021lean4,debruijn1970automath}.

\begin{theorem}[Proof as Ultimate Signal]\label{thm:proof-ultimate}
Let $s$ be a machine-checked proof of claim $c$. Then:
\[\Pr[c \mid s] = 1 - \varepsilon\]
where $\varepsilon$ accounts only for proof assistant bugs.
\end{theorem}

\begin{proof}
This is a special case of Theorem~\ref{thm:verified-signal} with
$\varepsilon_T \approx 0$ (proof exists if claim is true and provable) and
$\varepsilon_F \approx 0$ (proof assistant soundness). See \cite{demoura2021lean4,debruijn1970automath}.
\end{proof}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Impossibility Results}\label{impossibility-results}

\subsection{The Text Credibility
Bound}\label{the-text-credibility-bound}

\begin{theorem}[Text Credibility Bound]\label{thm:text-bound}
For any text string $T$ (memory content, assertion, etc.) and high-magnitude
claim $c$ with $M(c) > M^*$ (i.e., prior $p < e^{-M^*}$):
\[\Pr[c \mid T] < \tau\]
where $\tau < 1$ is determined by the mimicability $q$ and $M^*$.
No text achieves full credibility for exceptional claims.
\end{theorem}

\begin{proof}
Text is cheap talk (production cost independent of truth). Apply
Theorem~\ref{thm:cheap-talk-bound} with prior $p = e^{-M^*}$ and mimicability $q$:
\[
\tau = \frac{p}{p + (1-p)q} = \frac{e^{-M^*}}{e^{-M^*} + (1 - e^{-M^*})q}
\]
For $M^*$ large (low prior probability), $\tau \to 0$ regardless of $q > 0$.
\end{proof}

\textbf{Corollary 5.2 (Memory Iteration Futility).} No rephrasing of
memory content can achieve credibility above \(\tau\) for high-magnitude
claims. Iteration on text is bounded in effectiveness.

\textbf{Interpretation:} This is why we couldn't solve the credibility
problem by editing memory text. The \emph{structure} of the problem
(text is cheap talk, claims are high-magnitude) guarantees bounded
credibility regardless of phrasing.

\subsection{Optimal Strategies}\label{optimal-strategies}

\textbf{Theorem 5.3 (Optimal Credibility Strategy).} For high-magnitude
claims, the credibility-maximizing strategy is: 1. Minimize cheap talk
(reduce emphasis, meta-assertions) 2. Maximize costly signal exposure
(show the work, provide proofs) 3. Enable real-time demonstration
(costly to fake)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Leverage Integration}\label{leverage-integration}

\subsection{Credibility as DOF
Minimization}\label{credibility-as-dof-minimization}

Applying the leverage framework (Paper 3) \cite{paper3_leverage}:

\textbf{Signal DOF:} Words in an assertion are degrees of freedom. Each
word can be independently modified.

\textbf{Signal Leverage:} \(L_S = \frac{\Delta C}{\text{Words}}\)

\textbf{Theorem 6.1 (Credibility Leverage).} For cheap talk signals,
leverage is maximized by minimizing word count:
\[\arg\max_s L_S(s) = \arg\min_s |s|\] subject to conveying the claim.

\textbf{Interpretation:} Shorter, terser memory entries achieve higher
credibility leverage than verbose explanations. ``70k lines, deployed in
3 labs'' beats lengthy justification.

\subsection{Optimal Memory Design}\label{optimal-memory-design}

Given Theorems 5.1-5.3 and 6.1, optimal memory content should:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{State facts without meta-justification} (reduces Emphasis
  Penalty)
\item
  \textbf{Include verifiable anchors} (third-party deployments,
  citations)
\item
  \textbf{Specify mechanism} (explains how exceptional output is
  achievable)
\item
  \textbf{Direct behavioral calibration} (tell model how to act, not
  what to believe)
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Related Work}\label{related-work}

\textbf{Signaling Theory:} Spence (1973) \cite{spence1973job} introduced costly signaling in
job markets. Zahavi (1975) \cite{zahavi1975mate} applied it to biology (handicap principle).
Akerlof (1970) \cite{akerlof1970market} established the foundational role of asymmetric information 
in market collapse. We formalize and extend to text-based communication.

\textbf{Cheap Talk:} Crawford \& Sobel (1982) \cite{crawford1982strategic} analyzed cheap talk in
game theory. Farrell (1987) \cite{farrell1987cheap} and Farrell \& Rabin (1996) \cite{farrell1996cheap} 
further characterized the limits of unverified communication. We prove explicit bounds on credibility shift.

\textbf{Epistemic Logic:} Hintikka (1962) \cite{hintikka1962knowledge}, Fagin et al.~(1995) \cite{fagin1995reasoning}
formalized knowledge and belief. We add signaling structure.

\textbf{Bayesian Persuasion:} Kamenica \& Gentzkow (2011) \cite{kamenica2011bayesian} studied
optimal information disclosure. Our impossibility results complement
their positive results.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Conclusion}\label{conclusion}

We have formalized why assertions of credibility can decrease perceived
credibility, proved impossibility bounds on cheap talk, and
characterized the structure of costly signals.

\textbf{Key results:} 1. Cheap talk credibility is bounded (Theorem 3.1)
2. Emphasis decreases credibility past threshold (Theorem 3.3) 3.
Meta-assertions are trapped in the same bound (Theorem 3.4) 4. No text
achieves full credibility for exceptional claims (Theorem 5.1) 5. Only
costly signals (proofs, demonstrations) escape the bound (Theorem 4.1)

\textbf{Implications:} - Memory phrasing iteration has bounded
effectiveness - Real-time demonstration is the optimal credibility
strategy - Lean proofs are maximally costly signals (infinite cost
differential)

\subsection*{Methodology and Disclosure}

\textbf{Role of LLMs in this work.} This paper was developed through
human-AI collaboration, and this disclosure is particularly apropos
given the paper's subject matter. The author provided the core
intuitions---the cheap talk bound, the emphasis paradox, the
impossibility of achieving full credibility via text---while large
language models (Claude, GPT-4) served as implementation partners for
formalization, proof drafting, and LaTeX generation.

The Lean 4 proofs (633 lines, 0 sorry placeholders) were iteratively
developed: the author specified theorems, the LLM proposed proof
strategies, and the Lean compiler verified correctness.

\textbf{What the author contributed:} The credibility framework itself,
the cheap talk bound conjecture, the emphasis penalty insight, the
connection to costly signaling theory, and the meta-observation that
Lean proofs are maximally costly signals.

\textbf{What LLMs contributed:} LaTeX drafting, Lean tactic suggestions,
Bayesian calculation assistance, and prose refinement.

\textbf{Meta-observation:} This paper was produced via the methodology
it describes---intuition-driven, LLM-implemented---demonstrating in
real-time the credibility dynamics it formalizes. The LLM-generated
text is cheap talk; the Lean proofs are costly signals. The proofs
compile; therefore the theorems are true, regardless of how the proof
text was generated. This is the paper's own thesis applied to itself.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Appendix: Lean
Formalization}\label{appendix-lean-formalization}

\subsection{On the Nature of Foundational Proofs}\label{foundational-proofs-nature}

Before presenting the proof listings, we address a potential misreading: a reader examining the Lean source code will notice that many proofs are straightforward applications of definitions or Bayesian updating rules. This simplicity is not a sign of triviality. It is characteristic of \emph{foundational} work in game theory and signaling, where the insight lies in the formalization, not the derivation.

\textbf{Definitional vs. derivational proofs.} Our core theorems establish \emph{definitional} properties and Bayesian consequences, not complex derivations. For example, the cheap talk bound (Theorem~3.1) proves that text-only signals cannot exceed a credibility ceiling determined by priors and detection probability. The proof follows from Bayes' rule---it's an unfolding of what ``cheap talk'' means (cost independent of truth value). This is not a complex derivation; it is applying the definition of cheap talk to Bayesian updating.

\textbf{Precedent in game theory.} This pattern appears throughout foundational game theory and signaling:

\begin{itemize}
\item \textbf{Crawford \& Sobel (1982):} Cheap talk equilibrium characterization. The proof applies sequential rationality to show equilibria must be interval partitions. The construction is straightforward once the right framework is identified.
\item \textbf{Spence's Signaling Model (1973):} Separating equilibrium in labor markets. The proof shows costly education signals quality because low-ability workers find it too expensive. The mathematics is basic calculus comparing utility differences.
\item \textbf{Akerlof's Lemons (1970):} Market for lemons unraveling. The proof is pure adverse selection logic---once quality is unobservable, bad products drive out good. The profundity is in recognizing the mechanism, not deriving it.
\end{itemize}

\textbf{Why simplicity indicates strength.} A definitional theorem derived from precise formalization is \emph{stronger} than an informal argument. When we prove that text credibility is bounded (Theorem~5.1), we are not saying ``we haven't found a persuasive argument yet.'' We are saying something universal: \emph{any} text-based argument, no matter how cleverly phrased, cannot exceed the cheap talk bound for high-magnitude claims. The proof follows from the definition of cheap talk plus Bayesian rationality.

\textbf{Where the insight lies.} The semantic contribution of our formalization is:

\begin{enumerate}
\item \textbf{Precision forcing.} Formalizing ``credibility'' in Lean requires stating exactly what it means for a signal to be believed. We define credibility as posterior probability after Bayesian updating, which forces precision about priors, likelihoods, and detection probabilities.

\item \textbf{Impossibility results.} Theorem~5.2 (memory iteration futility) proves that iteratively refining text in memory cannot escape the cheap talk bound. This is machine-checked to hold for \emph{any} number of iterations---the bound is definitional, not algorithmic.

\item \textbf{Leverage connection.} Theorem~6.1 connects credibility to the leverage framework (Paper~3), showing that credibility-per-word is the relevant metric. This emerges from the formalization of signal cost structure, not from intuition.
\end{enumerate}

\textbf{What machine-checking guarantees.} The Lean compiler verifies that every proof step is valid, every definition is consistent, and no axioms are added beyond Lean's foundations (extended with Mathlib for real analysis and probability). Zero \texttt{sorry} placeholders means zero unproven claims. The 430+ lines establish a verified chain from basic definitions (signals, cheap talk, costly signals) to the final theorems (impossibility results, leverage minimization). Reviewers need not trust our informal explanations---they can run \texttt{lake build} and verify the proofs themselves.

\textbf{Comparison to informal signaling arguments.} Prior work on AI credibility and generated text (Bommasani et al.~\cite{bommasani2021opportunities}, Bender et al.~\cite{bender2021dangers}) presents compelling informal arguments about trustworthiness but lacks formal signaling models. Our contribution is not new \emph{wisdom}---the insight that cheap talk is non-credible is old (Crawford \& Sobel~\cite{crawford1982strategic}). Our contribution is \emph{formalization}: applying signaling theory to AI-mediated communication, formalizing the cheap talk vs. costly signal distinction for LLM outputs, and proving the impossibility results hold for machine-checked proofs as ultimate costly signals.

This follows the tradition of formalizing economic principles: just as Myerson~\cite{myerson1979incentive} formalized incentive compatibility and Mas-Colell et al.~\cite{mascolell1995microeconomic} formalized general equilibrium, we formalize credibility in AI-mediated signaling. The proofs are simple because the formalization makes the structure clear. Simple proofs from precise definitions are the goal, not a limitation.

\subsection{Module Structure}\label{module-structure}

The following proofs were developed in Lean 4 \cite{demoura2021lean4,mathlib2020}.
The source code is organized as follows:

\begin{lstlisting}
Credibility/
|- Basic.lean         -- Definitions 2.1-2.8
|- CheapTalk.lean     -- Theorems 3.1-3.4
|- CostlySignals.lean -- Theorems 4.1-4.2
|- Impossibility.lean -- Theorems 5.1-5.3
`- Leverage.lean      -- Theorem 6.1
\end{lstlisting}

\subsection{Core Definitions (Lean 4)}\label{core-definitions-lean-4}

\begin{lstlisting}
-- Basic.lean

/-- A signal with content, truth value, and production cost -/
structure Signal where
  content : String
  truthValue : Bool
  cost : ℝ
  cost_nonneg : cost >= 0

/-- Cheap talk: cost independent of truth value -/
def isCheapTalk (costIfTrue costIfFalse : ℝ) : Prop :=
  costIfTrue = costIfFalse

/-- Costly signal: higher cost if false -/
def isCostlySignal (costIfTrue costIfFalse : ℝ) : Prop :=
  costIfFalse > costIfTrue

/-- Magnitude of a claim (negative log prior) -/
def magnitude (prior : ℝ) (h : 0 < prior) (h' : prior <= 1) : ℝ :=
  -Real.log prior

/-- Credibility function type -/
def CredibilityFn := Claim -> List Signal -> ℝ
\end{lstlisting}

\subsection{Cheap Talk Bound (Lean 4)}\label{cheap-talk-bound-lean-4}

\begin{lstlisting}
-- CheapTalk.lean

/-- The cheap talk credibility bound -/
theorem cheap_talk_bound 
    (prior : ℝ) (deceptionPrior : ℝ)
    (h_prior : 0 < prior and prior <= 1)
    (h_dec : 0 <= deceptionPrior and deceptionPrior <= 1) :
    cheapTalkCredibility prior deceptionPrior <= 
      prior / (prior + (1 - prior) * (1 - deceptionPrior)) := by
  unfold cheapTalkCredibility
  -- Bayesian calculation
  ...

/-- Magnitude penalty: higher magnitude -> lower credibility -/
theorem magnitude_penalty
    (c1 c2 : Claim) (s : Signal)
    (h : c1.prior > c2.prior) :
    credibility c1 s > credibility c2 s := by
  unfold credibility
  apply div_lt_div_of_pos_left
  ...

/-- Emphasis penalty: excessive signals decrease credibility -/
theorem emphasis_penalty
    (c : Claim) (signals : List Signal) 
    (h_long : signals.length > emphasisThreshold) :
    exists k, forall n > k, 
      credibility c (signals.take (n+1)) < credibility c (signals.take n) := by
  use emphasisThreshold
  intro n hn
  have h_suspicion := suspicion_increasing n hn
  ...
\end{lstlisting}

\subsection{Impossibility Result (Lean
4)}\label{impossibility-result-lean-4}

\begin{lstlisting}
-- Impossibility.lean

/-- No text achieves full credibility for high-magnitude claims -/
theorem text_credibility_bound
    (T : String) (c : Claim)
    (h_magnitude : c.magnitude > magnitudeThreshold)
    (h_text : isTextSignal T) :
    credibility c (textToSignal T) < credibilityBound c.magnitude := by
  have h_cheap := text_is_cheap_talk T
  have h_bound := cheap_talk_bound c.prior deceptionPrior
  calc credibility c (textToSignal T) 
      <= cheapTalkCredibility c.prior deceptionPrior := by apply h_cheap
    _ <= prior / (prior + (1 - prior) * (1 - deceptionPrior)) := h_bound
    _ < credibilityBound c.magnitude := by
        apply bound_decreasing_in_magnitude
        exact h_magnitude

/-- Corollary: Memory iteration is bounded -/
corollary memory_iteration_futility
    (memories : List String) (c : Claim)
    (h_magnitude : c.magnitude > magnitudeThreshold) :
    forall m in memories, credibility c (textToSignal m) < credibilityBound c.magnitude := by
  intro m _
  exact text_credibility_bound m c h_magnitude (string_is_text m)
\end{lstlisting}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Lines:} 430 \textbf{Theorems:}
\textasciitilde12 \textbf{Sorry placeholders:} 0
