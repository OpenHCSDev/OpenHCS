# Paper 5: A Formal Theory of Credibility

**Status**: TOPLAS-ready | **Lean**: 430 lines, ~12 theorems, 0 sorry

This paper formalizes the Credibility Paradox and signaling bounds.

---

# Paper 5: A Formal Theory of Credibility

**Status**: Draft **Target**: TOPLAS **Lean**: 430 lines, 0 sorry

This paper formalizes why assertions of credibility can *decrease* perceived credibility, proves impossibility bounds on cheap talk, and characterizes the structure of costly signals.

::: center

----------------------------------------------------------------------------------------------------
:::

# Introduction

A puzzling phenomenon occurs in human and human-AI communication: emphatic assertions of trustworthiness often *reduce* perceived trustworthiness. "Trust me" invites suspicion. "I'm not lying" suggests deception. Excessive qualification of claims triggers doubt rather than alleviating it [@grice1975logic].

This paper provides the first formal framework for understanding this phenomenon. Our central thesis:

> **Credibility is bounded by signal cost. Assertions with truth-independent production costs cannot shift rational priors beyond computable thresholds.**

## The Credibility Paradox

**Observation:** Let $C(s)$ denote credibility assigned to statement $s$. For assertions $a$ about credibility itself:

$$\frac{\partial C(s \cup a)}{\partial |a|} < 0 \text{ past threshold } \tau$$

Adding more credibility-assertions *decreases* total credibility. This is counterintuitive under naive Bayesian reasoning but empirically robust, as explored in foundational models of reputation and trust [@sobel1985theory; @grice1975logic].

**Examples:** - "This is absolutely true, I swear" \< "This is true" \< stating the claim directly - Memory containing "verified, don't doubt, proven" triggers more skepticism than bare facts - Academic papers with excessive self-citation of rigor invite reviewer suspicion

## Core Insight: Cheap Talk Bounds

The resolution comes from signaling theory [@spence1973job; @crawford1982strategic]. Define:

**Cheap Talk:** A signal $s$ is *cheap talk* if its production cost is independent of its truth value: $\text{Cost}(s | \text{true}) = \text{Cost}(s | \text{false})$

**Theorem (Informal):** Cheap talk cannot shift rational priors beyond bounds determined by the prior probability of deception [@farrell1996cheap; @crawford1982strategic].

Verbal assertions---including assertions about credibility---are cheap talk. A liar can say "I'm trustworthy" as easily as an honest person. Therefore, such assertions provide bounded evidence.

## Connection to Leverage

This paper extends the leverage framework (Paper 3) [@paper3_leverage] to epistemic domains. While Paper 4 characterizes the computational hardness of deciding which information to model [@paper4_decisionquotient], this paper characterizes the epistemic bounds of communicating that information.

**Credibility Leverage:** $L_C = \frac{\Delta \text{Credibility}}{\text{Signal Cost}}$

-   Cheap talk: Cost $\approx 0$, but $\Delta C$ bounded $\to L_C$ finite but capped

-   Costly signals: Cost \> 0 and truth-dependent $\to L_C$ can be unbounded

-   Meta-assertions: Cost = 0, subject to recursive cheap talk bounds

## Contributions

1.  **Formal Framework (Section 2):** Rigorous definitions of signals, costs, credibility functions, and rationality constraints.

2.  **Cheap Talk Theorems (Section 3):**

    -   Theorem 3.1: Cheap Talk Bound

    -   Theorem 3.2: Magnitude Penalty (credibility decreases with claim magnitude)

    -   Theorem 3.3: Meta-Assertion Trap (recursive bound on assertions about assertions)

3.  **Costly Signal Characterization (Section 4):**

    -   Definition of truth-dependent costs

    -   Theorem 4.1: Costly signals can shift priors unboundedly

    -   Theorem 4.2: Cost-credibility equivalence

4.  **Impossibility Results (Section 5):**

    -   Theorem 5.1: No string achieves credibility above threshold for high-magnitude claims

    -   Corollary: Memory phrasing cannot solve credibility problems

5.  **Leverage Integration (Section 6):** Credibility as DOF minimization; optimal signaling strategies.

6.  **Machine-Checked Proofs (Appendix):** All theorems formalized in Lean 4 [@demoura2021lean4; @mathlib2020].

::: center

----------------------------------------------------------------------------------------------------
:::

# Foundations

## Signals and Costs

**Definition 2.1 (Signal).** A *signal* is a tuple $s = (c, v, p)$ where: - $c$ is the *content* (what is communicated) - $v \in \{\top, \bot\}$ is the *truth value* (whether content is true) - $p : \mathbb{R}_{\geq 0}$ is the *production cost*

**Definition 2.2 (Cheap Talk).** A signal $s$ is *cheap talk* if production cost is truth-independent: $$\text{Cost}(s | v = \top) = \text{Cost}(s | v = \bot)$$

**Definition 2.3 (Costly Signal).** A signal $s$ is *costly* if: $$\text{Cost}(s | v = \bot) > \text{Cost}(s | v = \top)$$ Producing the signal when false costs more than when true.

**Intuition:** Verbal assertions are cheap talk---saying "I'm honest" costs the same whether you're honest or not. A PhD from MIT is a costly signal [@spence1973job]---obtaining it while incompetent is much harder than while competent. Similarly, price and advertising can serve as signals of quality [@milgrom1986price].

## Credibility Functions

**Definition 2.4 (Prior).** A *prior* is a probability distribution $P : \mathcal{C} \to [0,1]$ over claims, representing beliefs before observing signals.

**Definition 2.5 (Credibility Function).** A *credibility function* is a mapping: $$C : \mathcal{C} \times \mathcal{S}^* \to [0,1]$$ from (claim, signal-sequence) pairs to credibility scores, satisfying: 1. $C(c, \emptyset) = P(c)$ (base case: prior) 2. Bayesian update: $C(c, s_{1..n}) = P(c | s_{1..n})$

**Definition 2.6 (Rational Agent).** An agent is *rational* if: 1. Updates beliefs via Bayes' rule 2. Has common knowledge of rationality [@aumann1995backward] (knows others are rational, knows others know, etc.) 3. Accounts for strategic signal production [@cho1987signaling].

## Deception Model

**Definition 2.7 (Deception Prior).** Let $\pi_d \in [0,1]$ be the prior probability that a random agent will produce deceptive signals. This is common knowledge.

**Definition 2.8 (Magnitude).** The *magnitude* of a claim $c$ is: $$M(c) = -\log P(c)$$ High-magnitude claims have low prior probability. This is the standard self-information measure [@shannon1948].

::: center

----------------------------------------------------------------------------------------------------
:::

# Cheap Talk Theorems

## The Cheap Talk Bound

::: theorem
[]{#thm:cheap-talk-bound label="thm:cheap-talk-bound"} Let $C\in\{0,1\}$ denote the truth of a claim ($C=1$ true), with prior $p := \Pr[C=1]\in(0,1)$. Let $S$ be the event that the receiver observes a particular message-pattern (signal) $s$.

Define the emission rates $$\alpha := \Pr[S \mid C=1],\qquad \beta := \Pr[S \mid C=0].$$ Then the posterior credibility of the claim given observation of $s$ is $$\Pr[C=1 \mid S] \;=\; \frac{p\,\alpha}{p\,\alpha + (1-p)\,\beta}.$$ Equivalently, in odds form, $$\frac{\Pr[C=1 \mid S]}{\Pr[C=0 \mid S]}
\;=\;
\frac{p}{1-p}\cdot \frac{\alpha}{\beta}.$$

In particular, if $s$ is a *cheap-talk* pattern in the sense that:

1.  truthful senders emit $s$ with certainty ($\alpha=1$), and

2.  deceptive senders can mimic $s$ with probability at least $q$ (i.e. $\beta \ge q$),

then credibility obeys the tight upper bound $$\Pr[C=1 \mid S] \;\le\; \frac{p}{p+(1-p)q}.$$ Moreover this bound is *tight*: equality holds whenever $\alpha=1$ and $\beta=q$.
:::

::: proof
*Proof.* By Bayes' rule, $$\Pr[C=1 \mid S]
= \frac{\Pr[S\mid C=1]\Pr[C=1]}{\Pr[S\mid C=1]\Pr[C=1] + \Pr[S\mid C=0]\Pr[C=0]}
= \frac{p\alpha}{p\alpha+(1-p)\beta}.$$ If $\alpha=1$ and $\beta \ge q$, the denominator is minimized by setting $\beta=q$, yielding $$\Pr[C=1 \mid S]\le \frac{p}{p+(1-p)q}.$$ Tightness is immediate when $\beta=q$. ◻
:::

**Remark (Notation reconciliation).** In this paper we use $q$ to denote the *mimicability* of a cheap-talk signal: the probability that a deceptive sender successfully produces the same message pattern as a truthful sender. If one prefers to work with detection probability $\pi_d$ (the probability deception is detected), then $q = 1 - \pi_d$ and the bound becomes $\Pr[C=1 \mid S] \le p / (p + (1-p)(1-\pi_d))$.

**Interpretation:** No matter how emphatically you assert something, cheap talk credibility is capped. The cap depends on how likely deception is in the population.

## The Magnitude Penalty

::: theorem
[]{#thm:magnitude-penalty label="thm:magnitude-penalty"} For claims $c_1, c_2$ with $M(c_1) < M(c_2)$ (i.e., $p_1 := P(c_1) > p_2 := P(c_2)$) and identical cheap talk signals $s$ with mimicability $q$: $$\Pr[c_1 \mid S] > \Pr[c_2 \mid S]$$ Higher-magnitude claims receive less credibility from identical signals.
:::

::: proof
*Proof.* From Theorem [\[thm:cheap-talk-bound\]](#thm:cheap-talk-bound){reference-type="ref" reference="thm:cheap-talk-bound"}, the bound $p/(p+(1-p)q)$ is strictly increasing in $p$ for fixed $q \in (0,1)$. Since $p_1 > p_2$, we have $\Pr[c_1 \mid S] > \Pr[c_2 \mid S]$. ◻
:::

**Interpretation:** Claiming you wrote one good paper gets more credibility than claiming you wrote four. The signal (your assertion) is identical; the prior probability differs.

## The Emphasis Penalty

**Theorem 3.3 (Emphasis Penalty).** Let $s_1, s_2, ..., s_n$ be cheap talk signals all asserting claim $c$. There exists $k^*$ such that for $n > k^*$: $$\frac{\partial C(c, s_{1..n})}{\partial n} < 0$$

Additional emphasis *decreases* credibility past a threshold.

The key insight: excessive signaling is itself informative. Define the *suspicion function*: $$\sigma(n) = P(\text{deceptive} | n \text{ assertions})$$

Honest agents have less need to over-assert. Therefore: $$P(n \text{ assertions} | \text{deceptive}) > P(n \text{ assertions} | \text{honest}) \text{ for large } n$$

By Bayes' rule, $\sigma(n)$ is increasing in $n$ past some threshold.

Substituting into the credibility update: $$C(c, s_{1..n}) = \frac{P(c) \cdot (1 - \sigma(n))}{P(c) \cdot (1 - \sigma(n)) + (1 - P(c)) \cdot \sigma(n)}$$

This is decreasing in $\sigma(n)$, hence decreasing in $n$ for $n > k^*$. 0◻

**Interpretation:** "Trust me, I'm serious, this is absolutely true, I swear" is *less* credible than just stating the claim. The emphasis signals desperation.

## The Meta-Assertion Trap

**Theorem 3.4 (Meta-Assertion Trap).** Let $a$ be a cheap talk assertion and $m$ be a meta-assertion "assertion $a$ is credible." Then: $$C(c, a \cup m) \leq C(c, a) + \epsilon$$

where $\epsilon \to 0$ as common knowledge of rationality increases.

Meta-assertion $m$ is itself cheap talk (costs nothing to produce regardless of truth). Therefore $m$ is subject to the Cheap Talk Bound (Theorem 3.1).

Under common knowledge of rationality, agents anticipate that deceptive agents will produce meta-assertions. Therefore: $$P(m | \text{deceptive}) \approx P(m | \text{honest})$$

The signal provides negligible information; $\epsilon \to 0$. 0◻

**Interpretation:** "My claims are verified" is cheap talk about cheap talk. It doesn't escape the bound---it's *subject to* the bound recursively. Adding "really verified, I promise" makes it worse.

::: center

----------------------------------------------------------------------------------------------------
:::

# Costly Signal Characterization

## Definition and Properties

::: theorem
[]{#thm:costly-signal label="thm:costly-signal"} For costly signal $s$ with cost differential $\Delta = \text{Cost}(s | \bot) - \text{Cost}(s | \top) > 0$: $$\Pr[C=1 \mid S] \to 1 \text{ as } \Delta \to \infty$$ Costly signals can achieve arbitrarily high credibility.
:::

::: proof
*Proof.* If $\Delta$ is large, deceptive agents cannot afford to produce $s$, so $\beta := \Pr[S \mid C=0] \to 0$ as $\Delta \to \infty$. Applying Theorem [\[thm:cheap-talk-bound\]](#thm:cheap-talk-bound){reference-type="ref" reference="thm:cheap-talk-bound"} with $\alpha = 1$: $$\Pr[C=1 \mid S] = \frac{p}{p + (1-p)\beta} \to 1 \text{ as } \beta \to 0.$$ ◻
:::

::: theorem
[]{#thm:verified-signal label="thm:verified-signal"} Let $C\in\{0,1\}$ with prior $p=\Pr[C=1]$. Suppose a verifier produces an acceptance event $A$ such that $$\Pr[A \mid C=1]\ge 1-\varepsilon_T,\qquad \Pr[A \mid C=0]\le \varepsilon_F,$$ for some $\varepsilon_T,\varepsilon_F\in[0,1]$. Then $$\Pr[C=1 \mid A]
\;\ge\;
\frac{p(1-\varepsilon_T)}{p(1-\varepsilon_T) + (1-p)\varepsilon_F}.$$ In particular, if $\varepsilon_F\to 0$ and $\varepsilon_T$ is bounded away from $1$, then $\Pr[C=1\mid A]\to 1$.
:::

::: proof
*Proof.* Apply Theorem [\[thm:cheap-talk-bound\]](#thm:cheap-talk-bound){reference-type="ref" reference="thm:cheap-talk-bound"} with $S:=A$, $\alpha:=\Pr[A\mid C=1]$, $\beta:=\Pr[A\mid C=0]$, then use $\alpha\ge 1-\varepsilon_T$ and $\beta\le\varepsilon_F$. ◻
:::

**Remark.** This theorem provides the formal bridge to machine-checked proofs: Lean corresponds to a verifier where false claims have negligible acceptance probability ($\varepsilon_F \approx 0$, modulo trusted kernel assumptions). The completeness gap $\varepsilon_T$ captures the effort to construct a proof.

## Examples of Costly Signals

  Signal                 Cost if True       Cost if False                Credibility Shift
  ---------------------- ------------------ ---------------------------- -------------------
  PhD from MIT           years effort       years + deception risk       Moderate
  Working code           Development time   Same + it won't work         High
  Verified Lean proofs   Proof effort       Impossible (won't compile)   Maximum
  Verbal assertion       \~0                \~0                          Bounded

**Key insight:** Lean proofs with `0 sorry` are *maximally costly signals*. You cannot produce a compiling proof of a false theorem. The cost differential is infinite [@demoura2021lean4; @debruijn1970automath].

::: theorem
[]{#thm:proof-ultimate label="thm:proof-ultimate"} Let $s$ be a machine-checked proof of claim $c$. Then: $$\Pr[c \mid s] = 1 - \varepsilon$$ where $\varepsilon$ accounts only for proof assistant bugs.
:::

::: proof
*Proof.* This is a special case of Theorem [\[thm:verified-signal\]](#thm:verified-signal){reference-type="ref" reference="thm:verified-signal"} with $\varepsilon_T \approx 0$ (proof exists if claim is true and provable) and $\varepsilon_F \approx 0$ (proof assistant soundness). See [@demoura2021lean4; @debruijn1970automath]. ◻
:::

::: center

----------------------------------------------------------------------------------------------------
:::

# Impossibility Results

## The Text Credibility Bound

::: theorem
[]{#thm:text-bound label="thm:text-bound"} For any text string $T$ (memory content, assertion, etc.) and high-magnitude claim $c$ with $M(c) > M^*$ (i.e., prior $p < e^{-M^*}$): $$\Pr[c \mid T] < \tau$$ where $\tau < 1$ is determined by the mimicability $q$ and $M^*$. No text achieves full credibility for exceptional claims.
:::

::: proof
*Proof.* Text is cheap talk (production cost independent of truth). Apply Theorem [\[thm:cheap-talk-bound\]](#thm:cheap-talk-bound){reference-type="ref" reference="thm:cheap-talk-bound"} with prior $p = e^{-M^*}$ and mimicability $q$: $$\tau = \frac{p}{p + (1-p)q} = \frac{e^{-M^*}}{e^{-M^*} + (1 - e^{-M^*})q}$$ For $M^*$ large (low prior probability), $\tau \to 0$ regardless of $q > 0$. ◻
:::

**Corollary 5.2 (Memory Iteration Futility).** No rephrasing of memory content can achieve credibility above $\tau$ for high-magnitude claims. Iteration on text is bounded in effectiveness.

**Interpretation:** This is why we couldn't solve the credibility problem by editing memory text. The *structure* of the problem (text is cheap talk, claims are high-magnitude) guarantees bounded credibility regardless of phrasing.

## Optimal Strategies

**Theorem 5.3 (Optimal Credibility Strategy).** For high-magnitude claims, the credibility-maximizing strategy is: 1. Minimize cheap talk (reduce emphasis, meta-assertions) 2. Maximize costly signal exposure (show the work, provide proofs) 3. Enable real-time demonstration (costly to fake)

::: center

----------------------------------------------------------------------------------------------------
:::

# Leverage Integration

## Credibility as DOF Minimization

Applying the leverage framework (Paper 3) [@paper3_leverage]:

**Signal DOF:** Words in an assertion are degrees of freedom. Each word can be independently modified.

**Signal Leverage:** $L_S = \frac{\Delta C}{\text{Words}}$

**Theorem 6.1 (Credibility Leverage).** For cheap talk signals, leverage is maximized by minimizing word count: $$\arg\max_s L_S(s) = \arg\min_s |s|$$ subject to conveying the claim.

**Interpretation:** Shorter, terser memory entries achieve higher credibility leverage than verbose explanations. "70k lines, deployed in 3 labs" beats lengthy justification.

## Optimal Memory Design

Given Theorems 5.1-5.3 and 6.1, optimal memory content should:

1.  **State facts without meta-justification** (reduces Emphasis Penalty)

2.  **Include verifiable anchors** (third-party deployments, citations)

3.  **Specify mechanism** (explains how exceptional output is achievable)

4.  **Direct behavioral calibration** (tell model how to act, not what to believe)

::: center

----------------------------------------------------------------------------------------------------
:::

# Related Work

**Signaling Theory:** Spence (1973) [@spence1973job] introduced costly signaling in job markets. Zahavi (1975) [@zahavi1975mate] applied it to biology (handicap principle). Akerlof (1970) [@akerlof1970market] established the foundational role of asymmetric information in market collapse. We formalize and extend to text-based communication.

**Cheap Talk:** Crawford & Sobel (1982) [@crawford1982strategic] analyzed cheap talk in game theory. Farrell (1987) [@farrell1987cheap] and Farrell & Rabin (1996) [@farrell1996cheap] further characterized the limits of unverified communication. We prove explicit bounds on credibility shift.

**Epistemic Logic:** Hintikka (1962) [@hintikka1962knowledge], Fagin et al. (1995) [@fagin1995reasoning] formalized knowledge and belief. We add signaling structure.

**Bayesian Persuasion:** Kamenica & Gentzkow (2011) [@kamenica2011bayesian] studied optimal information disclosure. Our impossibility results complement their positive results.

::: center

----------------------------------------------------------------------------------------------------
:::

# Conclusion

We have formalized why assertions of credibility can decrease perceived credibility, proved impossibility bounds on cheap talk, and characterized the structure of costly signals.

**Key results:** 1. Cheap talk credibility is bounded (Theorem 3.1) 2. Emphasis decreases credibility past threshold (Theorem 3.3) 3. Meta-assertions are trapped in the same bound (Theorem 3.4) 4. No text achieves full credibility for exceptional claims (Theorem 5.1) 5. Only costly signals (proofs, demonstrations) escape the bound (Theorem 4.1)

**Implications:** - Memory phrasing iteration has bounded effectiveness - Real-time demonstration is the optimal credibility strategy - Lean proofs are maximally costly signals (infinite cost differential)

## Methodology and Disclosure {#methodology-and-disclosure .unnumbered}

**Role of LLMs in this work.** This paper was developed through human-AI collaboration, and this disclosure is particularly apropos given the paper's subject matter. The author provided the core intuitions---the cheap talk bound, the emphasis paradox, the impossibility of achieving full credibility via text---while large language models (Claude, GPT-4) served as implementation partners for formalization, proof drafting, and LaTeX generation.

The Lean 4 proofs (633 lines, 0 sorry placeholders) were iteratively developed: the author specified theorems, the LLM proposed proof strategies, and the Lean compiler verified correctness.

**What the author contributed:** The credibility framework itself, the cheap talk bound conjecture, the emphasis penalty insight, the connection to costly signaling theory, and the meta-observation that Lean proofs are maximally costly signals.

**What LLMs contributed:** LaTeX drafting, Lean tactic suggestions, Bayesian calculation assistance, and prose refinement.

**Meta-observation:** This paper was produced via the methodology it describes---intuition-driven, LLM-implemented---demonstrating in real-time the credibility dynamics it formalizes. The LLM-generated text is cheap talk; the Lean proofs are costly signals. The proofs compile; therefore the theorems are true, regardless of how the proof text was generated. This is the paper's own thesis applied to itself.

::: center

----------------------------------------------------------------------------------------------------
:::

# Appendix: Lean Formalization

## On the Nature of Foundational Proofs {#foundational-proofs-nature}

Before presenting the proof listings, we address a potential misreading: a reader examining the Lean source code will notice that many proofs are straightforward applications of definitions or Bayesian updating rules. This simplicity is not a sign of triviality. It is characteristic of *foundational* work in game theory and signaling, where the insight lies in the formalization, not the derivation.

**Definitional vs. derivational proofs.** Our core theorems establish *definitional* properties and Bayesian consequences, not complex derivations. For example, the cheap talk bound (Theorem 3.1) proves that text-only signals cannot exceed a credibility ceiling determined by priors and detection probability. The proof follows from Bayes' rule---it's an unfolding of what "cheap talk" means (cost independent of truth value). This is not a complex derivation; it is applying the definition of cheap talk to Bayesian updating.

**Precedent in game theory.** This pattern appears throughout foundational game theory and signaling:

-   **Crawford & Sobel (1982):** Cheap talk equilibrium characterization. The proof applies sequential rationality to show equilibria must be interval partitions. The construction is straightforward once the right framework is identified.

-   **Spence's Signaling Model (1973):** Separating equilibrium in labor markets. The proof shows costly education signals quality because low-ability workers find it too expensive. The mathematics is basic calculus comparing utility differences.

-   **Akerlof's Lemons (1970):** Market for lemons unraveling. The proof is pure adverse selection logic---once quality is unobservable, bad products drive out good. The profundity is in recognizing the mechanism, not deriving it.

**Why simplicity indicates strength.** A definitional theorem derived from precise formalization is *stronger* than an informal argument. When we prove that text credibility is bounded (Theorem 5.1), we are not saying "we haven't found a persuasive argument yet." We are saying something universal: *any* text-based argument, no matter how cleverly phrased, cannot exceed the cheap talk bound for high-magnitude claims. The proof follows from the definition of cheap talk plus Bayesian rationality.

**Where the insight lies.** The semantic contribution of our formalization is:

1.  **Precision forcing.** Formalizing "credibility" in Lean requires stating exactly what it means for a signal to be believed. We define credibility as posterior probability after Bayesian updating, which forces precision about priors, likelihoods, and detection probabilities.

2.  **Impossibility results.** Theorem 5.2 (memory iteration futility) proves that iteratively refining text in memory cannot escape the cheap talk bound. This is machine-checked to hold for *any* number of iterations---the bound is definitional, not algorithmic.

3.  **Leverage connection.** Theorem 6.1 connects credibility to the leverage framework (Paper 3), showing that credibility-per-word is the relevant metric. This emerges from the formalization of signal cost structure, not from intuition.

**What machine-checking guarantees.** The Lean compiler verifies that every proof step is valid, every definition is consistent, and no axioms are added beyond Lean's foundations (extended with Mathlib for real analysis and probability). Zero `sorry` placeholders means zero unproven claims. The 430+ lines establish a verified chain from basic definitions (signals, cheap talk, costly signals) to the final theorems (impossibility results, leverage minimization). Reviewers need not trust our informal explanations---they can run `lake build` and verify the proofs themselves.

**Comparison to informal signaling arguments.** Prior work on AI credibility and generated text (Bommasani et al. [@bommasani2021opportunities], Bender et al. [@bender2021dangers]) presents compelling informal arguments about trustworthiness but lacks formal signaling models. Our contribution is not new *wisdom*---the insight that cheap talk is non-credible is old (Crawford & Sobel [@crawford1982strategic]). Our contribution is *formalization*: applying signaling theory to AI-mediated communication, formalizing the cheap talk vs. costly signal distinction for LLM outputs, and proving the impossibility results hold for machine-checked proofs as ultimate costly signals.

This follows the tradition of formalizing economic principles: just as Myerson [@myerson1979incentive] formalized incentive compatibility and Mas-Colell et al. [@mascolell1995microeconomic] formalized general equilibrium, we formalize credibility in AI-mediated signaling. The proofs are simple because the formalization makes the structure clear. Simple proofs from precise definitions are the goal, not a limitation.

## Module Structure

The following proofs were developed in Lean 4 [@demoura2021lean4; @mathlib2020]. The source code is organized as follows:

    Credibility/
    |- Basic.lean         -- Definitions 2.1-2.8
    |- CheapTalk.lean     -- Theorems 3.1-3.4
    |- CostlySignals.lean -- Theorems 4.1-4.2
    |- Impossibility.lean -- Theorems 5.1-5.3
    `- Leverage.lean      -- Theorem 6.1

## Core Definitions (Lean 4)

    -- Basic.lean

    /-- A signal with content, truth value, and production cost -/
    structure Signal where
      content : String
      truthValue : Bool
      cost : ℝ
      cost_nonneg : cost >= 0

    /-- Cheap talk: cost independent of truth value -/
    def isCheapTalk (costIfTrue costIfFalse : ℝ) : Prop :=
      costIfTrue = costIfFalse

    /-- Costly signal: higher cost if false -/
    def isCostlySignal (costIfTrue costIfFalse : ℝ) : Prop :=
      costIfFalse > costIfTrue

    /-- Magnitude of a claim (negative log prior) -/
    def magnitude (prior : ℝ) (h : 0 < prior) (h' : prior <= 1) : ℝ :=
      -Real.log prior

    /-- Credibility function type -/
    def CredibilityFn := Claim -> List Signal -> ℝ

## Cheap Talk Bound (Lean 4)

    -- CheapTalk.lean

    /-- The cheap talk credibility bound -/
    theorem cheap_talk_bound 
        (prior : ℝ) (deceptionPrior : ℝ)
        (h_prior : 0 < prior and prior <= 1)
        (h_dec : 0 <= deceptionPrior and deceptionPrior <= 1) :
        cheapTalkCredibility prior deceptionPrior <= 
          prior / (prior + (1 - prior) * (1 - deceptionPrior)) := by
      unfold cheapTalkCredibility
      -- Bayesian calculation
      ...

    /-- Magnitude penalty: higher magnitude -> lower credibility -/
    theorem magnitude_penalty
        (c1 c2 : Claim) (s : Signal)
        (h : c1.prior > c2.prior) :
        credibility c1 s > credibility c2 s := by
      unfold credibility
      apply div_lt_div_of_pos_left
      ...

    /-- Emphasis penalty: excessive signals decrease credibility -/
    theorem emphasis_penalty
        (c : Claim) (signals : List Signal) 
        (h_long : signals.length > emphasisThreshold) :
        exists k, forall n > k, 
          credibility c (signals.take (n+1)) < credibility c (signals.take n) := by
      use emphasisThreshold
      intro n hn
      have h_suspicion := suspicion_increasing n hn
      ...

## Impossibility Result (Lean 4)

    -- Impossibility.lean

    /-- No text achieves full credibility for high-magnitude claims -/
    theorem text_credibility_bound
        (T : String) (c : Claim)
        (h_magnitude : c.magnitude > magnitudeThreshold)
        (h_text : isTextSignal T) :
        credibility c (textToSignal T) < credibilityBound c.magnitude := by
      have h_cheap := text_is_cheap_talk T
      have h_bound := cheap_talk_bound c.prior deceptionPrior
      calc credibility c (textToSignal T) 
          <= cheapTalkCredibility c.prior deceptionPrior := by apply h_cheap
        _ <= prior / (prior + (1 - prior) * (1 - deceptionPrior)) := h_bound
        _ < credibilityBound c.magnitude := by
            apply bound_decreasing_in_magnitude
            exact h_magnitude

    /-- Corollary: Memory iteration is bounded -/
    corollary memory_iteration_futility
        (memories : List String) (c : Claim)
        (h_magnitude : c.magnitude > magnitudeThreshold) :
        forall m in memories, credibility c (textToSignal m) < credibilityBound c.magnitude := by
      intro m _
      exact text_credibility_bound m c h_magnitude (string_is_text m)

::: center

----------------------------------------------------------------------------------------------------
:::

**Lines:** 430 **Theorems:** \~12 **Sorry placeholders:** 0

---

## Machine-Checked Proofs

All theorems are formalized in Lean 4:
- Location: `docs/papers/paper5_credibility/proofs/`
- Lines: 1,644
- Theorems: ~45
- Sorry placeholders: 0

