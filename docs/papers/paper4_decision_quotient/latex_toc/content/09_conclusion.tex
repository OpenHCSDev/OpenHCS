\section{Conclusion}

\subsection*{Methodology and Disclosure}

\textbf{Role of LLMs in this work.} This paper was developed through human-AI collaboration. The author provided the core intuitions---the connection between decision-relevance and computational complexity, the conjecture that SUFFICIENCY-CHECK is \coNP-complete, and the insight that the $\SigmaP{2}$ structure collapses for MINIMUM-SUFFICIENT-SET. Large language models (Claude, GPT-4) served as implementation partners for proof drafting, Lean formalization, and \LaTeX{} generation.

The Lean 4 proofs were iteratively refined: the author specified the target statements, the LLM proposed proof strategies, and the Lean compiler served as the arbiter of correctness. The complexity-theoretic reductions required careful human oversight to ensure the polynomial bounds were correctly established.

\textbf{What the author contributed:} The problem formulations (SUFFICIENCY-CHECK, MINIMUM-SUFFICIENT-SET, ANCHOR-SUFFICIENCY), the hardness conjectures, the tractability conditions, and the connection to over-modeling in engineering practice.

\textbf{What LLMs contributed:} \LaTeX{} drafting, Lean tactic exploration, reduction construction assistance, and prose refinement.

The proofs are machine-checked; their validity is independent of generation method. We disclose this methodology in the interest of academic transparency.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Summary of Results}

This paper establishes the computational complexity of coordinate sufficiency problems:

\begin{itemize}
\item \textbf{SUFFICIENCY-CHECK} is \coNP-complete (Theorem~\ref{thm:sufficiency-conp})
\item \textbf{MINIMUM-SUFFICIENT-SET} is \coNP-complete (Theorem~\ref{thm:minsuff-conp})
\item \textbf{ANCHOR-SUFFICIENCY} is $\SigmaP{2}$-complete (Theorem~\ref{thm:anchor-sigma2p})
\item An encoding-regime separation contrasts explicit-state polynomial-time (polynomial in $|S|$) with a succinct worst-case ETH lower bound witnessed by a hard family with $k^*=n$ (Theorem~\ref{thm:dichotomy})
\item A full intermediate query-access lower bound is formalized in the Boolean query submodel: $\Omega(2^n)$ worst-case queries for SUFFICIENCY-CHECK via an indistinguishable pair on the $I=\emptyset$ subproblem (Proposition~\ref{prop:query-regime-obstruction})
\item Tractable subcases exist for explicit-state encoding, separable utility, and tree-structured utility with explicit local factors (Theorem~\ref{thm:tractable})
\end{itemize}

These results place the problem of identifying decision-relevant coordinates at the first and second levels of the polynomial hierarchy.

Beyond classification, the paper contributes a formal claim-typing framework (Section~\ref{sec:interpretive-foundations}): structural complexity is a property of the fixed decision relation, while representational hardness is regime-conditional access cost. This is why encoding-regime changes can move practical hardness without changing the underlying semantics.

The reduction constructions and key equivalence theorems are machine-checked in Lean 4 (see Appendix~\ref{app:lean} for proof listings). The formalization verifies that the TAUTOLOGY reduction correctly maps tautologies to sufficient coordinate sets; complexity classifications (coNP-completeness, $\SigmaP{2}$-completeness) follow by composition with standard complexity-theoretic results (TAUTOLOGY is \coNP-complete, $\exists\forall$-SAT is $\SigmaP{2}$-complete). The strengthened gadget showing that non-tautologies yield instances with \emph{all coordinates relevant} is also formalized.

\subsection*{Complexity Characterization}

The results provide precise complexity characterizations within the formal model:

\begin{enumerate}
\item \textbf{Exact bounds.} SUFFICIENCY-CHECK is \coNP-complete---both \coNP-hard and in \coNP.

\item \textbf{Constructive reductions.} The reductions from TAUTOLOGY and $\exists\forall$-SAT are explicit and machine-checked.

\item \textbf{Encoding-regime separation.} Under [E], SUFFICIENCY-CHECK is polynomial in $|S|$. Under [S+ETH], there exist succinct worst-case instances (with $k^*=n$) requiring $2^{\Omega(n)}$ time. Under [Q\_bool], the mechanized lower bound is $\Omega(2^n)$ queries in the worst case for full SUFFICIENCY-CHECK (via the $I=\emptyset$ subproblem). Other intermediate access models remain open.
\end{enumerate}

\subsection*{The Complexity Redistribution Corollary}

Section~\ref{sec:simplicity-tax} develops a quantitative consequence: when a problem requires $k$ dimensions and a model handles only $j < k$ natively, the remaining $k - j$ dimensions must be handled externally at each decision site. For $n$ sites, total external work is $(k-j) \times n$.

The set identity is elementary; its operational content comes from composition with the hardness results on exact relevance minimization. This redistribution corollary is formalized in Lean 4 (\texttt{HardnessDistribution.lean}), proving:
\begin{itemize}
\item Redistribution identity: complexity burden cannot be eliminated by omission, only moved between native handling and external handling
\item Dominance: complete models have lower total work than incomplete models
\item Amortization: there exists a threshold $n^*$ beyond which higher-dimensional models have lower total cost
\end{itemize}

\subsection*{Open Questions}

Several questions remain for future work:
\begin{itemize}
\item \textbf{Fixed-parameter tractability (primary):} Is SUFFICIENCY-CHECK FPT when parameterized by the minimal sufficient-set size $k^*$, or is it W[2]-hard under this parameterization?
\item \textbf{Sequential/stochastic bridge extension:} Characterize the exact frontier where adjacent sequential objectives reduce to the static class via Proposition~\ref{prop:one-step-bridge}, and where genuinely new complexity objects (e.g., horizon/sample/regret complexity) must replace the present coNP/\(\Sigma_2^P\) analysis.
\item \textbf{Average-case complexity:} What is the complexity under natural distributions on decision problems?
\item \textbf{Learning cost formalization:} Can central cost $H_{\text{central}}$ be formalized as the rank of a concept matroid, making the amortization threshold precisely computable?
\end{itemize}

\subsection*{Practical Corollaries}

The practical corollaries are regime-indexed and theorem-indexed:
\begin{itemize}
\item \textbf{[E] and structured regimes:} polynomial-time exact procedures exist (Theorem~\ref{thm:tractable}).
\item \textbf{[Q\_bool] query-access lower bound:} worst-case full-problem query complexity is $\Omega(2^n)$ (Proposition~\ref{prop:query-regime-obstruction}).
\item \textbf{[S+ETH] hard families:} exact minimization inherits exponential worst-case cost (Theorem~\ref{thm:dichotomy} together with Theorem~\ref{thm:sufficiency-conp}).
\item \textbf{[S\_bool] mechanized criterion:} minimization reduces to relevance-cardinality constraints (Corollary~\ref{cor:practice-diagnostic}).
\item \textbf{Redistribution consequences:} omitted native coverage externalizes work with explicit growth/amortization laws (Theorems~\ref{thm:tax-conservation}--\ref{thm:amortization}).
\end{itemize}

Hence the design choice is typed: enforce a tractable regime, or adopt weakened guarantees with explicit verification boundaries.
Equivalently, over-specification is a conditional attempted-competence-failure policy in this framework; once exact competence is available in the active regime (or no attempted exact competence was made), persistent over-specification is irrational for the same verified-cost objective.
By Proposition~\ref{prop:attempted-competence-matrix}, this is not a close call: in the integrity-preserving matrix, irrational cells outnumber rational cells (3 vs 1).


\appendix
