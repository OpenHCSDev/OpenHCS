\section{Conclusion}

\subsection*{Methodology and Disclosure}

\textbf{Role of LLMs in this work.} This paper was developed through human-AI collaboration. The author provided the core intuitions---the connection between decision-relevance and computational complexity, the conjecture that SUFFICIENCY-CHECK is \coNP-complete, and the insight that the $\SigmaP{2}$ structure collapses for MINIMUM-SUFFICIENT-SET. Large language models (Claude, GPT-4) served as implementation partners for proof drafting, Lean formalization, and \LaTeX{} generation.

The Lean 4 proofs were iteratively refined: the author specified the target statements, the LLM proposed proof strategies, and the Lean compiler served as the arbiter of correctness. The complexity-theoretic reductions required careful human oversight to ensure the polynomial bounds were correctly established.

\textbf{What the author contributed:} The problem formulations (SUFFICIENCY-CHECK, MINIMUM-SUFFICIENT-SET, ANCHOR-SUFFICIENCY), the hardness conjectures, the tractability conditions, and the connection to over-modeling in engineering practice.

\textbf{What LLMs contributed:} \LaTeX{} drafting, Lean tactic exploration, reduction construction assistance, and prose refinement.

The proofs are machine-checked; their validity is independent of generation method. We disclose this methodology in the interest of academic transparency.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Summary of Results}

This paper establishes the computational complexity of coordinate sufficiency problems:

\begin{itemize}
\item \textbf{SUFFICIENCY-CHECK} is \coNP-complete (Theorem~\ref{thm:sufficiency-conp})
\item \textbf{MINIMUM-SUFFICIENT-SET} is \coNP-complete (Theorem~\ref{thm:minsuff-conp})
\item \textbf{ANCHOR-SUFFICIENCY} is $\SigmaP{2}$-complete (Theorem~\ref{thm:anchor-sigma2p})
\item An encoding-regime separation contrasts explicit-state polynomial-time (polynomial in $|S|$) with a succinct worst-case ETH lower bound witnessed by a hard family with $k^*=n$ (Theorem~\ref{thm:dichotomy})
\item Tractable subcases exist for explicit-state encoding, separable utility, and tree-structured utility with explicit local factors (Theorem~\ref{thm:tractable})
\end{itemize}

These results place the problem of identifying decision-relevant coordinates at the first and second levels of the polynomial hierarchy.

The reduction constructions and key equivalence theorems are machine-checked in Lean 4 (see Appendix~\ref{app:lean} for proof listings). The formalization verifies that the TAUTOLOGY reduction correctly maps tautologies to sufficient coordinate sets; complexity classifications (coNP-completeness, $\SigmaP{2}$-completeness) follow by composition with standard complexity-theoretic results (TAUTOLOGY is \coNP-complete, $\exists\forall$-SAT is $\SigmaP{2}$-complete). The strengthened gadget showing that non-tautologies yield instances with \emph{all coordinates relevant} is also formalized.

\subsection*{Complexity Characterization}

The results provide precise complexity characterizations within the formal model:

\begin{enumerate}
\item \textbf{Exact bounds.} SUFFICIENCY-CHECK is \coNP-complete---both \coNP-hard and in \coNP.

\item \textbf{Constructive reductions.} The reductions from TAUTOLOGY and $\exists\forall$-SAT are explicit and machine-checked.

\item \textbf{Encoding-regime separation.} Under the explicit-state encoding, SUFFICIENCY-CHECK is polynomial in $|S|$. Under ETH, there exist succinctly encoded worst-case instances (witnessed by a strengthened gadget family with $k^*=n$) requiring $2^{\Omega(n)}$ time. Intermediate regimes are not ruled out by the lower-bound statement.
\end{enumerate}

\subsection*{The Complexity Conservation Law}

Section~\ref{sec:simplicity-tax} develops a quantitative consequence: when a problem requires $k$ dimensions and a model handles only $j < k$ natively, the remaining $k - j$ dimensions must be handled externally at each decision site. For $n$ sites, total external work is $(k-j) \times n$.

This conservation law is formalized in Lean 4 (\texttt{HardnessDistribution.lean}), proving:
\begin{itemize}
\item Conservation: complexity cannot be eliminated, only redistributed
\item Dominance: complete models have lower total work than incomplete models
\item Amortization: there exists a threshold $n^*$ beyond which higher-dimensional models have lower total cost
\end{itemize}

\subsection*{Open Questions}

Several questions remain for future work:
\begin{itemize}
\item \textbf{Fixed-parameter tractability:} Is SUFFICIENCY-CHECK FPT when parameterized by the size of the minimal sufficient set?
\item \textbf{Average-case complexity:} What is the complexity under natural distributions on decision problems?
\item \textbf{Quantum complexity:} Does quantum computation provide speedups for sufficiency checking?
\item \textbf{Learning cost formalization:} Can central cost $H_{\text{central}}$ be formalized as the rank of a concept matroid, making the amortization threshold precisely computable?
\end{itemize}

\subsection*{Practical interpretation}

While this work is theoretical, the complexity bounds have practical meaning. Under ETH, worst-case instances of SUFFICIENCY-CHECK and related problems grow exponentially with problem size; practitioners should therefore not expect polynomial‑time worst‑case solvers for arbitrary inputs. In practice, real datasets often have structure (sparsity, bounded dependency depth, separable utilities) that the tractable subcases capture. Thus, engineering efforts should focus on (a) detecting and exploiting structural restrictions in inputs, and (b) designing heuristic or approximate methods for large unstructured instances. Our experiments illustrate performance on representative inputs, but the theoretical bounds underline the need for domain constraints to achieve scalable, worst‑case guarantees.


\appendix
