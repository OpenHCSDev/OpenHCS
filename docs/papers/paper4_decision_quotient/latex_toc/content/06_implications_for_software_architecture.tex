\section{Informal Corollaries for Software Architecture}\label{sec:implications}

This section states informal corollaries for software architecture derived from the formal model. The complexity results have direct implications for software engineering practice, but the statements in this section are interpretive consequences; machine-checked theorem statements are in Section~\ref{sec:simplicity-tax} and Appendix~\ref{app:lean}. We use Theorem~\ref{thm:overmodel-diagnostic} as the interpretive lens: persistent over-specification is evidence about unresolved boundary characterization in the current representation.

\subsection{Why Persistent Over-Specification Is Diagnostic}

The formal statement appears in Section~\ref{sec:engineering-justification}: Theorem~\ref{thm:overmodel-diagnostic}. We do not restate the complexity classification here; this section treats that theorem as a premise and derives architecture-level corollaries. In practice, configuration growth is read as evidence that decision relevance is not yet characterized in a tractable representation.

\subsection{Architectural Decision Quotient}

The sufficiency framework suggests a measure for architectural decisions:

\begin{definition}[Architectural Decision Quotient]
For a software system with configuration space $S$ and behavior space $B$:
\[
\text{ADQ}(I) = \frac{|\{b \in B : b \text{ achievable with some } s \text{ where } s_I \text{ fixed}\}|}{|B|}
\]
\end{definition}

High ADQ means the configuration subset $I$ leaves many behaviors achievable---it doesn't constrain the system much. Low ADQ means $I$ strongly constrains behavior.

\begin{proposition}
Decisions with low ADQ (strongly constraining) require fewer additional decisions to fully specify system behavior.
\end{proposition}

\subsection{Corollaries for Practice}

\begin{corollary}[Practice-facing corollaries]
\label{cor:practice-corollaries}
Within the formal model, the following corollaries hold:
\begin{enumerate}
\item \textbf{Treat over-modeling as a diagnostic:} Persistent inclusion of ``extra'' parameters signals unresolved boundary characterization, not automatic irreducibility.
\item \textbf{Use bounded scenarios:} When the scenario space is small (Proposition~\ref{prop:sufficiency-char}), minimal modeling becomes tractable.
\item \textbf{Exploit structure:} Tree-structured dependencies, bounded alternatives, and separable utilities admit efficient algorithms.
\item \textbf{Use heuristics in the unstructured regime:} For general instances outside the tractable classes, exact optimization has worst-case hardness.
\end{enumerate}
\end{corollary}

\begin{proof}[Derivation sketch]
Items (1) and (4) follow from the hardness theorems in Section~\ref{sec:hardness} and the encoding-regime separation in Theorem~\ref{thm:dichotomy}. Items (2) and (3) follow from the tractable conditions in Theorem~\ref{thm:tractable}.
\end{proof}

\subsection{Hardness Distribution: Right Place vs Wrong Place}\label{sec:hardness-distribution}

A general principle emerges from the complexity results: problem hardness is conserved and is \emph{distributed} across a system in qualitatively different ways.

\begin{definition}[Hardness Distribution]\label{def:hardness-distribution}
Let $P$ be a problem family under the succinct encoding of Section~\ref{sec:encoding}. In this section, baseline hardness $H(P;n)$ denotes worst-case computational step complexity on instances with $n$ coordinates (equivalently, as a function of succinct input length $L$) in the fixed encoding regime. A \emph{solution architecture} $S$ partitions this baseline hardness into:
\begin{itemize}
\item $H_{\text{central}}(S)$: hardness paid once, at design time or in a shared component
\item $H_{\text{distributed}}(S)$: hardness paid per use site
\end{itemize}
For $n$ use sites, total realized hardness is:
\[H_{\text{total}}(S) = H_{\text{central}}(S) + n \cdot H_{\text{distributed}}(S)\]
\end{definition}

\begin{proposition}[Hardness Conservation Principle]\label{def:hardness-conservation}
For any problem family $P$ measured by $H(P;n)$ above, any solution architecture $S$ and any number of use sites $n \ge 1$, if $H_{\text{total}}(S)$ is measured in the same worst-case step units over the same input family, then:
\[
H_{\text{total}}(S) = H_{\text{central}}(S) + n \cdot H_{\text{distributed}}(S) \geq H(P;n).
\]
For SUFFICIENCY-CHECK, Theorem~\ref{thm:dichotomy} provides the baseline on the hard succinct family: $H(\textsc{SUFFICIENCY-CHECK};n)=2^{\Omega(n)}$ under ETH.
\end{proposition}

\begin{proof}
By definition, $H(P;n)$ is a worst-case lower bound for correct solutions in this encoding regime and cost metric. Any such solution architecture decomposes total realized work as $H_{\text{central}} + n\cdot H_{\text{distributed}}$, so that total cannot fall below the baseline.
\end{proof}

\begin{definition}[Hardness Efficiency]\label{def:hardness-efficiency}
The \emph{hardness efficiency} of solution $S$ with $n$ use sites is:
\[\eta(S, n) = \frac{H_{\text{central}}(S)}{H_{\text{central}}(S) + n \cdot H_{\text{distributed}}(S)}\]
\end{definition}

High $\eta$ indicates centralized hardness (paid once); low $\eta$ indicates distributed hardness (paid repeatedly).

\begin{theorem}[Centralization Dominance]\label{thm:centralization-dominance}
For $n > 1$ use sites, solutions with higher $H_{\text{central}}$ and lower $H_{\text{distributed}}$ yield:
\begin{enumerate}
\item Lower total realized hardness: $H_{\text{total}}(S_1) < H_{\text{total}}(S_2)$ when $H_{\text{distributed}}(S_1) < H_{\text{distributed}}(S_2)$
\item Fewer error sites: errors in centralized components affect 1 location; errors in distributed components affect $n$ locations
\item Higher leverage: one unit of central effort affects $n$ sites
\end{enumerate}
\end{theorem}

\begin{proof}
(1) follows from the total hardness formula. (2) follows from error site counting. (3) follows from the definition of leverage as $L = \Delta\text{Effect}/\Delta\text{Effort}$.
\end{proof}

\begin{corollary}[Right Hardness vs Wrong Hardness]\label{cor:right-wrong-hardness}
A solution exhibits \emph{hardness in the right place} when:
\begin{itemize}
\item Hardness is centralized (high $H_{\text{central}}$, low $H_{\text{distributed}}$)
\item Hardness is paid at design/compile time rather than runtime
\item Hardness is enforced by tooling (type checker, compiler) rather than convention
\end{itemize}
A solution exhibits \emph{hardness in the wrong place} when:
\begin{itemize}
\item Hardness is distributed (low $H_{\text{central}}$, high $H_{\text{distributed}}$)
\item Hardness is paid repeatedly at each use site
\item Hardness relies on human discipline rather than mechanical enforcement
\end{itemize}
\end{corollary}

\textbf{Example: Type System Instantiation.} Consider a capability $C$ (e.g., provenance tracking) that requires hardness $H(C)$:

\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Approach} & $H_{\text{central}}$ & $H_{\text{distributed}}$ \\
\midrule
Native type system support & High (learning cost) & Low (type checker enforces) \\
Manual implementation & Low (no new concepts) & High (reimplement per site) \\
\bottomrule
\end{tabular}
\end{center}

For $n$ use sites, manual implementation costs $n \cdot H_{\text{distributed}}$, growing without bound. Native support costs $H_{\text{central}}$ once, amortized across all uses. The ``simpler'' approach (manual) is only simpler at $n = 1$; for $n > H_{\text{central}}/H_{\text{distributed}}$, native support dominates.

\begin{remark}[Connection to Decision Quotient]
The decision quotient (Section~\ref{sec:foundations}) measures which coordinates are decision-relevant. Hardness distribution measures where the cost of \emph{handling} those coordinates is paid. A high-axis system makes relevance explicit (central hardness); a low-axis system requires users to track relevance themselves (distributed hardness).
\end{remark}

\paragraph{Mechanized strengthening reference.}
The strengthened all-coordinates-relevant reduction is presented in Section~\ref{sec:hardness} (``Mechanized strengthening'') and formalized in \texttt{Reduction\_AllCoords.lean} via \texttt{all\_coords\_relevant\_of\_not\_tautology}.

The next section develops the major practical consequence of this framework: the Simplicity Tax Theorem.
