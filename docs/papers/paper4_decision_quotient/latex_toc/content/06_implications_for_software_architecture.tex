\section{Informal Corollaries for Software Architecture}\label{sec:implications}

This section states informal corollaries for software architecture derived from the formal model. The complexity results have direct implications for software engineering practice, but the statements in this section are interpretive consequences; machine-checked theorem statements are in Section~\ref{sec:simplicity-tax} and Appendix~\ref{app:lean}. We use Theorem~\ref{thm:overmodel-diagnostic} as the interpretive lens: persistent over-specification is evidence about unresolved boundary characterization in the current representation.

\subsection{Why Persistent Over-Specification Is Diagnostic}

Software architects routinely specify more configuration parameters than strictly necessary. In the formal model, this is best interpreted diagnostically:

\begin{corollary}[Diagnostic Over-Specification]
Given a software system with $n$ configuration parameters, checking whether a proposed subset suffices is \coNP-complete. Finding the minimum such set is also \coNP-complete.
\end{corollary}

This explains why configuration files grow over time: inability to remove parameters is often evidence that decision relevance is not yet characterized in a tractable form.

\subsection{Architectural Decision Quotient}

The sufficiency framework suggests a measure for architectural decisions:

\begin{definition}[Architectural Decision Quotient]
For a software system with configuration space $S$ and behavior space $B$:
\[
\text{ADQ}(I) = \frac{|\{b \in B : b \text{ achievable with some } s \text{ where } s_I \text{ fixed}\}|}{|B|}
\]
\end{definition}

High ADQ means the configuration subset $I$ leaves many behaviors achievable---it doesn't constrain the system much. Low ADQ means $I$ strongly constrains behavior.

\begin{proposition}
Decisions with low ADQ (strongly constraining) require fewer additional decisions to fully specify system behavior.
\end{proposition}

\subsection{Corollaries for Practice}

\begin{corollary}[Practice-facing corollaries]
\label{cor:practice-corollaries}
Within the formal model, the following corollaries hold:
\begin{enumerate}
\item \textbf{Treat over-modeling as a diagnostic:} Persistent inclusion of ``extra'' parameters signals unresolved boundary characterization, not automatic irreducibility.
\item \textbf{Use bounded scenarios:} When the scenario space is small (Proposition~\ref{prop:sufficiency-char}), minimal modeling becomes tractable.
\item \textbf{Exploit structure:} Tree-structured dependencies, bounded alternatives, and separable utilities admit efficient algorithms.
\item \textbf{Use heuristics in the unstructured regime:} For general instances outside the tractable classes, exact optimization has worst-case hardness.
\end{enumerate}
\end{corollary}

\begin{proof}[Derivation sketch]
Items (1) and (4) follow from the hardness theorems in Section~\ref{sec:hardness} and the encoding-regime separation in Theorem~\ref{thm:dichotomy}. Items (2) and (3) follow from the tractable conditions in Theorem~\ref{thm:tractable}.
\end{proof}

\subsection{Hardness Distribution: Right Place vs Wrong Place}\label{sec:hardness-distribution}

A general principle emerges from the complexity results: problem hardness is conserved and is \emph{distributed} across a system in qualitatively different ways.

\begin{definition}[Hardness Distribution]\label{def:hardness-distribution}
Let $P$ be a problem family under the succinct encoding of Section~\ref{sec:encoding}. In this section, intrinsic hardness $H(P;n)$ denotes worst-case computational step complexity on instances with $n$ coordinates (equivalently, as a function of succinct input length $L$). A \emph{solution architecture} $S$ partitions this hardness into:
\begin{itemize}
\item $H_{\text{central}}(S)$: hardness paid once, at design time or in a shared component
\item $H_{\text{distributed}}(S)$: hardness paid per use site
\end{itemize}
For $n$ use sites, total realized hardness is:
\[H_{\text{total}}(S) = H_{\text{central}}(S) + n \cdot H_{\text{distributed}}(S)\]
\end{definition}

\begin{definition}[Hardness Conservation Principle]\label{def:hardness-conservation}
For any problem family $P$ measured by $H(P;n)$ above, any solution architecture $S$ and any number of use sites $n \ge 1$ satisfy:
\[
H_{\text{total}}(S) = H_{\text{central}}(S) + n \cdot H_{\text{distributed}}(S) \geq H(P;n).
\]
For SUFFICIENCY-CHECK, Theorem~\ref{thm:dichotomy} provides the baseline on the hard succinct family: $H(\textsc{SUFFICIENCY-CHECK};n)=2^{\Omega(n)}$ under ETH.
\end{definition}

\begin{definition}[Hardness Efficiency]\label{def:hardness-efficiency}
The \emph{hardness efficiency} of solution $S$ with $n$ use sites is:
\[\eta(S, n) = \frac{H_{\text{central}}(S)}{H_{\text{central}}(S) + n \cdot H_{\text{distributed}}(S)}\]
\end{definition}

High $\eta$ indicates centralized hardness (paid once); low $\eta$ indicates distributed hardness (paid repeatedly).

\begin{theorem}[Centralization Dominance]\label{thm:centralization-dominance}
For $n > 1$ use sites, solutions with higher $H_{\text{central}}$ and lower $H_{\text{distributed}}$ yield:
\begin{enumerate}
\item Lower total realized hardness: $H_{\text{total}}(S_1) < H_{\text{total}}(S_2)$ when $H_{\text{distributed}}(S_1) < H_{\text{distributed}}(S_2)$
\item Fewer error sites: errors in centralized components affect 1 location; errors in distributed components affect $n$ locations
\item Higher leverage: one unit of central effort affects $n$ sites
\end{enumerate}
\end{theorem}

\begin{proof}
(1) follows from the total hardness formula. (2) follows from error site counting. (3) follows from the definition of leverage as $L = \Delta\text{Effect}/\Delta\text{Effort}$.
\end{proof}

\begin{corollary}[Right Hardness vs Wrong Hardness]\label{cor:right-wrong-hardness}
A solution exhibits \emph{hardness in the right place} when:
\begin{itemize}
\item Hardness is centralized (high $H_{\text{central}}$, low $H_{\text{distributed}}$)
\item Hardness is paid at design/compile time rather than runtime
\item Hardness is enforced by tooling (type checker, compiler) rather than convention
\end{itemize}
A solution exhibits \emph{hardness in the wrong place} when:
\begin{itemize}
\item Hardness is distributed (low $H_{\text{central}}$, high $H_{\text{distributed}}$)
\item Hardness is paid repeatedly at each use site
\item Hardness relies on human discipline rather than mechanical enforcement
\end{itemize}
\end{corollary}

\textbf{Example: Type System Instantiation.} Consider a capability $C$ (e.g., provenance tracking) that requires hardness $H(C)$:

\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Approach} & $H_{\text{central}}$ & $H_{\text{distributed}}$ \\
\midrule
Native type system support & High (learning cost) & Low (type checker enforces) \\
Manual implementation & Low (no new concepts) & High (reimplement per site) \\
\bottomrule
\end{tabular}
\end{center}

For $n$ use sites, manual implementation costs $n \cdot H_{\text{distributed}}$, growing without bound. Native support costs $H_{\text{central}}$ once, amortized across all uses. The ``simpler'' approach (manual) is only simpler at $n = 1$; for $n > H_{\text{central}}/H_{\text{distributed}}$, native support dominates.

\begin{remark}[Connection to Decision Quotient]
The decision quotient (Section~\ref{sec:foundations}) measures which coordinates are decision-relevant. Hardness distribution measures where the cost of \emph{handling} those coordinates is paid. A high-axis system makes relevance explicit (central hardness); a low-axis system requires users to track relevance themselves (distributed hardness).
\end{remark}

\paragraph{Mechanized strengthening (sketch).}
We formalized a strengthened reduction in the mechanization: given a Boolean formula $\varphi$ over $n$ variables we construct a decision problem with $n$ coordinates so that if $\varphi$ is not a tautology then every coordinate is decision-relevant. Intuitively, the construction places a copy of the base gadget at each coordinate and makes the global ``accept'' condition hold only when every coordinate's local test succeeds; a single falsifying assignment at one coordinate therefore changes the global optimal set, witnessing that coordinate's relevance. The mechanized statement and proof appear in the development as \texttt{Reduction\_AllCoords.lean} (the lemma \texttt{all\_coords\_relevant\_of\_not\_tautology}).

The next section develops the major practical consequence of this framework: the Simplicity Tax Theorem.
