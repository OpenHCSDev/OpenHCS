\section{Implications for Practice: Why Over-Modeling Is Optimal}\label{sec:engineering-justification}

This section states corollaries for engineering practice. Within the formal model, the complexity results of Sections~\ref{sec:hardness} and~\ref{sec:dichotomy} shift parts of this workflow from informal judgment toward explicit, checkable criteria. The observed behaviors---configuration over-specification, absence of automated minimization tools, heuristic model selection---are not failures of discipline but \emph{provably optimal responses} to computational constraints under the stated cost model.

Some common prescriptions implicitly require exact minimization of sufficient parameter sets. In the worst case, that task is \coNP-complete in our model, so we should calibrate critiques and expectations accordingly. A rational response is to include everything and pay linear maintenance costs, rather than attempt exponential minimization costs.

\subsection{Configuration Simplification is SUFFICIENCY-CHECK}

Real engineering problems reduce directly to the decision problems studied in this paper.

\begin{theorem}[Configuration Simplification Reduces to SUFFICIENCY-CHECK]
\label{thm:config-reduction}
Given a software system with configuration parameters $P = \{p_1, \ldots, p_n\}$ and observed behaviors $B = \{b_1, \ldots, b_m\}$, the problem of determining whether parameter subset $I \subseteq P$ preserves all behaviors is equivalent to SUFFICIENCY-CHECK.
\end{theorem}

\begin{proof}
Construct decision problem $\mathcal{D} = (A, X_1, \ldots, X_n, U)$ where:
\begin{itemize}
\item Actions $A = B$ (each behavior is an action)
\item Coordinates $X_i$ = domain of parameter $p_i$
\item State space $S = X_1 \times \cdots \times X_n$
\item Utility $U(b, s) = 1$ if behavior $b$ occurs under configuration $s$, else $U(b, s) = 0$
\end{itemize}

Then $\Opt(s) = \{b \in B : b \text{ occurs under configuration } s\}$.

Coordinate set $I$ is sufficient iff:
\[
s_I = s'_I \implies \Opt(s) = \Opt(s')
\]

This holds iff configurations agreeing on parameters in $I$ exhibit identical behaviors.

Therefore, ``does parameter subset $I$ preserve all behaviors?'' is exactly SUFFICIENCY-CHECK for the constructed decision problem.
\end{proof}

\begin{remark}
This reduction is \emph{parsimonious}: configuration simplification can be cast as an instance of SUFFICIENCY-CHECK under a direct encoding.
\end{remark}

\subsection{Computational Rationality of Over-Modeling}

We now prove that over-specification is an optimal engineering strategy given the stated cost model and complexity constraints.

\begin{theorem}[Rational Over-Modeling]
\label{thm:rational-overmodel}
Consider an engineer specifying a system configuration with $n$ parameters. Let:
\begin{itemize}
\item $C_{\text{over}}(k)$ = cost of maintaining $k$ extra parameters beyond minimal
\item $C_{\text{find}}(n)$ = cost of finding minimal sufficient parameter set
\item $C_{\text{under}}$ = expected cost of production failures from underspecification
\end{itemize}

When SUFFICIENCY-CHECK is \coNP-complete (Theorem~\ref{thm:sufficiency-conp}):
\begin{enumerate}
\item Naive exhaustive search inspects $2^n$ candidate subsets, so the brute-force finding cost grows exponentially in $n$; more formally, exhaustive search yields an upper bound $C_{\text{find}}(n) = O(2^n)$. A matching *unconditional* lower bound of $2^{\Omega(n)}$ does not follow from \coNP-completeness alone. Instead, \coNP-completeness implies there is no polynomial‑time algorithm for arbitrary inputs unless $\Pclass = \coNP$. Stronger assumptions (e.g., the Exponential Time Hypothesis) yield explicit worst-case lower bounds: under ETH, SUFFICIENCY-CHECK has a $2^{\Omega(n)}$ worst-case time lower bound in the succinct encoding (witnessed by the strengthened TAUTOLOGY gadget family).
\item Maintenance cost is linear: $C_{\text{over}}(k) = O(k)$.
\item Under the ETH (or when considering naive exhaustive search), exponential finding cost dominates linear maintenance cost for sufficiently large $n$.
\end{enumerate}

Therefore, there exists $n_0$ such that for all $n > n_0$, over-modeling minimizes total expected cost:
\[
C_{\text{over}}(k) < C_{\text{find}}(n) + C_{\text{under}}
\]

Over-modeling is economically optimal under the stated model and complexity constraints.
\end{theorem}

\begin{proof}
By Theorem~\ref{thm:sufficiency-conp}, SUFFICIENCY-CHECK is \coNP-complete, so under the assumption $\Pclass \neq \coNP$ there is no polynomial‑time algorithm that decides sufficiency for arbitrary inputs.

Finding the minimal sufficient set by brute force requires checking many candidate sets. Exhaustive search examines
\[
\sum_{i=0}^{n} \binom{n}{i} = 2^n
\]
candidate subsets, so naive search has cost $O(2^n)$. This shows only that brute-force is exponential. To assert a matching lower bound of $2^{\Omega(n)}$ for *all* algorithms requires additional assumptions. For example, under the Exponential Time Hypothesis (ETH), TAUTOLOGY (and hence SUFFICIENCY-CHECK in the succinct encoding) requires $2^{\Omega(n)}$ time, and SUFFICIENCY-CHECK inherits this conditional lower bound via our reduction.

% Note: the ETH implication is a worst-case lower bound in the succinct encoding, witnessed by the reduction family.

Maintaining $k$ extra parameters incurs:
\begin{itemize}
\item Documentation cost: $O(k)$ entries
\item Testing cost: $O(k)$ test cases
\item Migration cost: $O(k)$ parameters to update
\end{itemize}

Total maintenance cost is $C_{\text{over}}(k) = O(k)$.

For concrete threshold: when $n = 20$ parameters, exhaustive search requires $2^{20} \approx 10^6$ checks. Including $k = 5$ extra parameters costs $O(5)$ maintenance overhead but avoids $10^6$ computational work.

Since $2^n$ grows faster than any polynomial in $k$ or $n$, there exists $n_0$ such that for all $n > n_0$:
\[
C_{\text{over}}(k) \ll C_{\text{find}}(n)
\]

Adding underspecification risk $C_{\text{under}}$ (production failures from missing parameters), which is unbounded in the model, makes over-specification strictly dominant.
\end{proof}

\begin{corollary}[Impossibility of Automated Configuration Minimization]
\label{cor:no-auto-minimize}
Assuming $\Pclass \neq \coNP$, there exists no polynomial-time algorithm that:
\begin{enumerate}
\item Takes an arbitrary configuration file with $n$ parameters
\item Identifies the minimal sufficient parameter subset
\item Guarantees correctness (no false negatives)
\end{enumerate}
\end{corollary}

\begin{proof}
Such an algorithm would solve MINIMUM-SUFFICIENT-SET in polynomial time, contradicting Theorem~\ref{thm:minsuff-conp} (assuming $\Pclass \neq \coNP$).
\end{proof}

\begin{remark}
Corollary~\ref{cor:no-auto-minimize} explains the observed absence of ``config cleanup'' tools in software engineering practice. Engineers who include extra parameters are not exhibiting poor discipline---they are adapting to computational constraints. The problem is not lack of tooling effort; it is mathematical intractability.
\end{remark}

\subsection{Connection to Observed Practice}

These theorems provide mathematical grounding for three widespread engineering behaviors:

\textbf{1. Configuration files grow over time.} Removing parameters requires solving \coNP-complete problems. Engineers rationally choose linear maintenance cost over exponential minimization cost.

\textbf{2. Heuristic model selection dominates.} ML practitioners use AIC, BIC, cross-validation instead of optimal feature selection because optimal selection is intractable (Theorem~\ref{thm:rational-overmodel}).

\textbf{3. ``Include everything'' is a legitimate strategy.} When determining relevance/minimal sufficiency is exponential in the worst case (e.g., $O(2^n)$ by naive subset enumeration, and under ETH a $2^{\Omega(n)}$ worst-case lower bound in the succinct encoding; see Theorem~\ref{thm:dichotomy}), including all $n$ parameters costs $O(n)$. For large $n$, this is the rational choice.

These behaviors are not ad hoc workarounds; under the stated computational model they are rational responses to worst-case intractability. The complexity results provide a mathematical lens: over-modeling is not a failure---it is the rational strategy under the model.
