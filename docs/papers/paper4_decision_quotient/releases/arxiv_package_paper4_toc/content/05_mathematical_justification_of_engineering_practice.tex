\section{Implications for Practice: Diagnostic Reading of Over-Modeling}\label{sec:engineering-justification}

This section states corollaries for engineering practice. Within the formal model, the complexity results of Sections~\ref{sec:hardness} and~\ref{sec:dichotomy} shift parts of this workflow from informal judgment toward explicit, checkable criteria. The observed behaviors---configuration over-specification, absence of automated minimization tools, heuristic model selection---are best read as \emph{diagnostic signals} that the decision boundary is not yet fully characterized in a tractable representation.

Some common prescriptions implicitly require exact minimization of sufficient parameter sets. In the worst case, that task is \coNP-complete in our model, so critiques should distinguish between true irrelevance and unresolved relevance structure under the chosen encoding.
The interpretive key for this section is Theorem~\ref{thm:overmodel-diagnostic}: persistent failure to isolate a minimal sufficient set should be read as a boundary-characterization signal in the current model, not as a universal irreducibility claim.

\subsection{Configuration Simplification is SUFFICIENCY-CHECK}

Real engineering problems reduce directly to the decision problems studied in this paper.

\begin{theorem}[Configuration Simplification Reduces to SUFFICIENCY-CHECK]
\label{thm:config-reduction}
Given a software system with configuration parameters $P = \{p_1, \ldots, p_n\}$ and observed behaviors $B = \{b_1, \ldots, b_m\}$, the problem of determining whether parameter subset $I \subseteq P$ preserves all behaviors is equivalent to SUFFICIENCY-CHECK.
\end{theorem}

\begin{proof}
Construct decision problem $\mathcal{D} = (A, X_1, \ldots, X_n, U)$ where:
\begin{itemize}
\item Actions $A = B$ (each behavior is an action)
\item Coordinates $X_i$ = domain of parameter $p_i$
\item State space $S = X_1 \times \cdots \times X_n$
\item Utility $U(b, s) = 1$ if behavior $b$ occurs under configuration $s$, else $U(b, s) = 0$
\end{itemize}

Then $\Opt(s) = \{b \in B : b \text{ occurs under configuration } s\}$.

Coordinate set $I$ is sufficient iff:
\[
s_I = s'_I \implies \Opt(s) = \Opt(s')
\]

This holds iff configurations agreeing on parameters in $I$ exhibit identical behaviors.

Therefore, ``does parameter subset $I$ preserve all behaviors?'' is exactly SUFFICIENCY-CHECK for the constructed decision problem.
\end{proof}

\begin{theorem}[Over-Modeling as Diagnostic Signal]
\label{thm:overmodel-diagnostic}
\label{cor:overmodel-diagnostic}
By contraposition of Theorem~\ref{thm:config-reduction}, if the minimal sufficient parameter set is not immediately identifiable in the modeled system, then the decision boundary is not completely characterized by the current parameterization.
\end{theorem}

\begin{proof}
Assume the decision boundary were completely characterized by the current parameterization. Then, via Theorem~\ref{thm:config-reduction}, the corresponding sufficiency instance would admit an exact statement of which parameter subsets preserve behavior, including the minimal sufficient set. Contraposition gives the claim: persistent failure to identify a minimal sufficient set signals incomplete characterization of decision relevance in the model.
\end{proof}

\subsection{Cost Asymmetry Under ETH}

We now prove a cost asymmetry result under the stated cost model and complexity constraints.\footnote{Naive subset enumeration still gives an intuitive baseline of $O(2^n)$ checks, but that is an algorithmic upper bound; the theorem below uses ETH for the lower-bound argument.}

\begin{theorem}[Cost-Asymmetry Consequence Under ETH]
\label{thm:rational-overmodel}
Consider an engineer specifying a system configuration with $n$ parameters. Let:
\begin{itemize}
\item $C_{\text{over}}(k)$ = cost of maintaining $k$ extra parameters beyond minimal
\item $C_{\text{find}}(n)$ = cost of finding minimal sufficient parameter set
\item $C_{\text{under}}$ = expected cost of production failures from underspecification
\end{itemize}

Assume ETH in the succinct encoding model of Section~\ref{sec:encoding}. Then:
\begin{enumerate}
\item Exact identification of minimal sufficient sets has worst-case finding cost $C_{\text{find}}(n)=2^{\Omega(n)}$. (Under ETH, SUFFICIENCY-CHECK has a $2^{\Omega(n)}$ lower bound in the succinct model, and exact minimization subsumes this decision task.)
\item Maintenance cost is linear: $C_{\text{over}}(k) = O(k)$.
\item Under ETH, exponential finding cost dominates linear maintenance cost for sufficiently large $n$.
\end{enumerate}

Therefore, there exists $n_0$ such that for all $n > n_0$, the finding-vs-maintenance asymmetry satisfies:
\[
C_{\text{over}}(k) < C_{\text{find}}(n) + C_{\text{under}}
\]

Within this model, persistent over-specification is evidence of unresolved boundary characterization rather than evidence that all included parameters are intrinsically necessary.
\end{theorem}

\begin{proof}
Under ETH, the TAUTOLOGY reduction used in Theorem~\ref{thm:sufficiency-conp} yields a $2^{\Omega(n)}$ worst-case lower bound for SUFFICIENCY-CHECK in the succinct encoding. Any exact algorithm that outputs a minimum sufficient set can decide whether the optimum size is $0$ by checking whether the returned set is empty; this is exactly the SUFFICIENCY-CHECK query for $I=\emptyset$. Hence exact minimal-set finding inherits the same exponential worst-case lower bound.

Maintaining $k$ extra parameters incurs:
\begin{itemize}
\item Documentation cost: $O(k)$ entries
\item Testing cost: $O(k)$ test cases
\item Migration cost: $O(k)$ parameters to update
\end{itemize}

Total maintenance cost is $C_{\text{over}}(k) = O(k)$.

Since $2^n$ grows faster than any polynomial in $k$ or $n$, there exists $n_0$ such that for all $n > n_0$:
\[
C_{\text{over}}(k) \ll C_{\text{find}}(n)
\]

Adding underspecification risk $C_{\text{under}}$ reinforces conservative configurations until additional structure is identified.
\end{proof}

\begin{corollary}[Impossibility of Automated Configuration Minimization]
\label{cor:no-auto-minimize}
Assuming $\Pclass \neq \coNP$, there exists no polynomial-time algorithm that:
\begin{enumerate}
\item Takes an arbitrary configuration file with $n$ parameters
\item Identifies the minimal sufficient parameter subset
\item Guarantees correctness (no false negatives)
\end{enumerate}
\end{corollary}

\begin{proof}
Such an algorithm would solve MINIMUM-SUFFICIENT-SET in polynomial time, contradicting Theorem~\ref{thm:minsuff-conp} (assuming $\Pclass \neq \coNP$).
\end{proof}

\begin{remark}
Corollary~\ref{cor:no-auto-minimize} explains the observed absence of ``config cleanup'' tools in software engineering practice. The correct interpretation is diagnostic: persistent inability to minimize indicates incomplete tractable characterization of decision relevance under the current representation.
\end{remark}

\subsection{Diagnostic Reading of Observed Practice}

These theorems provide mathematical grounding for three widespread engineering behaviors:

\textbf{1. Configuration files grow over time.} Difficulty removing parameters indicates unresolved relevance structure in the current model/encoding.

\textbf{2. Heuristic model selection dominates.} ML practitioners use AIC, BIC, cross-validation instead of optimal feature selection because exact selection is intractable without additional structure (Theorem~\ref{thm:rational-overmodel}).

\textbf{3. ``Include everything'' is a conservative upper-bound policy.} When determining relevance/minimal sufficiency has a $2^{\Omega(n)}$ worst-case lower bound under ETH in the succinct encoding (Theorem~\ref{thm:dichotomy}), including all $n$ parameters costs $O(n)$ while the exact boundary remains unresolved.

These behaviors are not ad hoc workarounds; under the stated computational model they are practical consequences of a boundary-identification bottleneck. The complexity results provide a diagnostic lens: persistent over-modeling should prompt either stronger structural assumptions or explicit acceptance of approximation.
