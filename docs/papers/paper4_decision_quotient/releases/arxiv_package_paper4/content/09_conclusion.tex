\section{Conclusion}

We have presented a Lean 4 framework for formalizing polynomial-time reductions and demonstrated it through a comprehensive complexity analysis of decision-relevant information.

\subsection*{Formalization Contributions}

The primary contributions are methodological:

\begin{enumerate}
\item \textbf{Reusable reduction infrastructure.} The bundled \texttt{KarpReduction} type, polynomial bound tracking, and composition lemmas provide a foundation for future complexity formalizations.

\item \textbf{Demonstrated methodology.} Five complete reduction proofs (TAUTOLOGY, SET-COVER, ETH, W[2], and the Simplicity Tax) show that complexity-theoretic arguments are tractable to formalize with $\sim$600 lines per reduction.

\item \textbf{Integration patterns.} We show how to connect custom complexity definitions with Mathlib's computability and polynomial libraries.

\item \textbf{Artifact quality.} Zero \texttt{sorry} placeholders, documented axiom dependencies, and reproducible builds via \texttt{lake build}.
\end{enumerate}

\subsection*{Verified Complexity Results}

Through the case study, we machine-verified:

\begin{itemize}
\item Checking whether a coordinate set is sufficient is \coNP-complete
\item Finding the minimum sufficient set is \coNP-complete (the $\SigmaP{2}$ structure collapses)
\item Anchor sufficiency (fixed-coordinate subcube) is $\SigmaP{2}$-complete
\item A complexity dichotomy separates easy (logarithmic) from hard (linear) cases
\item Tractable subcases exist for bounded actions, separable utilities, and tree structures
\end{itemize}

These results establish a fundamental principle of rational decision-making under uncertainty:

\begin{quote}
\textbf{Determining what you need to know is harder than knowing everything.}
\end{quote}

This is not a metaphor or heuristic observation. It is a mathematical theorem with universal scope. Any agent facing structured uncertainty---whether a climate scientist, financial analyst, software engineer, or artificial intelligence---faces the same computational constraint. The ubiquity of over-modeling across domains is not coincidence, laziness, or poor discipline. It is the provably optimal response to intractability.

The principle has immediate normative force: stop criticizing engineers for including ``irrelevant'' parameters. Stop demanding minimal models. Stop building tools that promise to identify ``what really matters.'' These aspirations conflict with computational reality. The dichotomy theorem (Theorem~\ref{thm:dichotomy}) characterizes exactly when tractability holds; outside those boundaries, over-modeling is not a failure mode---it is the only rational strategy.

All proofs are machine-checked in Lean 4, ensuring correctness of the core mathematical claims including the reduction mappings and equivalence theorems. Complexity classifications follow from standard complexity-theoretic results (TAUTOLOGY is coNP-complete, $\exists\forall$-SAT is $\Sigma_2^\text{P}$-complete) under the encoding model described in Section~\ref{sec:hardness}.

\subsection*{Why These Results Are Final}

The theorems proven here are \emph{ceiling results}---no stronger claims are possible within their respective frameworks:

\begin{enumerate}
\item \textbf{Exact complexity characterization (not just lower bounds).} We prove SUFFICIENCY-CHECK is \coNP-complete (Theorem~\ref{thm:sufficiency-conp}). This is \emph{both} a lower bound (\coNP-hard) and an upper bound (in \coNP). The complexity class is exact. Additional lower or upper bounds would be redundant.

\item \textbf{Universal impossibility ($\forall$), not probabilistic prevalence ($\mu = 1$).} Theorems quantify over \emph{all} decision problems satisfying the structural constraints, not measure-1 subsets. Measure-theoretic claims like ``hard instances are prevalent'' would \emph{weaken} the results from ``always hard (unless P = coNP)'' to ``almost always hard.''

\item \textbf{Constructive reductions, not existence proofs.} Theorem~\ref{thm:sufficiency-conp} provides an explicit polynomial-time reduction from TAUTOLOGY to SUFFICIENCY-CHECK. This is stronger than proving hardness via non-constructive arguments (e.g., diagonalization). The reduction is machine-checked and executable.

\item \textbf{Dichotomy is complete (Theorem~\ref{thm:dichotomy}).} The complexity separates into exactly two cases: polynomial (when minimal sufficient set has size $O(\log |S|)$) or exponential (when size is $\Omega(n)$). Under standard assumptions (\Pclass{} $\neq$ \coNP), there are no intermediate cases. The dichotomy is exhaustive.

\item \textbf{Tractable cases are maximal (Section~\ref{sec:tractable}).} The tractability conditions (bounded actions, separable utilities, tree structure) are shown to be \emph{tight}---relaxing any condition yields \coNP-hardness. These are the boundary cases, not a subset of tractable instances.
\end{enumerate}

\textbf{What would NOT strengthen the results:}

\begin{itemize}
\item \textbf{Additional complexity classes:} SUFFICIENCY-CHECK is \coNP-complete. Proving it is also NP-hard, PSPACE-hard, or \#P-hard would add no information (these follow from \coNP-completeness under standard reductions).

\item \textbf{Average-case hardness:} We prove worst-case hardness. Average-case results would be \emph{weaker} (average $\leq$ worst) and would require distributional assumptions not present in the problem definition.

\item \textbf{\#P-hardness of counting:} When the decision problem is asking ``does there exist?'' (existential) or ``are all?'' (universal), the corresponding counting problem is trivially at least as hard. Proving \#P-hardness separately would be redundant unless we change the problem to count something else.

\item \textbf{Approximation hardness beyond inapproximability:} The coNP-completeness of MINIMUM-SUFFICIENT-SET (Theorem~\ref{thm:minsuff-conp}) implies no polynomial-time algorithm can approximate the minimal sufficient set size within any constant factor (unless P = coNP). This is maximal inapproximability---the problem admits no non-trivial approximation.
\end{itemize}

These results close the complexity landscape for coordinate sufficiency. Within classical complexity theory, the characterization is complete.

\subsection*{The Simplicity Tax: A Major Practical Consequence}

A widespread belief holds that ``simpler is better''---that preferring simple tools and minimal models is a mark of sophistication. This paper proves that belief is context-dependent and often wrong.

The \emph{Simplicity Tax Theorem} (Section~\ref{sec:simplicity-tax}) establishes: when a problem requires $k$ axes of variation and a tool natively supports only $j < k$ of them, the remaining $k - j$ axes must be handled externally at \emph{every use site}. For $n$ use sites, the ``simpler'' tool creates $(k-j) \times n$ units of external work. A tool matched to the problem's complexity creates zero external work.

\begin{quote}
\textbf{True sophistication is matching tool complexity to problem complexity.}
\end{quote}

Preferring ``simple'' tools for complex problems is not wisdom---it is a failure to account for distributed costs. The simplicity tax is paid invisibly, at every use site, by every user, forever. The sophisticated engineer asks not ``which tool is simpler?'' but ``which tool matches my problem's intrinsic complexity?''

This result is machine-checked in Lean 4 (\texttt{HardnessDistribution.lean}). The formalization proves conservation (you can't eliminate the tax, only redistribute it), dominance (complete tools always beat incomplete tools), and the amortization threshold (beyond which the ``complex'' tool is strictly cheaper).

\subsection*{The Foundational Contribution}

This paper proves a universal constraint on optimization under uncertainty. The constraint is:
\begin{itemize}
\item \textbf{Mathematical}, not empirical---it follows from the structure of computation
\item \textbf{Universal}, not domain-specific---it applies to any decision problem with coordinate structure
\item \textbf{Permanent}, not provisional---no algorithmic breakthrough can circumvent \coNP-completeness (unless P = coNP)
\end{itemize}

The result explains phenomena across disciplines: why feature selection uses heuristics, why configuration files grow, why sensitivity analysis is approximate, why model selection is art rather than science. These are not separate problems with separate explanations. They are manifestations of a single computational constraint, now formally characterized.

The Simplicity Tax Theorem adds a practical corollary: the universal response to intractability (over-modeling) is not just rational---attempting to avoid it by using ``simpler'' tools is actively counterproductive. Simplicity that mismatches problem complexity creates more work, not less.

Open questions remain (fixed-parameter tractability, quantum complexity, average-case behavior under natural distributions), but the foundational question---\emph{is identifying relevance fundamentally hard?}---is answered: yes.

\subsection*{Future Work}

Several directions extend this work:

\begin{enumerate}
\item \textbf{Mathlib integration.} Our reduction framework could be contributed to Mathlib's computability library, providing standard definitions for Karp reductions, NP/coNP membership, and polynomial bounds.

\item \textbf{Additional reductions.} The methodology extends to other classical reductions (3-SAT to CLIQUE, HAMPATH to TSP, etc.). A library of machine-verified reductions would be valuable for both education and research.

\item \textbf{Automation.} The patterns identified in Section~\ref{sec:engineering} suggest opportunities for tactic development. A \texttt{complexity} tactic analogous to \texttt{continuity} could automate routine reduction steps.

\item \textbf{Turing machine formalization.} Our current work operates at the reduction level, assuming polynomial-time bounds. Full Turing machine formalization would enable end-to-end verification from machine model to complexity class.

\item \textbf{Parameterized complexity library.} W-hierarchy and FPT definitions are not yet in Mathlib. Our W[2]-hardness proof provides a starting point.
\end{enumerate}

\subsection*{Methodology Disclosure}

This paper was developed through human-AI collaboration. The author provided problem formulations, hardness conjectures, and proof strategies. Large language models (Claude) assisted with LaTeX drafting, Lean tactic exploration, and proof search.

The Lean 4 compiler served as the ultimate arbiter: proofs either compile or they don't. The validity of machine-checked theorems is independent of generation method. We disclose this methodology in the interest of academic transparency.


\appendix

