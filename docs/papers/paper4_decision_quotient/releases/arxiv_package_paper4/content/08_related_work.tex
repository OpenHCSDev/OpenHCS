\section{Related Work}\label{sec:related}

\subsection{Formalized Complexity Theory}

Machine verification of complexity-theoretic results remains sparse compared to other areas of mathematics. We survey existing work and position our contribution.

\paragraph{Coq formalizations.} Forster et al.~\cite{forster2019verified} developed a Coq library for computability theory, including undecidability proofs. Their work focuses on computability rather than complexity classes. Kunze et al.~\cite{kunze2019formal} formalized the Cook-Levin theorem in Coq, proving SAT is NP-complete. Our work extends this methodology to coNP-completeness and approximation hardness.

\paragraph{Isabelle/HOL.} Nipkow and colleagues formalized substantial algorithm verification in Isabelle~\cite{nipkow2002isabelle}, but complexity-theoretic reductions are less developed. Recent work on algorithm complexity~\cite{haslbeck2021verified} provides time bounds for specific algorithms rather than hardness reductions.

\paragraph{Lean and Mathlib.} Mathlib's computability library~\cite{mathlib2020} provides primitive recursive functions and basic computability results. Our work extends this to polynomial-time reductions and complexity classes. To our knowledge, this is the first Lean 4 formalization of coNP-completeness proofs and approximation hardness.

\paragraph{The verification gap.} Published complexity proofs occasionally contain errors~\cite{lipton2009np}. Machine verification eliminates this uncertainty. Our contribution demonstrates that complexity reductions are amenable to formalization with reasonable effort ($\sim$5,600 lines for the full reduction suite).

\subsection{Computational Decision Theory}

The complexity of decision-making has been studied extensively. Papadimitriou~\cite{papadimitriou1994complexity} established foundational results on the complexity of game-theoretic solution concepts. Our work extends this to the meta-question of identifying relevant information. For a modern treatment of complexity classes, see Arora and Barak \cite{arora2009computational}.

\subsection{Feature Selection Complexity}

In machine learning, feature selection asks which input features are relevant for prediction. Blum and Langley~\cite{blum1997selection} survey the field, noting hardness in general settings. Amaldi and Kann~\cite{amaldi1998complexity} proved that finding minimum feature sets for linear classifiers is NP-hard, and established inapproximability bounds: no polynomial-time algorithm can approximate the minimum feature set within factor $2^{\log^{1-\epsilon} n}$ unless NP $\subseteq$ DTIME$(n^{\text{polylog } n})$.

Our results extend this line: the decision-theoretic analog (SUFFICIENCY-CHECK) is \coNP-complete, and MINIMUM-SUFFICIENT-SET inherits this hardness. The key insight is that sufficiency checking is ``dual'' to feature selection---rather than asking which features predict a label, we ask which coordinates determine optimal action. The \coNP{} (rather than NP) classification reflects this duality: insufficiency has short certificates (counterexample state pairs), while sufficiency requires universal verification.

\subsection{Sufficient Statistics}

Fisher~\cite{fisher1922mathematical} introduced sufficient statistics: a statistic $T(X)$ is \emph{sufficient} for parameter $\theta$ if the conditional distribution of $X$ given $T(X)$ does not depend on $\theta$. Lehmann and Scheff√©~\cite{lehmann1950completeness} characterized minimal sufficient statistics and their uniqueness properties.

Our coordinate sufficiency is the decision-theoretic analog: a coordinate set $I$ is sufficient if knowing $s_I$ determines optimal action, regardless of the remaining coordinates. The parallel is precise:
\begin{itemize}
\item \textbf{Statistics:} $T$ is sufficient $\iff$ $P(X | T(X), \theta) = P(X | T(X))$
\item \textbf{Decisions:} $I$ is sufficient $\iff$ $\Opt(s) = \Opt(s')$ whenever $s_I = s'_I$
\end{itemize}

Fisher's factorization theorem provides a characterization; our Theorem~\ref{thm:minsuff-conp} shows that \emph{finding} minimal sufficient statistics (in the decision-theoretic sense) is computationally hard.

\subsection{Causal Inference and Adjustment Sets}

Pearl~\cite{pearl2009causality} and Spirtes et al.~\cite{spirtes2000causation} developed frameworks for identifying causal effects from observational data. A central question is: which variables must be adjusted for to identify a causal effect? The \emph{adjustment criterion} and \emph{back-door criterion} characterize sufficient adjustment sets.

Our sufficiency problem is analogous: which coordinates must be observed to determine optimal action? The complexity results suggest that optimal adjustment set selection may also be intractable---a conjecture supported by recent work on the complexity of causal discovery~\cite{chickering2004large}.

The connection runs deeper: Shpitser and Pearl~\cite{shpitser2006identification} showed that identifying causal effects is NP-hard in general graphs. Our \coNP-completeness result for SUFFICIENCY-CHECK is the decision-theoretic counterpart.

\subsection{Minimum Description Length and Kolmogorov Complexity}

The Minimum Description Length (MDL) principle~\cite{rissanen1978modeling, grunwald2007minimum} formalizes model selection as compression: the best model minimizes description length of data plus model. Kolmogorov complexity~\cite{li2008introduction} provides the theoretical foundation---the shortest program that generates the data.

Our decision quotient connects to this perspective: a coordinate set $I$ is sufficient if it compresses the decision problem without loss---knowing $s_I$ is as good as knowing $s$ for decision purposes. The minimal sufficient set is the MDL-optimal compression of the decision problem.

The complexity results explain why MDL-based model selection uses heuristics: finding the true minimum description length is uncomputable (Kolmogorov complexity) or intractable (MDL approximations). Our results show the decision-theoretic analog is \coNP-complete---intractable but decidable.

\subsection{Value of Information}

The value of information (VOI) framework~\cite{howard1966information} quantifies how much a decision-maker should pay for information. Our work addresses a different question: not the \emph{value} of information, but the \emph{complexity} of identifying which information has value.

Interestingly, VOI is typically polynomial to compute given the decision problem structure, while identifying which information \emph{to value} (our problem) is \coNP-complete. This separation explains why VOI is practical while optimal sensor placement remains heuristic.

\subsection{Sensitivity Analysis}

Sensitivity analysis asks how outputs change with inputs. Local sensitivity (derivatives) is polynomial; global sensitivity (Sobol indices~\cite{sobol2001global}) requires sampling. Identifying which inputs \emph{matter} for decision-making is our sufficiency problem---which we show is \coNP-complete.

This explains why practitioners use sampling-based sensitivity analysis rather than exact methods: exact identification of decision-relevant inputs is intractable. The dichotomy theorem (Theorem~\ref{thm:dichotomy}) characterizes when sensitivity analysis becomes tractable (logarithmic relevant inputs) versus intractable (linear relevant inputs).

\subsection{Model Selection}

Statistical model selection (AIC~\cite{akaike1974new}, BIC~\cite{schwarz1978estimating}, cross-validation~\cite{stone1974cross}) provides practical heuristics for choosing among models. Our results provide theoretical justification: optimal model selection is intractable, so heuristics are necessary.

The Simplicity Tax Theorem (Section~\ref{sec:simplicity-tax}) adds a warning: model selection heuristics that favor ``simpler'' models may incur hidden costs when the true model is complex. The simplicity preference fallacy---choosing low-parameter models without accounting for per-site costs---is the decision-theoretic formalization of overfitting-by-underfitting.


