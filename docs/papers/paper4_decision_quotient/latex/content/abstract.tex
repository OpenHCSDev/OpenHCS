Engineers routinely include irrelevant information in their models. Climate scientists model atmospheric chemistry when predicting regional temperatures. Financial analysts track hundreds of indicators when making portfolio decisions. Software architects specify dozens of configuration parameters when only a handful affect outcomes.

This paper proves that such \emph{over-modeling} is not laziness---it is computationally rational. Identifying precisely which variables are ``decision-relevant'' is \coNP-complete \cite{cook1971complexity, karp1972reducibility}, finding the \emph{minimum} set of relevant variables is \coNP-complete, and a fixed-coordinate ``anchor'' version is $\SigmaP{2}$-complete \cite{stockmeyer1976polynomial}. These results formalize a fundamental insight:

\begin{quote}
\textbf{Determining what you need to know is harder than knowing everything.}
\end{quote}

We introduce the \emph{decision quotient}---a measure of decision-relevant complexity---and prove a complexity dichotomy: checking sufficiency is polynomial when the minimal sufficient set has logarithmic size, but exponential when it has linear size. We identify tractable subcases (bounded actions, separable utilities, tree-structured dependencies) that admit polynomial algorithms.

A major practical consequence is the \emph{Simplicity Tax Theorem}: using a simple tool for a complex problem is necessarily harder than using a tool matched to the problem's complexity. When a tool lacks native support for required dimensions, users must supply that information externally at every use site. The ``simpler'' tool creates more total work, not less. This overturns the common intuition that ``simpler is always better''---simplicity is only a virtue when the problem is also simple.

\textbf{These are ceiling results:} The complexity characterizations are exact (both upper and lower bounds). The theorems quantify universally over all problem instances ($\forall$), not probabilistically ($\mu = 1$). The dichotomy is complete---no intermediate cases exist under standard assumptions. The tractability conditions are maximal---relaxing any yields hardness. No stronger complexity claims are possible within classical complexity theory.

All results are machine-checked in Lean 4 \cite{moura2021lean4} ($\sim$5,000 lines across 33 files, 200+ theorems). The Lean formalization proves: (1) polynomial-time reduction composition; (2) correctness of the TAUTOLOGY and $\exists\forall$-SAT reduction mappings; (3) equivalence of sufficiency checking with coNP/$\Sigma_2^\text{P}$-complete problems under standard encodings; (4) the Simplicity Tax Theorem including conservation, dominance, and the amortization threshold. Complexity classifications (coNP-complete, $\SigmaP{2}$-complete) are derived by combining these machine-checked results with the well-known complexity of TAUTOLOGY and $\exists\forall$-SAT.

\textbf{Keywords:} computational complexity, decision theory, model selection, coNP-completeness, polynomial hierarchy, simplicity tax, Lean 4