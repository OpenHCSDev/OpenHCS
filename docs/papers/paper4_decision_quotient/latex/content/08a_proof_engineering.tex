\section{Proof Engineering Insights}\label{sec:engineering}

This section discusses lessons learned from formalizing complexity-theoretic reductions in Lean 4, intended to guide future formalization efforts.

\subsection{Patterns That Worked}

\paragraph{Bundled reductions.} Packaging the reduction function, correctness proof, and polynomial bound into a single structure (\texttt{KarpReduction}) was essential. Early attempts using separate lemmas led to proof state explosion when composing reductions.

\paragraph{Definitional equality for simple cases.} We defined concepts so that simple cases reduce definitionally whenever the definitions admit it:
\begin{verbatim}
-- Sufficiency of all coordinates is definitionally true
example (D : DecisionProblemWithCoords n) :
    Sufficient D Finset.univ := fun _ _ _ => rfl
\end{verbatim}

\paragraph{Separation of polynomial bounds.} We prove polynomial-time bounds separately from correctness, then combine. This mirrors the structure of pen-and-paper proofs and makes debugging easier.

\paragraph{Explicit size functions.} Rather than relying on implicit encodings, we define explicit \texttt{size} functions for each type. This avoids universe issues and makes polynomial bounds concrete:
\begin{verbatim}
def size_formula : Formula → ℕ
  | .var _ => 1
  | .not φ => 1 + size_formula φ
  | .and φ ψ => 1 + size_formula φ + size_formula ψ
  | .or φ ψ => 1 + size_formula φ + size_formula ψ
\end{verbatim}

\subsection{Patterns That Failed}

\paragraph{Unbundled type classes.} Early attempts used type classes for complexity properties:
\begin{verbatim}
class InNP (P : DecisionProblem α) where
  witness_type : Type*
  verify : α → witness_type → Prop
  ...
\end{verbatim}
This failed because instance search couldn't handle the necessary universe polymorphism. Bundled structures with explicit witnesses worked better.

\paragraph{Definitional unfolding for reductions.} Attempting to make reduction correctness hold by \texttt{rfl} led to unwieldy definitions. It's better to accept that \texttt{correct} requires a short proof.

\paragraph{Direct SAT encoding.} Our first reduction encoded SAT variables as coordinates directly. This required dependent types indexed by the number of variables, causing universe issues. The solution: encode via finite types with explicit bounds.

\subsection{Challenges Specific to Complexity Theory}

\paragraph{Polynomial composition.} Proving that polynomial-time reductions compose to polynomial-time requires polynomial arithmetic. Mathlib's \texttt{Polynomial} provides this, but connecting abstract polynomials to concrete time bounds requires care.

\paragraph{The oracle model.} For $\SigmaP{2}$-completeness, we need oracle Turing machines. We model these abstractly:
\begin{verbatim}
structure OracleTM (Oracle : Type*) where
  query : Oracle → Bool
  compute : (Oracle → Bool) → Input → Output
\end{verbatim}
Full Turing machine formalization is future work; our proofs work at the reduction level.

\paragraph{ETH and SETH.} Conditional lower bounds require assuming the Exponential Time Hypothesis. We encode this as an axiom in a separate file, clearly marked:
\begin{verbatim}
-- This is an ASSUMPTION, not a theorem
axiom ETH : ¬∃ (f : Formula → Bool),
  (∀ φ, f φ = true ↔ Satisfiable φ) ∧
  ∃ c < 2, ∀ n, time_on_size f n ≤ c ^ n
\end{verbatim}

\paragraph{Parameterized complexity.} The W-hierarchy requires careful definition. We model W[t] via weighted satisfiability:
\begin{verbatim}
def InW (t : ℕ) (P : DecisionProblem α) : Prop :=
  ∃ (reduce : α → WeightedFormula t),
    ∀ x, P x ↔ (reduce x).satisfiable
\end{verbatim}

\subsection{Automation Opportunities}

Automation targets for repetitive proof patterns:

\begin{enumerate}
\item \textbf{Reduction templates:} Given a mapping and correctness statement, generate the \texttt{KarpReduction} structure.

\item \textbf{Polynomial bound synthesis:} Given a recursive function, synthesize its polynomial bound from the recurrence.

\item \textbf{Witness extraction:} For NP membership, automatically extract witness types from existential statements.

\item \textbf{Counterexample search:} For coNP-hardness, search for counterexamples to proposed reductions.
\end{enumerate}

We implemented (1) as a macro; (2)--(4) remain manual. A \texttt{complexity} tactic analogous to \texttt{continuity} or \texttt{measurability} is the direct extension for automating routine reduction steps.

\subsection{Recommendations for Future Work}

\paragraph{Start with membership, then hardness.} Proving ``$P \in \text{coNP}$'' is usually easier than ``$P$ is coNP-hard.'' The membership proof clarifies the witness structure needed for the hardness reduction.

\paragraph{Formalize the target problem first.} Before reducing from TAUTOLOGY to SUFFICIENCY-CHECK, we formalized SUFFICIENCY-CHECK completely. This caught encoding issues early.

\paragraph{Use \texttt{\#print axioms} continuously.} We ran axiom checks after each major lemma. This caught unintended classical dependencies in constructive components.

\paragraph{Separate ``math'' from ``encoding.''} Keep the mathematical content (``sufficiency is the same as tautology under this encoding'') separate from encoding details (``how to represent formulas as coordinates''). This separation aids both clarity and reuse.

\subsection{Lines of Code by Component}

\begin{center}
\begin{tabular}{lr}
\hline
\textbf{Component} & \textbf{Lines} \\
\hline
Core definitions (Basic, Finite, Quotient) & 285 \\
Sufficiency \& Computation & 493 \\
Polynomial reductions & 530 \\
Hardness proofs (SAT, QBF, $\Sigma_2^P$, ETH) & 1,471 \\
Tractability (bounded, separable, tree, FPT) & 471 \\
Dichotomy \& complexity main & 275 \\
Query complexity \& information & 418 \\
Algorithm complexity & 170 \\
Instances \& summary & 250 \\
\hline
\textbf{Total} & \textbf{$\sim$5,600} \\
\hline
\end{tabular}
\end{center}

The hardness proofs constitute 26\% of the codebase, with tractability results at 8\%. Core infrastructure (definitions, reductions, computation) accounts for 23\%, suggesting good reuse potential for other complexity formalizations.
