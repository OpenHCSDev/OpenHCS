\begin{abstract}
We study the \emph{identification capacity} of classification systems---the ability to determine which class an entity belongs to given limited observations. An observer receives binary query responses (``does entity $v$ satisfy attribute $I$?'') and must identify the entity's class. We prove fundamental limits on identification cost as a function of observation model.

\textbf{Information barrier.} When distinct classes share identical attribute profiles, no algorithm---regardless of computational power---can identify the class from attribute queries alone. This is a channel capacity result: the mutual information $I(C; \pi(V)) < H(C)$ when the attribute mapping $\pi$ is not injective on classes.

\textbf{Rate-identification tradeoff.} We analyze the tradeoff between \emph{tag rate} $L$ (bits per entity), \emph{identification cost} $W$ (queries per identification), and \emph{error rate} $D$. A nominal tag achieving $L = \lceil \log_2 k \rceil$ bits for $k$ classes yields $W = O(1)$ with $D = 0$. Without tags ($L = 0$), identification requires $W = \Omega(n)$ queries and may incur $D > 0$.

\textbf{Converse.} Any scheme achieving $D = 0$ requires $L \geq \log_2 k$ bits. This is tight: nominal tagging achieves the bound with $W = O(1)$.

\textbf{Matroid structure.} Minimal sufficient query sets form the bases of a matroid. The \emph{identification dimension}---common cardinality of all minimal query sets---is well-defined and computable.

\textbf{Applications.} The theory instantiates to type systems (duck vs.\ nominal typing), databases (attribute vs.\ key lookup), and biological taxonomy (phenotype vs.\ species identifier). The unbounded gap $\Omega(n)$ vs.\ $O(1)$ explains convergence toward hybrid systems in practice.

All results are machine-checked in Lean 4 (6,000+ lines, 0 \texttt{sorry}).

\textbf{Keywords:} identification capacity, query complexity, rate-distortion, matroid structure, classification systems
\end{abstract}

