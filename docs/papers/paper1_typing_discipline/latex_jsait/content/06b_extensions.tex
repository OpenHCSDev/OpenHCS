\subsection{Noisy Query Model}

Throughout this paper, queries are deterministic: $q_I(v) \in \{0,1\}$ is a fixed function of $v$. In practice, observations may be corrupted. We sketch an extension to noisy queries and state the resulting open problems.

\begin{definition}[Noisy observation channel]
A \emph{noisy observation channel} with crossover probability $\epsilon \in [0, 1/2)$ returns:
\[
\tilde{q}_I(v) = \begin{cases}
q_I(v) & \text{with probability } 1 - \epsilon \\
1 - q_I(v) & \text{with probability } \epsilon
\end{cases}
\]
Each query response is an independent BSC$(\epsilon)$ corruption of the true value.
\end{definition}

\begin{definition}[Noisy identification capacity]
The \emph{$\epsilon$-noisy identification capacity} is the supremum rate (in bits per entity) at which zero-error identification is achievable when all attribute queries pass through a BSC$(\epsilon)$.
\end{definition}

In the noiseless case ($\epsilon = 0$), Theorem~\ref{thm:identification-capacity} shows the capacity is binary: $\log_2 k$ if $\pi$ is class-injective, $0$ otherwise. For $\epsilon > 0$, several questions arise.

\textbf{Open problem (noisy identification cost).} For $\epsilon > 0$ and class-injective $\pi$, zero-error identification is impossible with finite queries (since BSC has nonzero error probability). With bounded error $\delta > 0$, we expect the identification cost to scale as $W = \Theta\left(\frac{\log(1/\delta)}{(1 - 2\epsilon)^2}\right)$ queries per entity. A key observation is that a nominal tag of $L \geq \lceil \log_2 k \rceil$ bits (transmitted noiselessly) should restore $O(1)$ identification regardless of query noise.

The third point is the key insight: \emph{nominal tags provide a noise-free side channel}. Even when attribute observations are corrupted, a clean tag enables $O(1)$ identification. This strengthens the case for nominal tagging in noisy environments, precisely the regime where ``duck typing'' would require many repeated queries to achieve confidence.

\textbf{Connection to identification via channels.} The noisy model connects more directly to Ahlswede-Dueck identification \cite{ahlswede1989identification}. In their framework, identification capacity over a noisy channel can exceed Shannon capacity (double-exponential codebook sizes). Our setting differs: we have \emph{adaptive queries} rather than block codes, and the decoder must identify a \emph{class} rather than test a hypothesis. Characterizing the interplay between adaptive query strategies and channel noise is an open problem.

\subsection{Rate-Distortion-Query Tradeoff Surface}

The $(L, W, D)$ tradeoff admits a natural geometric interpretation. We have identified the unique Pareto-optimal point at $D = 0$ (Theorem~\ref{thm:lwd-optimal}), but the full tradeoff surface contains additional structure.

\textbf{Fixed-$W$ slices.} For fixed query budget $W$, what is the minimum tag rate $L$ to achieve distortion $D$? When $W \geq d$ (the distinguishing dimension), zero distortion is achievable with $L = 0$ via exhaustive querying. When $W < d$, the observer cannot distinguish all classes, and either:
\begin{itemize}
\item Accept $D > 0$ (misidentification), or
\item Add tags ($L > 0$) to compensate for insufficient queries.
\end{itemize}

\textbf{Fixed-$L$ slices.} For fixed tag rate $L < \log_2 k$, the tag partitions the $k$ classes into $2^L$ groups. Within each group, the observer must use queries to distinguish. The query cost is determined by the distinguishing dimension \emph{within each group}, which is potentially much smaller than the global dimension.

\textbf{Open problem (subadditivity of query cost).} For a tag of rate $L$ partitioning classes into groups $G_1, \ldots, G_{2^L}$, we expect $W(L) \leq \max_i d(G_i)$, where $d(G_i)$ is the distinguishing dimension within group $G_i$. Optimal tag design should minimize this maximum. Characterizing the optimal partition remains open.

\subsection{Semantic Distortion Measures}

We have treated distortion $D$ as binary (correct identification or not). Richer distortion measures are possible:

\begin{itemize}
\item \textbf{Hierarchical distortion}: Misidentifying a class within the same genus (biological) or module (type system) is less severe than cross-genus errors.
\item \textbf{Weighted distortion}: Some misidentifications have higher cost than others (e.g., type errors causing security vulnerabilities vs. benign type confusion).
\end{itemize}

\subsection{Connection to Rate-Distortion-Perception Theory}

Blau and Michaeli \cite{blau2019rethinking} extended classical rate-distortion theory by adding a \emph{perception} constraint: the reconstructed distribution must match a target distribution under some divergence measure. This creates a three-way tradeoff between rate, distortion, and perceptual quality.

Our $(L, W, D)$ framework admits a parallel interpretation. The query cost $W$ plays a role analogous to the perception constraint: it measures the \emph{interactive cost} of achieving low distortion, rather than a distributional constraint. Just as rate-distortion-perception theory asks ``what is the minimum rate to achieve distortion $D$ while satisfying perception constraint $P$?'', we ask ``what is the minimum tag rate $L$ to achieve distortion $D$ with query budget $W$?''

The analogy suggests several directions:
\begin{itemize}
\item \textbf{Perception as identification fidelity}: In classification systems that must preserve statistical properties (e.g., sampling from a type distribution), a perception constraint would require the observer's class estimates to have the correct marginal distribution, not just low expected error.
\item \textbf{Three-resource tradeoffs}: The $(L, W, D)$ Pareto frontier (Theorem~\ref{thm:lwd-optimal}) is a discrete analogue of the rate-distortion-perception tradeoff surface. Characterizing this surface for specific classification problems would extend the geometric rate-distortion program to identification settings.
\end{itemize}

Formalizing these connections would unify identification capacity with the broader rate-distortion-perception literature.

