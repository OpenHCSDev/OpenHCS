\subsection{Semantic Compression: The Problem}

The fundamental problem of semantic compression is: given a value $v$ from a large space $\mathcal{V}$, how can we represent $v$ compactly while preserving the ability to answer semantic queries about $v$?

Classical rate-distortion theory \cite{shannon1959coding} studies the tradeoff between representation size and reconstruction fidelity. We extend this framework to a discrete classification setting with three dimensions: \emph{tag length} $L$ (storage cost), \emph{witness cost} $W$ (query complexity), and \emph{distortion} $D$ (semantic fidelity).

\subsection{Universe of Discourse}

\begin{definition}[Classification scheme]
A \emph{classification scheme} is any procedure (deterministic or randomized), with arbitrary time and memory, whose only access to a value $v \in \mathcal{V}$ is via:
\begin{enumerate}
\item The \emph{observation family} $\Phi = \{q_I : I \in \mathcal{I}\}$, where $q_I(v) = 1$ iff $v$ satisfies attribute $I$; and optionally
\item A \emph{nominal-tag primitive} $\tau: \mathcal{V} \to \mathcal{T}$ returning an opaque type identifier.
\end{enumerate}
All theorems in this paper quantify over all such schemes.
\end{definition}

This definition is intentionally broad: schemes may be adaptive, randomized, or computationally unbounded. The constraint is \emph{observational}, not computational.

\begin{theorem}[Information barrier]\label{thm:info-barrier}
For all classification schemes with access only to $\Phi$ (no nominal tag), the output is constant on $\sim_\Phi$-equivalence classes. Therefore, no such scheme can compute any property that varies within a $\sim_\Phi$-class.
\end{theorem}

\begin{proof}
Let $v \sim_\Phi w$, meaning $q_I(v) = q_I(w)$ for all $I \in \mathcal{I}$. Any scheme's execution trace depends only on query responses. Since all queries return identical values for $v$ and $w$, the scheme cannot distinguish them. Any output must therefore be identical.
\end{proof}

\begin{proposition}[Model capture]\label{prop:model-capture}
Any real-world classification protocol whose evidence consists solely of attribute-membership queries is representable as a scheme in the above model. Conversely, any additional capability corresponds to adding new observations to $\Phi$.
\end{proposition}

This proposition forces any objection into a precise form: to claim the theorem does not apply, one must name the additional observation capability not in $\Phi$. ``Different universe'' is not a coherent objection---it must reduce to ``I have access to oracle $X \notin \Phi$.''

\subsection{The Two-Axis Model}

We adopt a two-axis model of semantic structure, where each value is characterized by:
\begin{itemize}
\item \textbf{Bases axis ($B$)}: The inheritance lineage---which types the value inherits from
\item \textbf{Structure axis ($S$)}: The interface signature---which methods/attributes the value provides
\end{itemize}

\begin{definition}[Two-axis representation]
A value $v \in \mathcal{V}$ has representation $(B(v), S(v))$ where:
\begin{align}
B(v) &= \text{MRO}(\text{type}(v)) \quad \text{(Method Resolution Order)} \\
S(v) &= \pi(v) = (q_I(v))_{I \in \mathcal{I}} \quad \text{(interface profile)}
\end{align}
\end{definition}

\begin{theorem}[Model completeness]\label{thm:model-completeness}
In any class system with explicit inheritance, the pair $(B, S)$ is complete: every semantic property of a value is a function of $(B(v), S(v))$.
\end{theorem}

\begin{proof}
The proof proceeds by showing that any additional property (e.g., module location, metadata) is either derived from $(B, S)$ or stored within the namespace (part of $S$). In Python, \texttt{type(name, bases, namespace)} is the universal type constructor, making $(B, S)$ constitutive.
\end{proof}

\subsection{Interface Equivalence and Observational Limits}

Recall from Section 1 the interface equivalence relation:

\begin{definition}[Interface equivalence (restated)]
Values $v, w \in \mathcal{V}$ are interface-equivalent, written $v \sim w$, iff $\pi(v) = \pi(w)$---i.e., they satisfy exactly the same interfaces.
\end{definition}

\begin{proposition}[Equivalence class structure]
The relation $\sim$ partitions $\mathcal{V}$ into equivalence classes. Let $\mathcal{V}/{\sim}$ denote the quotient space. An interface-only observer effectively operates on $\mathcal{V}/{\sim}$, not $\mathcal{V}$.
\end{proposition}

\begin{corollary}[Information loss quantification]
The information lost by interface-only observation is:
\[
H(\mathcal{V}) - H(\mathcal{V}/{\sim}) = H(\mathcal{V} | \pi)
\]
where $H$ denotes entropy. This quantity is positive whenever multiple types share the same interface profile.
\end{corollary}

\subsection{Witness Cost: Query Complexity for Semantic Properties}

\begin{definition}[Witness procedure]
A \emph{witness procedure} for property $P: \mathcal{V} \to Y$ is an algorithm $A$ that:
\begin{enumerate}
\item Takes as input a value $v \in \mathcal{V}$ (via query access only)
\item Makes queries to the primitive set $\Phi_{\mathcal{I}}^+$
\item Outputs $P(v)$
\end{enumerate}
\end{definition}

\begin{definition}[Witness cost]
The \emph{witness cost} of property $P$ is:
\[
W(P) = \min_{A \text{ computes } P} c(A)
\]
where $c(A)$ is the worst-case number of primitive queries made by $A$.
\end{definition}

\begin{remark}[Relationship to query complexity]
Witness cost is a form of query complexity \cite{buhrman2002complexity} specialized to semantic properties. Unlike Kolmogorov complexity, $W$ is computable and depends on the primitive set, not a universal machine.
\end{remark}

\begin{lemma}[Witness cost lower bounds]
For any property $P$:
\begin{enumerate}
\item If $P$ is interface-computable: $W(P) \leq |\mathcal{I}|$
\item If $P$ varies within some $\sim$-class: $W(P) = \infty$ for interface-only observers
\item With nominal-tag access: $W(\text{type-identity}) = O(1)$
\end{enumerate}
\end{lemma}

\subsection{The $(L, W, D)$ Tradeoff}

We now define the three-dimensional tradeoff space that characterizes observation strategies.

\begin{definition}[Tag length (Rate)]
The \emph{tag length} $L$ is the number of machine words required to store a type identifier per value:
\[
L = \begin{cases}
O(1) & \text{if nominal tags are stored} \\
0 & \text{if no explicit tags}
\end{cases}
\]
Under a fixed word size $w$ bits, $L = O(1)$ corresponds to $\Theta(w)$ bits per value.
\end{definition}

\begin{definition}[Witness cost (Query complexity)]
The \emph{witness cost} $W$ is the minimum number of primitive queries required for type identity checking:
\[
W = W(\text{type-identity})
\]
\end{definition}

\begin{definition}[Distortion (Semantic fidelity)]
The \emph{distortion} $D$ is a worst-case semantic failure indicator:
\[
D = \begin{cases}
0 & \text{if } \forall v_1, v_2: \text{type}(v_1) = \text{type}(v_2) \Rightarrow \text{behavior}(v_1) \equiv \text{behavior}(v_2) \\
1 & \text{otherwise}
\end{cases}
\]
Here $\text{behavior}(v)$ denotes the observable behavior of $v$ under program execution (method dispatch outcomes, attribute access results).
\end{definition}

\begin{remark}[Distortion interpretation]
$D = 0$ means the observation strategy is \emph{sound}: type equality (as computed by the observer) implies behavioral equivalence. $D = 1$ means the strategy may conflate behaviorally distinct values.
\end{remark}

\subsection{The $(L, W, D)$ Tradeoff Space}

\begin{definition}[Achievable region]
A point $(L, W, D)$ is \emph{achievable} if there exists an observation strategy realizing those values. Let $\mathcal{R} \subseteq \mathbb{R}_{\geq 0} \times \mathbb{R}_{\geq 0} \times \{0, 1\}$ denote the achievable region.
\end{definition}

\begin{definition}[Pareto optimality]
A point $(L^*, W^*, D^*)$ is \emph{Pareto-optimal} if there is no achievable $(L, W, D)$ with $L \leq L^*$, $W \leq W^*$, $D \leq D^*$, and at least one strict inequality.
\end{definition}

The main result of Section~\ref{sec:lwd} is that nominal-tag observation achieves the unique Pareto-optimal point with $D = 0$.

