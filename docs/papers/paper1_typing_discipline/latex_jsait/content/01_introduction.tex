\subsection{The Classification Problem}

Every system that classifies entities faces a fundamental question: \emph{which attributes should we observe?} A database schema chooses columns. A biological taxonomy chooses morphological features. A type system chooses between structural properties and nominal identity. A library classification system chooses subject facets.

This paper proves that the choice has unavoidable information-theoretic consequences. Specifically:
\begin{enumerate}
\item \textbf{Information barriers exist.} Observers limited to a fixed attribute family cannot compute properties that vary within equivalence classes induced by those attributes. This is not a computational limitation---it is an impossibility rooted in the observations themselves.
\item \textbf{Nominal tags are optimal.} Adding a single primitive---a tag identifying each entity's class---reduces witness cost from $\Omega(n)$ to $O(1)$. This is Pareto-optimal: no observer achieves lower cost without increasing tag length or semantic distortion.
\item \textbf{Minimal observation sets have matroid structure.} All minimal distinguishing query sets for a domain have equal cardinality. The ``distinguishing dimension'' of a classification problem is well-defined.
\end{enumerate}

The results apply universally. Programming language runtimes, database systems, biological taxonomies, and knowledge graphs all instantiate the same abstract structure. We develop the theory in full generality, then show how specific systems realize it.

\subsection{The Observation Model}

We formalize the observational constraint as a family of binary predicates. The terminology is deliberately abstract; concrete instantiations follow in Section~\ref{sec:applications}.

\begin{definition}[Entity space and attribute family]
Let $\mathcal{V}$ be a set of entities (program objects, database records, biological specimens, library items). Let $\mathcal{I}$ be a finite set of \emph{attributes}---observable properties that partition the entity space.
\end{definition}

\begin{remark}[Terminology]
We use ``attribute'' for the abstract concept. In type systems, attributes are \emph{interfaces} or \emph{method signatures}. In databases, they are \emph{columns}. In taxonomy, they are \emph{phenotypic characters}. In library science, they are \emph{facets}. The mathematics is identical.
\end{remark}

\begin{definition}[Interface observation family]
For each $I \in \mathcal{I}$, define the interface-membership observation $q_I: \mathcal{V} \to \{0,1\}$:
\[
q_I(v) = \begin{cases} 1 & \text{if } v \text{ satisfies interface } I \\ 0 & \text{otherwise} \end{cases}
\]
Let $\Phi_{\mathcal{I}} = \{q_I : I \in \mathcal{I}\}$ denote the interface observation family.
\end{definition}

\begin{definition}[Interface profile]
The interface profile function $\pi: \mathcal{V} \to \{0,1\}^{|\mathcal{I}|}$ maps each value to its complete interface signature:
\[
\pi(v) = (q_I(v))_{I \in \mathcal{I}}
\]
\end{definition}

\begin{definition}[Interface indistinguishability]
Values $v, w \in \mathcal{V}$ are \emph{interface-indistinguishable}, written $v \sim w$, iff $\pi(v) = \pi(w)$.
\end{definition}

The relation $\sim$ is an equivalence relation. We write $[v]_\sim$ for the equivalence class of $v$.

\begin{definition}[Interface-only observer]
An \emph{interface-only observer} is any procedure whose interaction with a value $v \in \mathcal{V}$ is limited to queries in $\Phi_{\mathcal{I}}$. Formally, the observer receives only $\pi(v)$, not $v$ itself.
\end{definition}

\subsection{The Central Question}

The central question is: \textbf{what semantic properties can an interface-only observer compute?}

A semantic property is a function $P: \mathcal{V} \to \{0,1\}$ (or more generally, $P: \mathcal{V} \to Y$ for some codomain $Y$). We say $P$ is \emph{interface-computable} if there exists a function $f: \{0,1\}^{|\mathcal{I}|} \to Y$ such that $P(v) = f(\pi(v))$ for all $v$.

\subsection{The Information Barrier}

\begin{theorem}[Information barrier]\label{thm:information-barrier}
Let $P: \mathcal{V} \to Y$ be any function. If $P$ is interface-computable, then $P$ is constant on $\sim$-equivalence classes:
\[
v \sim w \implies P(v) = P(w)
\]
Equivalently: no interface-only observer can compute any property that varies within an equivalence class.
\end{theorem}

\begin{proof}
Suppose $P$ is interface-computable via $f$, i.e., $P(v) = f(\pi(v))$ for all $v$. Let $v \sim w$, so $\pi(v) = \pi(w)$. Then:
\[
P(v) = f(\pi(v)) = f(\pi(w)) = P(w)
\]
\end{proof}

\begin{remark}[Information-theoretic nature]
The barrier is \emph{informational}, not computational. Given unlimited time, memory, and computational power, an interface-only observer still cannot distinguish $v$ from $w$ when $\pi(v) = \pi(w)$. The constraint is on the evidence itself.
\end{remark}

\begin{corollary}[Provenance is not interface-computable]\label{cor:provenance-barrier}
Let $\text{type}: \mathcal{V} \to \mathcal{T}$ be the type assignment function. If there exist values $v, w$ with $\pi(v) = \pi(w)$ but $\text{type}(v) \neq \text{type}(w)$, then type identity is not interface-computable.
\end{corollary}

\begin{proof}
Direct application of Theorem~\ref{thm:information-barrier} to $P = \text{type}$.
\end{proof}

\subsection{The Positive Result: Nominal Tagging}

We now show that augmenting interface observations with a single primitive---nominal-tag access---achieves constant witness cost.

\begin{definition}[Nominal-tag access]
A \emph{nominal tag} is a value $\tau(v) \in \mathcal{T}$ associated with each $v \in \mathcal{V}$, representing the type identity of $v$. The \emph{nominal-tag access} operation returns $\tau(v)$ in $O(1)$ time.
\end{definition}

\begin{definition}[Primitive query set]
The extended primitive query set is $\Phi_{\mathcal{I}}^+ = \Phi_{\mathcal{I}} \cup \{\tau\}$, where $\tau$ denotes nominal-tag access.
\end{definition}

\begin{definition}[Witness cost]
Let $W(P)$ denote the minimum number of primitive queries from $\Phi_{\mathcal{I}}^+$ required to compute property $P$:
\[
W(P) = \min \{ c(A) : A \text{ is a procedure computing } P \}
\]
where $c(A)$ counts the number of queries to $\Phi_{\mathcal{I}}^+$ made by $A$.
\end{definition}

\begin{theorem}[Constant witness for type identity]\label{thm:constant-witness}
Under nominal-tag access, type identity checking has constant witness cost:
\[
W(\text{type-identity}) = O(1)
\]
Specifically, the witness procedure is: return $\tau(v_1) = \tau(v_2)$.
\end{theorem}

\begin{proof}
The procedure makes exactly 2 primitive queries (one $\tau$ access per value) and one comparison. This is $O(1)$ regardless of the number of interfaces $|\mathcal{I}|$.
\end{proof}

\begin{theorem}[Interface-only lower bound]\label{thm:interface-lower-bound}
For interface-only observers, type identity checking requires:
\[
W(\text{type-identity}) = \Omega(|\mathcal{I}|)
\]
in the worst case.
\end{theorem}

\begin{proof}
Construct a family of $|\mathcal{I}|$ types where each type $T_i$ satisfies exactly the interfaces $\{I_1, \ldots, I_i\}$. Distinguishing $T_{i}$ from $T_{i+1}$ requires querying $I_{i+1}$. Thus, distinguishing all pairs requires querying all $|\mathcal{I}|$ interfaces.
\end{proof}

\subsection{Main Contributions}

This paper establishes the following results:

\begin{enumerate}
\item \textbf{Information Barrier Theorem} (Theorem~\ref{thm:information-barrier}): Interface-only observers cannot compute any property that varies within $\sim$-equivalence classes. This is an information-theoretic impossibility, not a computational limitation.

\item \textbf{Constant-Witness Theorem} (Theorem~\ref{thm:constant-witness}): Nominal-tag access achieves $W(\text{type-identity}) = O(1)$, with matching lower bound $\Omega(|\mathcal{I}|)$ for interface-only observers (Theorem~\ref{thm:interface-lower-bound}).

\item \textbf{Complexity Separation} (Section~\ref{sec:complexity}): We establish O(1) vs O(k) vs $\Omega(n)$ complexity bounds for error localization under different observation regimes.

\item \textbf{Matroid Structure} (Section~\ref{sec:matroid}): Minimal distinguishing query sets form the bases of a matroid. All such sets have equal cardinality, establishing a well-defined ``distinguishing dimension.''

\item \textbf{$(L, W, D)$ Optimality} (Section~\ref{sec:lwd}): Nominal-tag observers achieve the unique Pareto-optimal point in the $(L, W, D)$ tradeoff space (tag length, witness cost, distortion).

\item \textbf{Machine-Checked Proofs}: All results formalized in Lean 4 (6,086 lines, 265 theorems, 0 \texttt{sorry} placeholders).
\end{enumerate}

\subsection{Related Work and Positioning}

The information barrier (Theorem~\ref{thm:information-barrier}) is related to results in query complexity and communication complexity, where limited observations constrain computable functions. The matroid structure connects to lattice-theoretic approaches in abstract interpretation \cite{cousot1977abstract}.

The $(L, W, D)$ analysis extends classical rate-distortion theory \cite{shannon1959coding, berger1971rate} to a discrete classification setting with three dimensions: tag length $L$, witness cost $W$ (query complexity), and semantic distortion $D$ (fidelity).

\textbf{Historical context.} In programming language theory, the question of whether ``duck typing'' (attribute-only observation) is equivalent to nominal typing has been debated since Smalltalk (1980) and formalized in discussions of structural vs.\ nominal subtyping \cite{Cardelli1985}. Proponents argue that if two entities ``walk like a duck and quack like a duck,'' they should be treated identically. Critics argue that provenance matters.

We prove that the witness-cost gap between duck typing and nominal typing is unbounded: duck typing incurs $\Omega(n)$ witness cost where nominal tagging achieves $O(1)$. This is not an approximation or heuristic---it is a machine-checked theorem (Lean 4, 0 \texttt{sorry}).

\textbf{Prescriptive implications.} Programming languages have independently converged on hybrid classification: Python added Abstract Base Classes (PEP 3119), TypeScript introduced branded types, Rust's trait system combines structural interfaces with nominal identity. This convergence reflects the information-theoretic optimality of nominal tags proved in this paper. The same principles apply to database schema design (primary keys as nominal tags), biological taxonomy (species identifiers), and knowledge representation (entity URIs).

The contribution is not advocacy for a particular language feature, but identification of a universal tradeoff that any classification system must navigate.

\subsection{Paper Organization}

Section~\ref{sec:framework} formalizes the compression framework and defines the $(L, W, D)$ tradeoff. Section~\ref{sec:complexity} establishes complexity bounds for error localization. Section~\ref{sec:matroid} proves the matroid structure of type axes. Section~\ref{sec:witness} analyzes witness cost in detail. Section~\ref{sec:lwd} proves Pareto optimality. Section~\ref{sec:applications} instantiates the theory in real runtimes. Section~\ref{sec:conclusion} concludes. Appendix~\ref{sec:lean} describes the Lean 4 formalization.

