# arXiv abstract (Paper 1, JSAIT)

Copy/paste the single paragraph below into the arXiv “Abstract” field (do **not** include this header line).

We extend classical rate-distortion theory to a discrete classification setting with three resources: tag rate $L$ (bits of storage per entity), identification cost $W$ (queries to determine class membership), and distortion $D$ (misidentification probability). We prove an information barrier: when distinct classes share identical attribute profiles (i.e., the attribute-profile map $\pi$ is not injective on classes), zero-error identification from attribute queries alone is impossible. We characterize the unique Pareto-optimal zero-error point in the $(L,W,D)$ tradeoff space: a nominal tag of length $L=\lceil\log_2 k\rceil$ bits for $k$ classes yields $W=O(1)$ and $D=0$. Without tags ($L=0$), zero-error identification requires $W=\Omega(d)$ attribute queries, where $d$ is the distinguishing dimension; in the worst case $d=n$ (the ambient attribute count), giving $W=\Omega(n)$. In the presence of attribute collisions, any tag-free scheme incurs $D>0$. Conversely, in any information-barrier domain, any scheme achieving $D=0$ requires $L\ge \log_2 k$ bits; this is tight. We show minimal sufficient query sets form the bases of a matroid, so the distinguishing dimension is well-defined, connecting to zero-error source coding via graph entropy. We instantiate the theory to type systems, databases, and biological taxonomy. All results are machine-checked in Lean 4 (6000+ lines, 0 sorry).
