\section{Probability Model}\label{probability-model}

We derive the relationship between DOF and error probability from Paper 1's axis independence theorem.

\subsection{Error Independence from Axis Orthogonality}

The independence of errors is not an axiom---it is a consequence of axis orthogonality proven in Paper 1~\cite{paper1_typing_discipline}.

\begin{theorem}[Error Independence]\label{thm:error-independence}
If axes $\{A_1, \ldots, A_n\}$ are orthogonal (Paper 1, Theorem \texttt{minimal\_complete\_unique\_orthogonal}), then errors along each axis are statistically independent.
\end{theorem}

\begin{proof}
By Paper 1's orthogonality theorem, orthogonal axes satisfy:
\[
\forall i \neq j: A_i \perp A_j \quad (\text{no axis constrains another})
\]

An \emph{error along axis $A_i$} is a deviation from specification in the dimension $A_i$ controls. By orthogonality:
\begin{itemize}
\item Deviation along $A_i$ does not affect the value along $A_j$
\item The probability of error in $A_i$ is independent of the state of $A_j$
\end{itemize}

Therefore:
\[
P(\text{error in } A_i \land \text{error in } A_j) = P(\text{error in } A_i) \cdot P(\text{error in } A_j)
\]

This is the definition of statistical independence.
\end{proof}

\begin{corollary}[DOF = Independent Error Sources]\label{cor:dof-errors}
DOF$(A) = n$ implies $n$ independent sources of error, each with probability $p$.
\end{corollary}

\begin{proof}
DOF counts independent axes (Paper 2, Definition~\ref{def:dof}). By Theorem~\ref{thm:error-independence}, independent axes have independent errors.
\end{proof}

\begin{theorem}[Error Compounding]\label{thm:error-compound}
For a system to be correct, all $n$ independent axes must be error-free. Errors compound multiplicatively.
\end{theorem}

\begin{proof}
By Paper 2's coherence theorem (\texttt{oracle\_arbitrary}), incoherence in any axis violates system correctness. An error in axis $A_i$ introduces incoherence along $A_i$. Therefore, correctness requires $\bigwedge_{i=1}^{n} \neg\text{error}(A_i)$. By Theorem~\ref{thm:error-independence}, this probability is $(1-p)^n$.
\end{proof}

\subsection{Error Probability Formula}

\begin{theorem}[Error Probability]\label{thm:error-prob}
For architecture with $n$ DOF and per-component error rate $p$:
\[
P_{\text{error}}(n) = 1 - (1-p)^n
\]
\end{theorem}

\begin{proof}
By Theorem~\ref{thm:error-independence} (derived from Paper 1's orthogonality), each DOF has independent error probability $p$, so each is correct with probability $(1-p)$. By Theorem~\ref{thm:error-compound}, all $n$ DOF must be correct:
\[
P_{\text{correct}}(n) = (1-p)^n
\]
Therefore:
\[
P_{\text{error}}(n) = 1 - P_{\text{correct}}(n) = 1 - (1-p)^n
\]
\end{proof}

\begin{corollary}[Linear Approximation]\label{cor:linear-approx}
For small $p$ (specifically, $p < 0.1$):
\[
P_{\text{error}}(n) \approx n \cdot p
\]
with relative error less than $10\%$.
\end{corollary}

\begin{proof}
Using Taylor expansion: $(1-p)^n = e^{n \ln(1-p)} \approx e^{-np}$ for small $p$.
Further: $e^{-np} \approx 1 - np$ for $np < 1$.
Therefore: $P_{\text{error}}(n) = 1 - (1-p)^n \approx 1 - (1 - np) = np$.
\end{proof}

\begin{corollary}[DOF-Error Monotonicity]\label{cor:dof-monotone}
For architectures $A_1, A_2$:
\[
\text{DOF}(A_1) < \text{DOF}(A_2) \implies P_{\text{error}}(A_1) < P_{\text{error}}(A_2)
\]
\end{corollary}

\begin{proof}
$P_{\text{error}}(n) = 1 - (1-p)^n$ is strictly increasing in $n$ for $p \in (0,1)$.
\end{proof}

\subsection{Expected Errors}

\begin{theorem}[Expected Error Bound]\label{thm:expected-errors}
Expected number of errors in architecture $A$:
\[
\mathbb{E}[\text{\# errors}] = p \cdot \text{DOF}(A)
\]
\end{theorem}

\begin{proof}
By linearity of expectation:
\[
\mathbb{E}[\text{\# errors}] = \sum_{i=1}^{\text{DOF}(A)} P(\text{error in DOF}_i) = \sum_{i=1}^{\text{DOF}(A)} p = p \cdot \text{DOF}(A)
\]
\end{proof}

\begin{example}[Concrete Calculations]
Assume $p = 0.01$ (1\% per-component error rate):
\begin{itemize}
\item DOF $= 1$: $P_{\text{error}} = 1 - 0.99 = 0.01$ (1\%)
\item DOF $= 10$: $P_{\text{error}} = 1 - 0.99^{10} \approx 0.096$ (9.6\%)
\item DOF $= 100$: $P_{\text{error}} = 1 - 0.99^{100} \approx 0.634$ (63.4\%)
\end{itemize}
\end{example}

\subsection{Connection to Reliability Theory}

The error model has a direct interpretation in classical reliability theory \cite{patterson2013computer}, connecting software architecture to a mature mathematical framework with 60+ years of theoretical development.

\begin{theorem}[DOF-Reliability Isomorphism]\label{thm:dof-reliability}
An architecture with DOF $= n$ is \emph{isomorphic} to a series reliability system with $n$ components. The isomorphism:
\begin{enumerate}
\item \textbf{Preserves ordering:} If $\text{DOF}(A_1) < \text{DOF}(A_2)$, then $P_{\text{error}}(A_1) < P_{\text{error}}(A_2)$
\item \textbf{Is invertible:} Round-trip mapping preserves DOF exactly
\item \textbf{Connects domains:} $P_{\text{error}}(n) = 1 - R_{\text{series}}(n)$ where $R_{\text{series}}(n) = (1-p)^n$
\end{enumerate}
\end{theorem}

\textbf{Interpretation:} Each DOF is a ``component'' that must work correctly. This is the reliability analog of Theorem~\ref{thm:error-independence}, which derives error independence from axis orthogonality.

\begin{theorem}[Approximation Error Bound]\label{thm:approx-bound}
The linear approximation $P_{\text{error}}(n) \approx n \cdot p$ has error $O(n^2 p^2)$. Specifically:
\[
|P_{\text{error}}(n) - n \cdot p| = \frac{n(n-1)p^2}{2} + O(n^3 p^3)
\]
For $p = 0.01$ and $n = 10$: error $< 0.5\%$. The approximation preserves all ordering relationships.
\end{theorem}

\begin{proof}
By binomial expansion: $(1-p)^n = 1 - np + \binom{n}{2}p^2 - O(p^3)$.
Therefore: $1 - (1-p)^n = np - \frac{n(n-1)}{2}p^2 + O(n^3 p^3)$.
The quadratic correction is bounded by $n^2 p^2$, which is negligible in the software regime.
\end{proof}

\textbf{Linear Approximation Justification:} For small $p$ (the software engineering regime where $p \approx 0.01$), the linear model $P_{\text{error}} \approx n \cdot p$ is:
\begin{enumerate}
\item Accurate (first-order Taylor expansion with proven $O(n^2 p^2)$ error bound)
\item Preserves all ordering relationships (if $n_1 < n_2$, then $n_1 p < n_2 p$)
\item Cleanly provable in natural number arithmetic (avoiding real analysis)
\end{enumerate}

\subsection{Epistemic Grounding}

The probability model is not axiomatic---it is derived from the epistemic foundations established in Papers 1 and 2:

\begin{enumerate}
\item \textbf{Paper 1} proves axis orthogonality (\texttt{minimal\_complete\_unique\_orthogonal})
\item \textbf{Theorem~\ref{thm:error-independence}} derives error independence from orthogonality
\item \textbf{Paper 2} establishes that DOF = 1 guarantees coherence (Theorem~\texttt{oracle\_arbitrary})
\item \textbf{Theorem~\ref{thm:error-compound}} connects errors to incoherence
\end{enumerate}

This derivation chain ensures the probability model rests on proven foundations, not assumed axioms.

\subsection{Leverage Gap: Quantitative Predictions}

The leverage framework provides \emph{quantitative}, empirically testable predictions about modification costs.

\begin{theorem}[Leverage Gap]\label{thm:leverage-gap}
For architectures with equal capabilities, the modification cost ratio equals the inverse leverage ratio:
\[
\frac{\text{ModCost}(A_2)}{\text{ModCost}(A_1)} = \frac{\text{DOF}(A_2)}{\text{DOF}(A_1)} = \frac{L(A_1)}{L(A_2)}
\]
\end{theorem}

\begin{theorem}[Testable Prediction]\label{thm:testable-prediction}
If architecture $A_1$ has $n\times$ lower DOF than $A_2$ (for equal capabilities), then $A_1$ requires $n\times$ fewer expected modifications. This is empirically testable against PR/commit data.
\end{theorem}

\begin{corollary}[DOF Ratio Predicts Error Ratio]\label{cor:dof-ratio}
The ratio of expected errors between two architectures equals the ratio of their DOF:
\[
\frac{\mathbb{E}[\text{errors}(A_2)]}{\mathbb{E}[\text{errors}(A_1)]} = \frac{\text{DOF}(A_2)}{\text{DOF}(A_1)}
\]
\end{corollary}

These theorems transform architectural intuitions into testable hypotheses. A claim that ``Pattern X is 3Ã— better than Pattern Y'' can be verified by comparing DOF and measuring modification frequency in real codebases.

\subsection{Formalization}

Formalized in \texttt{Leverage/Probability.lean}:
\begin{itemize}
\item \texttt{dof\_reliability\_isomorphism}: Theorem~\ref{thm:dof-reliability} (the central isomorphism)
\item \texttt{isomorphism\_preserves\_failure\_ordering}: Ordering preservation
\item \texttt{isomorphism\_roundtrip}: Invertibility proof
\item \texttt{approximation\_error\_bound}: Theorem~\ref{thm:approx-bound} (Taylor bound)
\item \texttt{linear\_model\_preserves\_ordering}: Ordering preservation under approximation
\item \texttt{leverage\_gap}: Theorem~\ref{thm:leverage-gap}
\item \texttt{testable\_modification\_prediction}: Theorem~\ref{thm:testable-prediction}
\item \texttt{dof\_ratio\_predicts\_error\_ratio}: Corollary~\ref{cor:dof-ratio}
\item \texttt{lower\_dof\_lower\_errors}: Corollary~\ref{cor:dof-monotone}
\item \texttt{ssot\_minimal\_errors}: SSOT minimality
\end{itemize}

