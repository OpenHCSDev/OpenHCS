\section{Probability Model}\label{probability-model}

We derive the relationship between DOF and error probability from Paper 1's axis independence theorem.

\subsection{Error Independence from Axis Orthogonality}

The independence of errors is not an axiom---it is a consequence of axis orthogonality proven in Paper 1~\cite{paper1_typing_discipline}.

\begin{theorem}[Error Independence]\label{thm:error-independence}
If axes $\{A_1, \ldots, A_n\}$ are orthogonal (Paper 1, Theorem \texttt{minimal\_complete\_unique\_orthogonal}), then errors along each axis are statistically independent.
\end{theorem}

\begin{proof}
By Paper 1's orthogonality theorem, orthogonal axes satisfy:
\[
\forall i \neq j: A_i \perp A_j \quad (\text{no axis constrains another})
\]

An \emph{error along axis $A_i$} is a deviation from specification in the dimension $A_i$ controls. By orthogonality:
\begin{itemize}
\item Deviation along $A_i$ does not affect the value along $A_j$
\item The probability of error in $A_i$ is independent of the state of $A_j$
\end{itemize}

Therefore:
\[
P(\text{error in } A_i \land \text{error in } A_j) = P(\text{error in } A_i) \cdot P(\text{error in } A_j)
\]

This is the definition of statistical independence. \(\blacksquare\)
\end{proof}

\begin{corollary}[DOF = Independent Error Sources]\label{cor:dof-errors}
DOF$(A) = n$ implies $n$ independent sources of error, each with probability $p$.
\end{corollary}

\begin{proof}
DOF counts independent axes (Paper 2, Definition~\ref{def:dof}). By Theorem~\ref{thm:error-independence}, independent axes have independent errors. \(\blacksquare\)
\end{proof}

\begin{theorem}[Error Compounding]\label{thm:error-compound}
For a system to be correct, all $n$ independent axes must be error-free. Errors compound multiplicatively.
\end{theorem}

\begin{proof}
By Paper 2's coherence theorem (\texttt{oracle\_arbitrary}), incoherence in any axis violates system correctness. An error in axis $A_i$ introduces incoherence along $A_i$. Therefore, correctness requires $\bigwedge_{i=1}^{n} \neg\text{error}(A_i)$. By Theorem~\ref{thm:error-independence}, this probability is $(1-p)^n$. \(\blacksquare\)
\end{proof}

\subsection{Error Probability Formula}

\begin{theorem}[Error Probability]\label{thm:error-prob}
For architecture with $n$ DOF and per-component error rate $p$:
\[
P_{\text{error}}(n) = 1 - (1-p)^n
\]
\end{theorem}

\begin{proof}
By Theorem~\ref{thm:error-independence} (derived from Paper 1's orthogonality), each DOF has independent error probability $p$, so each is correct with probability $(1-p)$. By Theorem~\ref{thm:error-compound}, all $n$ DOF must be correct:
\[
P_{\text{correct}}(n) = (1-p)^n
\]
Therefore:
\[
P_{\text{error}}(n) = 1 - P_{\text{correct}}(n) = 1 - (1-p)^n
\] \(\blacksquare\)
\end{proof}

\begin{corollary}[Linear Approximation]\label{cor:linear-approx}
For small $p$ (specifically, $p < 0.1$):
\[
P_{\text{error}}(n) \approx n \cdot p
\]
with relative error less than $10\%$.
\end{corollary}

\begin{proof}
Using Taylor expansion: $(1-p)^n = e^{n \ln(1-p)} \approx e^{-np}$ for small $p$.
Further: $e^{-np} \approx 1 - np$ for $np < 1$.
Therefore: $P_{\text{error}}(n) = 1 - (1-p)^n \approx 1 - (1 - np) = np$. \(\blacksquare\)
\end{proof}

\begin{corollary}[DOF-Error Monotonicity]\label{cor:dof-monotone}
For architectures $A_1, A_2$:
\[
\text{DOF}(A_1) < \text{DOF}(A_2) \implies P_{\text{error}}(A_1) < P_{\text{error}}(A_2)
\]
\end{corollary}

\begin{proof}
$P_{\text{error}}(n) = 1 - (1-p)^n$ is strictly increasing in $n$ for $p \in (0,1)$. \(\blacksquare\)
\end{proof}

\subsection{Expected Errors}

\begin{theorem}[Expected Error Bound]\label{thm:expected-errors}
Expected number of errors in architecture $A$:
\[
\mathbb{E}[\text{\# errors}] = p \cdot \text{DOF}(A)
\]
\end{theorem}

\begin{proof}
By linearity of expectation:
\[
\mathbb{E}[\text{\# errors}] = \sum_{i=1}^{\text{DOF}(A)} P(\text{error in DOF}_i) = \sum_{i=1}^{\text{DOF}(A)} p = p \cdot \text{DOF}(A)
\] \(\blacksquare\)
\end{proof}

\begin{example}[Concrete Calculations]
Assume $p = 0.01$ (1\% per-component error rate):
\begin{itemize}
\item DOF $= 1$: $P_{\text{error}} = 1 - 0.99 = 0.01$ (1\%)
\item DOF $= 10$: $P_{\text{error}} = 1 - 0.99^{10} \approx 0.096$ (9.6\%)
\item DOF $= 100$: $P_{\text{error}} = 1 - 0.99^{100} \approx 0.634$ (63.4\%)
\end{itemize}
\end{example}

\subsection{Connection to Reliability Theory}

The error model has a direct interpretation in classical reliability theory \cite{patterson2013computer}, connecting software architecture to a mature mathematical framework with 60+ years of theoretical development.

\begin{theorem}[DOF as Series System]\label{thm:series-system}
An architecture with DOF = $n$ is a \emph{series system}: all $n$ degrees of freedom must be correctly specified for the system to be error-free. Thus:
\[
P_{\text{error}}(n) = 1 - R_{\text{series}}(n) \text{ where } R_{\text{series}}(n) = (1-p)^n
\]
\end{theorem}

\textbf{Interpretation:} Each DOF is a ``component'' that must work correctly. This is the reliability analog of Theorem~\ref{thm:error-independence}, which derives error independence from axis orthogonality.

\textbf{Linear Approximation Justification:} For small $p$ (the software engineering regime where $p \approx 0.01$), the linear model $P_{\text{error}} \approx n \cdot p$ is:
\begin{enumerate}
\item Accurate (first-order Taylor expansion)
\item Preserves all ordering relationships (if $n_1 < n_2$, then $n_1 p < n_2 p$)
\item Cleanly provable in natural number arithmetic (avoiding real analysis)
\end{enumerate}

\subsection{Epistemic Grounding}

The probability model is not axiomatic---it is derived from the epistemic foundations established in Papers 1 and 2:

\begin{enumerate}
\item \textbf{Paper 1} proves axis orthogonality (\texttt{minimal\_complete\_unique\_orthogonal})
\item \textbf{Theorem~\ref{thm:error-independence}} derives error independence from orthogonality
\item \textbf{Paper 2} establishes that DOF = 1 guarantees coherence (Theorem~\texttt{oracle\_arbitrary})
\item \textbf{Theorem~\ref{thm:error-compound}} connects errors to incoherence
\end{enumerate}

This derivation chain ensures the probability model rests on proven foundations, not assumed axioms.

\subsection{Formalization}

Formalized in \texttt{Leverage/Probability.lean}:
\begin{itemize}
\item \texttt{error\_independence\_from\_orthogonality}: Theorem~\ref{thm:error-independence} (references Paper 1)
\item \texttt{error\_compounding\_from\_coherence}: Theorem~\ref{thm:error-compound} (references Paper 2)
\item \texttt{error\_probability\_formula}: Theorem~\ref{thm:error-prob}
\item \texttt{dof\_error\_monotone}: Corollary~\ref{cor:dof-monotone}
\item \texttt{expected\_error\_bound}: Theorem~\ref{thm:expected-errors}
\item \texttt{linear\_model\_preserves\_ordering}: Theorem~\ref{thm:series-system}
\end{itemize}

